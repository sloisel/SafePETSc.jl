var documenterSearchIndex = {"docs":
[{"location":"guide/vectors/#Vectors","page":"Vectors","title":"Vectors","text":"SafePETSc provides distributed vectors through the Vec{T,Prefix} type, which wraps PETSc's distributed vector functionality with automatic memory management.","category":"section"},{"location":"guide/vectors/#Creating-Vectors","page":"Vectors","title":"Creating Vectors","text":"","category":"section"},{"location":"guide/vectors/#Uniform-Distribution","page":"Vectors","title":"Uniform Distribution","text":"Use Vec_uniform when all ranks have the same data:\n\n# Create a vector from uniform data\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\n\n# With custom partition\npartition = [1, 3, 5]  # rank 0: rows 1-2, rank 1: rows 3-4\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0]; row_partition=partition)\n\n# With custom prefix type (advanced, see PETSc Options below)\nv = Vec_uniform([1.0, 2.0]; Prefix=MPIDENSE)","category":"section"},{"location":"guide/vectors/#Sum-Distribution","page":"Vectors","title":"Sum Distribution","text":"Use Vec_sum when ranks contribute sparse entries:\n\nusing SparseArrays\n\n# Each rank contributes different entries\n# All contributions are summed\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nindices = [rank * 2 + 1, rank * 2 + 2]\nvalues = [1.0, 2.0]\nv = Vec_sum(sparsevec(indices, values, 10))","category":"section"},{"location":"guide/vectors/#Helper-Constructors","page":"Vectors","title":"Helper Constructors","text":"Create vectors similar to existing ones:\n\n# Zero vector with same size/partition as x\ny = zeros_like(x)\n\n# Ones vector\ny = ones_like(x)\n\n# Filled with specific value\ny = fill_like(x, 3.14)\n\n# With different element type\ny = zeros_like(x; T2=Float32)","category":"section"},{"location":"guide/vectors/#Vector-Operations","page":"Vectors","title":"Vector Operations","text":"","category":"section"},{"location":"guide/vectors/#Broadcasting","page":"Vectors","title":"Broadcasting","text":"Vectors support Julia's broadcasting syntax:\n\n# Element-wise operations\ny = x .+ 1.0\ny = 2.0 .* x\ny = x .^ 2\n\n# Vector-vector operations\nz = x .+ y\nz = x .* y\n\n# In-place operations\ny .= x .+ 1.0\ny .= 2.0 .* x .+ y","category":"section"},{"location":"guide/vectors/#Arithmetic","page":"Vectors","title":"Arithmetic","text":"# Addition and subtraction\nz = x + y\nz = x - y\n\n# Mixed with scalars\nz = x + 1.0\nz = 2.0 - x\n\n# Unary operations\nz = -x\nz = +x","category":"section"},{"location":"guide/vectors/#Linear-Algebra","page":"Vectors","title":"Linear Algebra","text":"# Adjoint (transpose)\nx_adj = x'\n\n# Adjoint-matrix multiplication\nresult = x' * A  # Returns adjoint vector","category":"section"},{"location":"guide/vectors/#Concatenation","page":"Vectors","title":"Concatenation","text":"Vectors can be concatenated to form new vectors or matrices:\n\nx = Vec_uniform([1.0, 2.0, 3.0])\ny = Vec_uniform([4.0, 5.0, 6.0])\n\n# Vertical concatenation (stacking) - creates a longer vector\nv = vcat(x, y)  # Vec{Float64,MPIAIJ} with 6 elements\n\n# Horizontal concatenation - creates a matrix (auto-upgrades to MPIDENSE)\nM = hcat(x, y)  # 3×2 Mat{Float64,MPIDENSE}\n\ntip: Vector Concatenation Behavior\nvcat of vectors returns a Vec{T,Prefix} (preserves vector type)\nhcat of vectors returns a Mat{T,Prefix} (creates a multi-column matrix)\nvcat of matrices returns a Mat{T,Prefix}The Prefix is automatically upgraded to MPIDENSE for horizontal concatenation since vectors are inherently dense.","category":"section"},{"location":"guide/vectors/#Partitioning","page":"Vectors","title":"Partitioning","text":"Vectors are partitioned across ranks to distribute work and memory.","category":"section"},{"location":"guide/vectors/#Default-Partitioning","page":"Vectors","title":"Default Partitioning","text":"# Equal distribution\nn = 100\nnranks = MPI.Comm_size(MPI.COMM_WORLD)\npartition = default_row_partition(n, nranks)\n\n# For n=100, nranks=4:\n# partition = [1, 26, 51, 76, 101]\n# rank 0: rows 1-25 (25 elements)\n# rank 1: rows 26-50 (25 elements)\n# rank 2: rows 51-75 (25 elements)\n# rank 3: rows 76-100 (25 elements)","category":"section"},{"location":"guide/vectors/#Custom-Partitioning","page":"Vectors","title":"Custom Partitioning","text":"# Unequal distribution\npartition = [1, 10, 30, 101]  # Different sizes per rank\nv = Vec_uniform(data; row_partition=partition)","category":"section"},{"location":"guide/vectors/#Partition-Requirements","page":"Vectors","title":"Partition Requirements","text":"Length nranks + 1\nFirst element is 1\nLast element is n + 1 (where n is vector length)\nStrictly increasing","category":"section"},{"location":"guide/vectors/#Properties","page":"Vectors","title":"Properties","text":"# Element type\nT = eltype(v)  # e.g., Float64\n\n# Size\nn = length(v)\nn = size(v, 1)\n\n# Partition information\npartition = v.obj.row_partition","category":"section"},{"location":"guide/vectors/#Row-Ownership-and-Indexing","page":"Vectors","title":"Row Ownership and Indexing","text":"","category":"section"},{"location":"guide/vectors/#Determining-Owned-Rows","page":"Vectors","title":"Determining Owned Rows","text":"Use own_row() to find which indices are owned by the current rank:\n\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\n\n# Get ownership range for this rank\nowned = own_row(v)  # e.g., 1:2 on rank 0, 3:4 on rank 1\n\nprintln(io0(), \"Rank $(MPI.Comm_rank(MPI.COMM_WORLD)) owns indices: $owned\")","category":"section"},{"location":"guide/vectors/#Indexing-Vectors","page":"Vectors","title":"Indexing Vectors","text":"Important: You can only index elements that are owned by the current rank. Attempting to access non-owned indices will result in an error.\n\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\nowned = own_row(v)\n\n# ✓ CORRECT - Access owned elements\nif 2 in owned\n    val = v[2]  # Returns 2.0 on the rank that owns index 2\nend\n\n# ✓ CORRECT - Access range of owned elements\nif owned == 1:2\n    vals = v[1:2]  # Returns [1.0, 2.0] on the rank that owns these indices\nend\n\n# ❌ WRONG - Accessing non-owned indices causes an error\nval = v[3]  # ERROR if rank doesn't own index 3!\n\nIndexing is non-collective - each rank can independently access its owned data without coordination.","category":"section"},{"location":"guide/vectors/#Use-Cases-for-Indexing","page":"Vectors","title":"Use Cases for Indexing","text":"Indexing is useful when you need to:\n\nExtract specific local values for computation\nImplement custom local operations\nInterface with non-PETSc code on owned data\n\n# Extract owned portion for local processing\nv = Vec_uniform(randn(100))\nowned = own_row(v)\n\n# Get local values\nlocal_vals = v[owned]\n\n# Process locally\nlocal_sum = sum(local_vals)\nlocal_max = maximum(local_vals)\n\n# Aggregate across ranks if needed\nglobal_sum = MPI.Allreduce(local_sum, +, MPI.COMM_WORLD)","category":"section"},{"location":"guide/vectors/#Row-wise-Operations-with-map_rows","page":"Vectors","title":"Row-wise Operations with map_rows","text":"The map_rows() function applies a function to each row of distributed vectors or matrices, similar to Julia's map but for distributed PETSc objects.","category":"section"},{"location":"guide/vectors/#Basic-Usage","page":"Vectors","title":"Basic Usage","text":"# Apply function to each element of a vector\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\nsquared = map_rows(x -> x[1]^2, v)  # Returns Vec([1.0, 4.0, 9.0, 16.0])\n\n# Transform vector to matrix (using adjoint for row output)\npowers = map_rows(x -> [x[1], x[1]^2, x[1]^3]', v)  # Returns 4×3 Mat\n\nNote: For vectors, the function receives a 1-element view, so use x[1] to access the scalar value.","category":"section"},{"location":"guide/vectors/#Output-Types","page":"Vectors","title":"Output Types","text":"The return type depends on what your function returns:\n\nScalar → Returns a Vec with same number of rows\nVector → Returns a Vec with expanded rows (m*n rows if each returns n-element vector)\nAdjoint Vector (row vector) → Returns a Mat{T,MPIDENSE} with m rows (always dense)\n\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\n\n# Scalar output: Vec with same size\ndoubled = map_rows(x -> 2 * x[1], v)\n\n# Vector output: Vec with expanded size (4 * 2 = 8 elements)\nexpanded = map_rows(x -> [x[1], x[1]^2], v)\n\n# Adjoint vector output: Mat with 4 rows, 3 columns\nmatrix_form = map_rows(x -> [x[1], x[1]^2, x[1]^3]', v)","category":"section"},{"location":"guide/vectors/#Combining-Multiple-Inputs","page":"Vectors","title":"Combining Multiple Inputs","text":"Process multiple vectors or matrices together:\n\nv1 = Vec_uniform([1.0, 2.0, 3.0])\nv2 = Vec_uniform([4.0, 5.0, 6.0])\n\n# Combine two vectors element-wise\ncombined = map_rows((x, y) -> [x[1] + y[1], x[1] * y[1]]', v1, v2)\n# Returns 3×2 matrix: [sum, product] for each pair\n\nImportant: All inputs must have the same row partition.","category":"section"},{"location":"guide/vectors/#Performance-Notes","page":"Vectors","title":"Performance Notes","text":"map_rows() is a collective operation - all ranks must call it\nThe function is applied only to locally owned rows on each rank\nResults are automatically assembled into a new distributed object\nWorks efficiently with both vectors and matrices (see Matrices guide)","category":"section"},{"location":"guide/vectors/#PETSc-Options-and-the-Prefix-Type-Parameter","page":"Vectors","title":"PETSc Options and the Prefix Type Parameter","text":"SafePETSc vectors have a Prefix type parameter (e.g., Vec{Float64,MPIAIJ}) that controls PETSc configuration through option prefixes. SafePETSc provides two built-in prefix types:","category":"section"},{"location":"guide/vectors/#Built-in-Prefix-Types","page":"Vectors","title":"Built-in Prefix Types","text":"MPIAIJ (default): General-purpose prefix\nString prefix: \"MPIAIJ_\"\nUse for: Most vector operations\nMPIDENSE: Dense matrix operations prefix\nString prefix: \"MPIDENSE_\"\nUse for: Vectors associated with dense matrices\n\nImportant: All PETSc vectors are inherently dense (they store all elements), regardless of the Prefix parameter. The prefix only affects which PETSc options are applied, not the internal storage format. Unlike matrices, there is no \"sparse vector\" format in PETSc.","category":"section"},{"location":"guide/vectors/#Setting-PETSc-Options","page":"Vectors","title":"Setting PETSc Options","text":"You can configure PETSc behavior for vectors with a specific prefix:\n\n# Configure CUDA vectors for dense operations\npetsc_options_insert_string(\"-MPIDENSE_vec_type cuda\")\n\n# Create vector with MPIDENSE prefix\nv = Vec_uniform(data; Prefix=MPIDENSE)\n# Now PETSc will use CUDA for this vector\n\n# Configure standard MPI vectors differently\npetsc_options_insert_string(\"-MPIAIJ_vec_type standard\")\nw = Vec_uniform(data; Prefix=MPIAIJ)\n# This vector uses the standard MPI type\n\nThe string prefix (e.g., \"MPIDENSE_\", \"MPIAIJ_\") is automatically prepended to option names when PETSc processes options for objects with that prefix type.\n\nAdvanced users can define custom prefix types (not documented here).","category":"section"},{"location":"guide/vectors/#Examples","page":"Vectors","title":"Examples","text":"","category":"section"},{"location":"guide/vectors/#Parallel-Computation","page":"Vectors","title":"Parallel Computation","text":"using SafePETSc\nusing MPI\n\nSafePETSc.Init()\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nnranks = MPI.Comm_size(MPI.COMM_WORLD)\n\n# Each rank creates local data\nn_global = 1000\npartition = default_row_partition(n_global, nranks)\nlo = partition[rank + 1]\nhi = partition[rank + 2] - 1\n\n# Generate data based on rank\ndata = collect(range(1.0, length=n_global))\n\n# Create distributed vector\nv = Vec_uniform(data)\n\n# Compute: y = 2x + 1\ny = 2.0 .* v .+ 1.0\n\nprintln(io0(), \"Computation complete\")","category":"section"},{"location":"guide/vectors/#Sparse-Contributions","page":"Vectors","title":"Sparse Contributions","text":"using SafePETSc\nusing SparseArrays\nusing MPI\n\nSafePETSc.Init()\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nn = 100\n\n# Each rank contributes to different parts\n# Use own_rank_only=true to assert local contributions\nlo = rank * 25 + 1\nhi = (rank + 1) * 25\nindices = collect(lo:hi)\nvalues = ones(length(indices)) * (rank + 1)\n\nv = Vec_sum(sparsevec(indices, values, n); own_rank_only=true)","category":"section"},{"location":"guide/vectors/#Performance-Tips","page":"Vectors","title":"Performance Tips","text":"Use Broadcasting: In-place broadcasting (y .= ...) avoids allocations\nBatch Operations: Combine multiple operations in one broadcast\nAvoid Extraction: Keep data in distributed vectors; don't extract to Julia arrays\nGPU-Aware: Set PETSc options for GPU execution\n\n# Good: in-place, batched\ny .= 2.0 .* x .+ 3.0 .* z .+ 1.0\n\n# Less good: multiple allocations\ny = 2.0 * x\ny = y + 3.0 * z\ny = y + 1.0","category":"section"},{"location":"guide/vectors/#Converting-to-Julia-Arrays","page":"Vectors","title":"Converting to Julia Arrays","text":"You can convert a distributed Vec to a native Julia Vector using the Vector() constructor. This is useful for interoperability with other Julia packages, data export, or analysis.\n\n# Create a distributed vector\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\n\n# Convert to Julia Vector\nv_julia = Vector(v)  # Returns Vector{Float64}","category":"section"},{"location":"guide/vectors/#Important:-Collective-Operation","page":"Vectors","title":"Important: Collective Operation","text":"The conversion is a collective operation - all ranks must call it:\n\n# ✓ CORRECT - All ranks participate\nv_julia = Vector(v)  # All ranks get the complete vector\n\n# ❌ WRONG - Will hang MPI!\nif rank == 0\n    v_julia = Vector(v)  # Only rank 0 calls, others wait forever\nend\n\nAfter conversion, all ranks receive the complete vector. The data is gathered from all ranks using MPI collective operations.","category":"section"},{"location":"guide/vectors/#When-to-Use-Conversions","page":"Vectors","title":"When to Use Conversions","text":"Good use cases:\n\nInteroperability: Pass data to packages that don't support PETSc\nSmall-scale analysis: Compute properties on small vectors\nData export: Save results to files\nVisualization: Convert for plotting libraries\n\nusing Plots\n\n# Solve distributed system\nA = Mat_uniform([2.0 1.0; 1.0 3.0])\nb = Vec_uniform([1.0, 2.0])\nx = A \\ b\n\n# Convert for plotting (small vector, so conversion is cheap)\nx_julia = Vector(x)\nplot(x_julia, label=\"Solution\")\n\nAvoid conversions for:\n\nLarge datasets: Gathers all data to all ranks (expensive!)\nIntermediate computations: Keep data in PETSc format\nRepeated access: Don't convert in loops\n\n# ❌ BAD - Expensive conversion in loop\nfor i in 1:1000\n    v_julia = Vector(v)  # Wasteful! Gathers data every iteration\n    process(v_julia[1])\nend\n\n# ✓ BETTER - Convert once if needed\nv_julia = Vector(v)\nfor i in 1:1000\n    process(v_julia[1])\nend","category":"section"},{"location":"guide/vectors/#Working-with-Converted-Vectors","page":"Vectors","title":"Working with Converted Vectors","text":"After conversion, you have a standard Julia array:\n\nusing Statistics\nusing LinearAlgebra\n\nx = Vec_uniform([1.0, 2.0, 3.0, 4.0])\nx_julia = Vector(x)\n\n# Use with any Julia package\nμ = mean(x_julia)     # Statistics\nσ = std(x_julia)\nn = norm(x_julia)     # LinearAlgebra\n\nprintln(io0(), \"Mean: $μ, Std: $σ, Norm: $n\")\n\nSee Converting to Native Julia Arrays for more details and examples.","category":"section"},{"location":"guide/vectors/#See-Also","page":"Vectors","title":"See Also","text":"Vec_uniform\nVec_sum\nzeros_like\nones_like\nfill_like\nInput/Output and Display - Display and conversion operations","category":"section"},{"location":"guide/vectors/#Pooling-and-Cleanup","page":"Vectors","title":"Pooling and Cleanup","text":"By default, released PETSc vectors are returned to an internal pool for reuse instead of being destroyed immediately. This reduces allocation overhead in vector-heavy workflows.\n\nDisable pooling: ENABLE_VEC_POOL[] = false\nManually free pooled vectors: clear_vec_pool!()\nCleanup points: SafeMPI.check_and_destroy!() performs partial GC and collective release processing; vectors in use remain valid, pooled vectors remain available for reuse","category":"section"},{"location":"guide/io/#Input/Output-and-Display","page":"Input/Output and Display","title":"Input/Output and Display","text":"When working with distributed MPI applications, managing output can be challenging. SafePETSc provides tools to handle display and IO operations effectively across multiple ranks.","category":"section"},{"location":"guide/io/#The-Challenge-with-MPI-Output","page":"Input/Output and Display","title":"The Challenge with MPI Output","text":"In MPI applications, each rank executes the same code. Without careful handling, printing operations can produce duplicate output:\n\n# ❌ Bad: This prints from every rank\nprintln(\"Solution: \", x)  # Prints 4 times with 4 ranks!","category":"section"},{"location":"guide/io/#The-io0()-Helper","page":"Input/Output and Display","title":"The io0() Helper","text":"SafePETSc provides the io0() function to easily control which ranks produce output:\n\n# ✓ Good: Prints only on rank 0\nprintln(io0(), \"Solution: \", x)","category":"section"},{"location":"guide/io/#How-io0()-Works","page":"Input/Output and Display","title":"How io0() Works","text":"io0() returns the provided IO stream if the current rank is in the set of selected ranks, and devnull on all other ranks:\n\nio0(io=stdout; r::Set{Int}=Set{Int}([0]), dn=devnull)\n\nParameters:\n\nio: The IO stream to use (default: stdout)\nr: Set of ranks that should produce output (default: Set{Int}([0]))\ndn: The IO stream to return for non-selected ranks (default: devnull)\n\nReturn value:\n\nReturns io if the current rank is in r\nReturns dn otherwise\n\nThis allows all ranks to execute the same code, but only the selected ranks actually write output.","category":"section"},{"location":"guide/io/#Basic-Usage","page":"Input/Output and Display","title":"Basic Usage","text":"using SafePETSc\nusing MPI\n\nSafePETSc.Init()\n\n# Create and solve system\nA = Mat_uniform([2.0 1.0; 1.0 3.0])\nb = Vec_uniform([1.0, 2.0])\nx = A \\ b\n\n# Print only on rank 0 (default)\nprintln(io0(), \"Solution computed\")\nprintln(io0(), x)  # Displays the vector","category":"section"},{"location":"guide/io/#Selecting-Different-Ranks","page":"Input/Output and Display","title":"Selecting Different Ranks","text":"You can specify which ranks should produce output:\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nnranks = MPI.Comm_size(comm)\n\n# Print from rank 0 (default)\nprintln(io0(), \"This is from rank 0\")\n\n# Print from rank 1\nprintln(io0(r=Set([1])), \"This is from rank 1\")\n\n# Print from the last rank\nprintln(io0(r=Set([nranks-1])), \"This is from the last rank\")\n\n# Print from multiple ranks (e.g., ranks 0 and 2)\nprintln(io0(r=Set([0, 2])), \"This is from ranks 0 and 2\")","category":"section"},{"location":"guide/io/#Writing-to-Files","page":"Input/Output and Display","title":"Writing to Files","text":"io0() works with any IO stream, including files:\n\nusing LinearAlgebra  # For norm()\n\n# Write to file only on rank 0 (assumes x, A, b from previous example)\nopen(\"results.txt\", \"w\") do f\n    println(io0(f), \"Results:\")\n    println(io0(f), \"Solution: \", x)\n    println(io0(f), \"Residual norm: \", norm(A*x - b))\nend","category":"section"},{"location":"guide/io/#Display-Methods-and-show()","page":"Input/Output and Display","title":"Display Methods and show()","text":"SafePETSc implements Julia's show() interface for vectors and matrices, allowing them to be displayed naturally.","category":"section"},{"location":"guide/io/#Automatic-Conversion","page":"Input/Output and Display","title":"Automatic Conversion","text":"When you display a Vec or Mat, it is automatically converted to a Julia array for display:\n\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\n\n# All of these work and trigger conversion:\nprintln(io0(), v)           # Uses show(io, v)\ndisplay(v)                  # Uses show(io, MIME\"text/plain\"(), v)\n@show v                     # Macro uses show()","category":"section"},{"location":"guide/io/#How-show()-Works","page":"Input/Output and Display","title":"How show() Works","text":"The show() methods are collective operations - all ranks participate:\n\nAll ranks call the conversion function (e.g., Vector(v) or Matrix(A))\nData is gathered from all ranks to all ranks\nEach rank has the complete array\nshow() displays it on each rank's IO stream\n\nWhen combined with io0(), only one rank actually displays:\n\n# All ranks execute this, but only rank 0 prints\nprintln(io0(), v)\n\n# Behind the scenes:\n# 1. All ranks: v_julia = Vector(v)  (collective)\n# 2. All ranks: show(io0(), v_julia)  (rank 0 shows, others write to devnull)\n\n⚠️ WARNING: Never Wrap show() in Rank-Dependent Conditionals!\n\nBecause show() is collective, wrapping it in a rank-dependent conditional will desynchronize the MPI cluster and cause it to hang:\n\n# ❌ WRONG - This will hang MPI!\nif rank == 0\n    println(v)  # Only rank 0 calls Vector(v), others wait forever\nend\n\n# ❌ WRONG - This will also hang!\nif rank == 0\n    @show v  # Only rank 0 participates in collective operations\nend\n\n# ✓ CORRECT - All ranks participate, only rank 0 prints\nprintln(io0(), v)  # All ranks call Vector(v), rank 0 displays\n\nIf you want rank-specific output, always use io0() - it ensures all ranks participate in the collective operations while controlling which ranks produce the output.","category":"section"},{"location":"guide/io/#Dense-vs-Sparse-Display","page":"Input/Output and Display","title":"Dense vs Sparse Display","text":"Matrices automatically choose the appropriate display format:\n\n# Dense matrix - displays as Matrix\nA_dense = Mat_uniform([1.0 2.0; 3.0 4.0])\nprintln(io0(), A_dense)  # Shows dense format\n\n# Sparse matrix - displays using sparse() format\nusing SparseArrays\nA_sparse = Mat_uniform(sparse([1, 2], [1, 2], [1.0, 4.0], 10, 10))\nprintln(io0(), A_sparse)  # Shows sparse format with (i, j, value) triples","category":"section"},{"location":"guide/io/#MIME-Type-Support","page":"Input/Output and Display","title":"MIME Type Support","text":"SafePETSc supports MIME type display for rich output in notebooks and other environments:\n\n# In Jupyter notebooks, Pluto, etc.\ndisplay(v)  # Uses MIME\"text/plain\"\ndisplay(A)  # Uses MIME\"text/plain\"\n\n# The three-argument show method is called:\n# show(io, MIME\"text/plain\"(), v)","category":"section"},{"location":"guide/io/#Converting-to-Native-Julia-Arrays","page":"Input/Output and Display","title":"Converting to Native Julia Arrays","text":"SafePETSc provides constructors to explicitly convert distributed PETSc objects to native Julia arrays. This is useful for interoperability with other Julia packages, data export, or small-scale analysis.","category":"section"},{"location":"guide/io/#Available-Conversions","page":"Input/Output and Display","title":"Available Conversions","text":"# Vector: Convert Vec to Julia Vector\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\nv_julia = Vector(v)  # Returns Vector{Float64}\n\n# Matrix: Convert Mat to Julia Matrix (dense)\nA = Mat_uniform([1.0 2.0; 3.0 4.0])\nA_dense = Matrix(A)  # Returns Matrix{Float64}\n\n# Sparse: Convert Mat to SparseMatrixCSC\nusing SparseArrays\nA_sparse = Mat_uniform(sparse([1, 2], [1, 2], [1.0, 4.0], 10, 10))\nA_csc = sparse(A_sparse)  # Returns SparseMatrixCSC{Float64, Int}","category":"section"},{"location":"guide/io/#Important:-Collective-Operations","page":"Input/Output and Display","title":"Important: Collective Operations","text":"All conversion functions are collective operations - every rank must call them:\n\n# ✓ CORRECT - All ranks participate\nv_julia = Vector(v)  # All ranks get the complete vector\n\n# ❌ WRONG - Will hang MPI!\nif rank == 0\n    v_julia = Vector(v)  # Only rank 0 calls, others wait forever\nend\n\nAfter conversion, all ranks receive the complete array. This is necessary because the conversion gathers distributed data from all ranks.","category":"section"},{"location":"guide/io/#When-to-Use-Conversions","page":"Input/Output and Display","title":"When to Use Conversions","text":"Good use cases:\n\nInteroperability: Pass data to packages that don't support PETSc\nSmall-scale analysis: Compute properties on small matrices/vectors\nData export: Save results to files in native Julia formats\nVisualization: Convert for plotting libraries\n\nusing Plots\n\n# Solve distributed system\nA = Mat_uniform([2.0 1.0; 1.0 3.0])\nb = Vec_uniform([1.0, 2.0])\nx = A \\ b\n\n# Convert for plotting (small vector, so conversion is cheap)\nx_julia = Vector(x)\nplot(x_julia, label=\"Solution\")\nsavefig(io0(), \"solution.png\")  # Only rank 0 saves\n\nWhen to avoid:\n\nLarge datasets: Conversion gathers all data to all ranks (expensive!)\nIntermediate computations: Keep data in PETSc format for efficiency\nRepeated access: Don't convert in loops\n\n# ❌ BAD - Expensive conversion in loop\nfor i in 1:1000\n    v_julia = Vector(v)  # Wasteful! Gathers data every iteration\n    process(v_julia[1])\nend\n\n# ✓ BETTER - Convert once if needed\nv_julia = Vector(v)\nfor i in 1:1000\n    process(v_julia[1])\nend\n\n# ✓ BEST - Don't convert at all if possible\n# Use PETSc operations directly instead","category":"section"},{"location":"guide/io/#Performance-Considerations","page":"Input/Output and Display","title":"Performance Considerations","text":"Conversion performance scales with:\n\nData size: Larger vectors/matrices take longer to gather\nRank count: More ranks means more communication\nNetwork: Collective operations require all-to-all communication\n\n# Small: fast conversion (< 1ms)\nv_small = Vec_uniform(ones(100))\nv_julia = Vector(v_small)\n\n# Large: expensive conversion (can be seconds!)\nv_large = Vec_uniform(ones(10_000_000))\nv_julia = Vector(v_large)  # All 10M elements sent to all ranks","category":"section"},{"location":"guide/io/#Working-with-Converted-Data","page":"Input/Output and Display","title":"Working with Converted Data","text":"After conversion, you have standard Julia arrays:\n\nusing LinearAlgebra\nusing Statistics\n\n# Convert PETSc objects\nx = Vec_uniform([1.0, 2.0, 3.0, 4.0])\nA = Mat_uniform([2.0 1.0; 1.0 3.0])\n\nx_julia = Vector(x)\nA_julia = Matrix(A)\n\n# Use with any Julia package\nμ = mean(x_julia)           # Statistics\nσ = std(x_julia)\nλ = eigvals(A_julia)        # LinearAlgebra\ndet_A = det(A_julia)\n\nprintln(io0(), \"Mean: $μ, Std: $σ\")\nprintln(io0(), \"Eigenvalues: \", λ)","category":"section"},{"location":"guide/io/#Sparse-Matrix-Conversion","page":"Input/Output and Display","title":"Sparse Matrix Conversion","text":"For sparse matrices, use sparse() to preserve sparsity:\n\nusing SparseArrays\n\n# Create large sparse matrix\nn = 10_000\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [2.0*ones(n); -ones(n-1); -ones(n-1)]\nA = Mat_sum(sparse(I, J, V, n, n))\n\n# Convert to CSC format (preserves sparsity)\nA_csc = sparse(A)  # SparseMatrixCSC - efficient!\n\n# Don't use Matrix() for sparse matrices!\nA_dense = Matrix(A)  # Creates 10000×10000 dense array - wasteful!\n\n# Work with sparse format\nnnz_A = nnz(A_csc)\nprintln(io0(), \"Nonzeros: $nnz_A out of $(n^2) entries\")","category":"section"},{"location":"guide/io/#Example:-Data-Export","page":"Input/Output and Display","title":"Example: Data Export","text":"using SafePETSc\nusing MPI\nusing SparseArrays\nusing DelimitedFiles  # For writedlm\n\nSafePETSc.Init()\n\n# Solve system\nA = Mat_uniform([2.0 1.0; 1.0 3.0])\nb = Vec_uniform([1.0, 2.0])\nx = A \\ b\n\n# Convert to Julia arrays (collective operation)\nx_julia = Vector(x)\nA_julia = Matrix(A)\n\n# Write to files (only rank 0)\nopen(\"solution.txt\", \"w\") do f\n    writedlm(io0(f), x_julia)\nend\n\nopen(\"matrix.txt\", \"w\") do f\n    writedlm(io0(f), A_julia)\nend\n\nprintln(io0(), \"Data exported to solution.txt and matrix.txt\")","category":"section"},{"location":"guide/io/#Advanced-IO-Patterns","page":"Input/Output and Display","title":"Advanced IO Patterns","text":"","category":"section"},{"location":"guide/io/#Per-Rank-Output-Files","page":"Input/Output and Display","title":"Per-Rank Output Files","text":"Sometimes you want each rank to write its own file:\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\n\n# Each rank writes to its own file - don't use io0()!\nopen(\"output_rank_$rank.txt\", \"w\") do f\n    println(f, \"Output from rank $rank\")\n    println(f, \"Local data: \", get_local_data())\nend\n\nIn this case, don't use io0() because you want all ranks to write.","category":"section"},{"location":"guide/io/#Debugging-Output","page":"Input/Output and Display","title":"Debugging Output","text":"For debugging, you might temporarily want output from all ranks:\n\n# Debugging: show output from all ranks\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nprintln(\"Rank $rank: x = \", x)  # All ranks print (useful for debugging)\n\n# Production: only rank 0\nprintln(io0(), \"Solution: \", x)  # Only rank 0 prints","category":"section"},{"location":"guide/io/#Best-Practices","page":"Input/Output and Display","title":"Best Practices","text":"Use io0() for normal output: Always use println(io0(), ...) for user-facing output\nShow vectors and matrices with io0(): println(io0(), x) not println(x)\nPer-rank files don't need io0(): When each rank writes its own file, write directly\nDebugging is the exception: Temporarily allowing all-rank output is fine for debugging\nFile IO works with io0(): open(\"file.txt\", \"w\") do f; println(io0(f), ...) end","category":"section"},{"location":"guide/io/#Example:-Complete-IO-Pattern","page":"Input/Output and Display","title":"Example: Complete IO Pattern","text":"using SafePETSc\nusing MPI\nusing LinearAlgebra\n\nSafePETSc.Init()\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nnranks = MPI.Comm_size(comm)\n\n# Setup and solve\nA = Mat_uniform([2.0 1.0; 1.0 3.0])\nb = Vec_uniform([1.0, 2.0])\nx = A \\ b\n\n# Standard output (rank 0 only)\nprintln(io0(), \"=\"^60)\nprintln(io0(), \"Parallel Linear Solve Complete\")\nprintln(io0(), \"  MPI ranks: \", nranks)\nprintln(io0(), \"  System size: \", length(b))\nprintln(io0(), \"=\"^60)\n\n# Display results\nprintln(io0(), \"\\nSolution vector:\")\nprintln(io0(), x)\n\nprintln(io0(), \"\\nResidual norm: \", norm(A*x - b))\n\n# Write detailed results to file (rank 0 only)\nopen(\"results.txt\", \"w\") do f\n    println(io0(f), \"Detailed Results\")\n    println(io0(f), \"=\"^60)\n    println(io0(f), \"\\nMatrix A:\")\n    println(io0(f), A)\n    println(io0(f), \"\\nRight-hand side b:\")\n    println(io0(f), b)\n    println(io0(f), \"\\nSolution x:\")\n    println(io0(f), x)\nend\n\nprintln(io0(), \"\\nResults written to results.txt\")\n\nThis pattern provides clean, professional output without duplication across MPI ranks.","category":"section"},{"location":"api/safempi/#SafeMPI-API-Reference","page":"SafeMPI","title":"SafeMPI API Reference","text":"The SafeMPI module provides distributed reference management for MPI-based parallel computing.","category":"section"},{"location":"api/safempi/#Core-Types","page":"SafeMPI","title":"Core Types","text":"","category":"section"},{"location":"api/safempi/#Reference-Management","page":"SafeMPI","title":"Reference Management","text":"","category":"section"},{"location":"api/safempi/#Trait-System","page":"SafeMPI","title":"Trait System","text":"","category":"section"},{"location":"api/safempi/#MPI-Utilities","page":"SafeMPI","title":"MPI Utilities","text":"","category":"section"},{"location":"api/safempi/#Configuration","page":"SafeMPI","title":"Configuration","text":"","category":"section"},{"location":"api/safempi/#SafePETSc.SafeMPI.DRef","page":"SafeMPI","title":"SafePETSc.SafeMPI.DRef","text":"DRef{T}\n\nA distributed reference to an object of type T that is managed across MPI ranks.\n\nWhen all ranks have released their references via garbage collection, the object is collectively destroyed on all ranks using the type's destroy_obj! method.\n\nConstructor\n\nDRef(obj::T; manager=default_manager[]) -> DRef{T}\n\nCreate a distributed reference to obj. The type T must opt-in to distributed management by defining destroy_trait(::Type{T}) = CanDestroy() and implementing destroy_obj!(obj::T).\n\nFinalizers automatically enqueue releases when the DRef is garbage collected. Call check_and_destroy!() to perform the actual collective destruction.\n\nExample\n\n# Define a type that can be managed\nstruct MyDistributedObject\n    data::Vector{Float64}\nend\n\nSafeMPI.destroy_trait(::Type{MyDistributedObject}) = SafeMPI.CanDestroy()\nSafeMPI.destroy_obj!(obj::MyDistributedObject) = println(\"Destroying object\")\n\n# Create a distributed reference\nref = DRef(MyDistributedObject([1.0, 2.0, 3.0]))\n# ref.obj accesses the underlying object\n# When ref is garbage collected and check_and_destroy!() is called, the object is destroyed\n\nSee also: DistributedRefManager, check_and_destroy!, destroy_trait\n\n\n\n\n\n","category":"type"},{"location":"api/safempi/#SafePETSc.SafeMPI.DistributedRefManager","page":"SafeMPI","title":"SafePETSc.SafeMPI.DistributedRefManager","text":"DistributedRefManager\n\nManages reference counting and collective destruction of distributed objects across MPI ranks.\n\nEvery rank keeps an identical counter_pool/free_ids state and runs the same ID allocation algorithm simultaneously, so there is no special root role. Finalizers simply enqueue release IDs locally. At safe points (check_and_destroy!), ranks Allgather pending releases, update mirrored counters deterministically, and destroy ready objects together, pushing the released IDs back into free_ids on every rank for reuse.\n\nSee also: DRef, check_and_destroy!, default_manager\n\n\n\n\n\n","category":"type"},{"location":"api/safempi/#SafePETSc.SafeMPI.check_and_destroy!","page":"SafeMPI","title":"SafePETSc.SafeMPI.check_and_destroy!","text":"check_and_destroy!(manager=default_manager[]; max_check_count::Integer=1)\n\nMPI Collective\n\nPerform garbage collection and process pending object releases, destroying objects when all ranks have released their references.\n\nThis function must be called explicitly to allow controlled cleanup points in the application. It performs a full garbage collection to trigger finalizers, then processes all pending release messages and collectively destroys objects that are ready.\n\nThe max_check_count parameter controls throttling: the function only performs cleanup every max_check_count calls. This reduces overhead in tight loops.\n\nExample\n\nSafeMPI.check_and_destroy!()  # Process releases immediately\nSafeMPI.check_and_destroy!(max_check_count=10)  # Only cleanup every 10th call\n\nSee also: DRef, DistributedRefManager\n\n\n\n\n\n","category":"function"},{"location":"api/safempi/#SafePETSc.SafeMPI.destroy_obj!","page":"SafeMPI","title":"SafePETSc.SafeMPI.destroy_obj!","text":"destroy_obj!(obj)\n\nTrait method called to collectively destroy an object when all ranks have released their references. Types that opt-in to distributed reference management must implement this method.\n\nExample\n\nSafeMPI.destroy_obj!(obj::MyType) = begin\n    # Perform collective cleanup (e.g., free MPI/PETSc resources)\n    cleanup_resources(obj)\nend\n\nSee also: DRef, destroy_trait\n\n\n\n\n\n","category":"function"},{"location":"api/safempi/#SafePETSc.SafeMPI.default_manager","page":"SafeMPI","title":"SafePETSc.SafeMPI.default_manager","text":"default_manager\n\nThe default DistributedRefManager instance used by all DRef objects unless explicitly overridden. Automatically initialized when the module loads.\n\n\n\n\n\n","category":"constant"},{"location":"api/safempi/#SafePETSc.SafeMPI.default_check","page":"SafeMPI","title":"SafePETSc.SafeMPI.default_check","text":"default_check\n\nReference to the default throttle count for check_and_destroy! calls. Set this to control how often automatic cleanup occurs during object creation. Default value is 10.\n\nExample:\n\nSafePETSc.default_check[] = 100  # Only cleanup every 100 object creations\n\n\n\n\n\n","category":"constant"},{"location":"api/safempi/#SafePETSc.SafeMPI.DestroySupport","page":"SafeMPI","title":"SafePETSc.SafeMPI.DestroySupport","text":"DestroySupport\n\nAbstract type for the trait system controlling which types can be managed by DRef. See CanDestroy and CannotDestroy.\n\n\n\n\n\n","category":"type"},{"location":"api/safempi/#SafePETSc.SafeMPI.CanDestroy","page":"SafeMPI","title":"SafePETSc.SafeMPI.CanDestroy","text":"CanDestroy <: DestroySupport\n\nTrait indicating that a type can be managed by DRef and supports collective destruction. Types must opt-in by defining destroy_trait(::Type{YourType}) = CanDestroy().\n\n\n\n\n\n","category":"type"},{"location":"api/safempi/#SafePETSc.SafeMPI.CannotDestroy","page":"SafeMPI","title":"SafePETSc.SafeMPI.CannotDestroy","text":"CannotDestroy <: DestroySupport\n\nTrait indicating that a type cannot be managed by DRef (default for all types).\n\n\n\n\n\n","category":"type"},{"location":"api/safempi/#SafePETSc.SafeMPI.destroy_trait","page":"SafeMPI","title":"SafePETSc.SafeMPI.destroy_trait","text":"destroy_trait(::Type) -> DestroySupport\n\nTrait function determining whether a type can be managed by DRef.\n\nReturns CanDestroy() for types that opt-in to distributed reference management, or CannotDestroy() for types that don't support it (default).\n\nExample\n\n# Opt-in a custom type\nSafeMPI.destroy_trait(::Type{MyType}) = SafeMPI.CanDestroy()\n\n\n\n\n\n","category":"function"},{"location":"api/safempi/#SafePETSc.SafeMPI.mpi_any","page":"SafeMPI","title":"SafePETSc.SafeMPI.mpi_any","text":"mpi_any(local_bool::Bool, comm=MPI.COMM_WORLD) -> Bool\n\nMPI Collective\n\nCollective logical OR reduction across all ranks in comm.\n\nReturns true on all ranks if any rank has local_bool == true, otherwise returns false on all ranks. This is useful for checking whether any rank encountered an error or special condition.\n\nExample\n\nlocal_error = (x < 0)  # Some local condition\nif SafeMPI.mpi_any(local_error)\n    # At least one rank has an error, all ranks enter this branch\n    error(\"Error detected on at least one rank\")\nend\n\nSee also: @mpiassert\n\n\n\n\n\n","category":"function"},{"location":"api/safempi/#SafePETSc.SafeMPI.mpi_uniform","page":"SafeMPI","title":"SafePETSc.SafeMPI.mpi_uniform","text":"mpi_uniform(A) -> Bool\n\nMPI Collective\n\nChecks whether the value A is identical across all MPI ranks.\n\nReturns true on all ranks if all ranks have the same value for A, otherwise returns false on all ranks. This is useful for verifying that distributed data structures are properly synchronized or that configuration values are consistent across all ranks.\n\nThe comparison is done by computing a SHA-1 hash of the serialized object on each rank and broadcasting rank 0's hash to all other ranks for comparison.\n\nExample\n\n# Verify that a configuration matrix is the same on all ranks\nconfig = [1.0 2.0; 3.0 4.0]\nSafeMPI.@mpiassert mpi_uniform(config) \"Configuration must be uniform across ranks\"\n\n# Safe to use as a uniform object\nconfig_petsc = Mat_uniform(config)\n\nSee also: @mpiassert, mpi_any\n\n\n\n\n\n","category":"function"},{"location":"api/safempi/#SafePETSc.SafeMPI.mpierror","page":"SafeMPI","title":"SafePETSc.SafeMPI.mpierror","text":"mpierror(msg::AbstractString, trace::Bool; comm=MPI.COMM_WORLD, code::Integer=1)\n\nMPI Collective\n\nBest-effort MPI-wide error terminator that avoids hangs:\n\nPrints [rank N] ERROR: msg on each process that reaches it\nIf trace is true, prints a backtrace\nIf MPI is initialized, aborts the communicator to cleanly stop all ranks (avoids deadlocks if other ranks are not in the same code path)\nFalls back to exit(code) if MPI is not initialized or already finalized\n\n\n\n\n\n","category":"function"},{"location":"api/safempi/#SafePETSc.SafeMPI.@mpiassert","page":"SafeMPI","title":"SafePETSc.SafeMPI.@mpiassert","text":"@mpiassert cond [message]\n\nMPI Collective\n\nMPI-aware assertion that checks cond on all ranks and triggers collective error handling if any rank fails the assertion.\n\nEach rank evaluates cond locally. If any rank has cond == false, all ranks are notified via mpi_any() and collectively enter error handling via mpierror(). Only ranks where the assertion failed will print a backtrace.\n\nThe assertion is skipped entirely if enable_assert[] == false (see set_assert).\n\nArguments\n\ncond: Boolean expression to check (assertion passes when cond == true)\nmessage: Optional custom error message (defaults to auto-generated message with file/line info)\n\nExample\n\n# Assert that all ranks have the same value\n@mpiassert SafeMPI.mpi_uniform(A) \"Matrix A must be uniform across ranks\"\n\n# Assert a local condition that must hold on all ranks\n@mpiassert n > 0 \"Array size must be positive\"\n\nSee also: mpi_any, mpierror, set_assert\n\n\n\n\n\n","category":"macro"},{"location":"api/safempi/#SafePETSc.SafeMPI.enable_assert","page":"SafeMPI","title":"SafePETSc.SafeMPI.enable_assert","text":"enable_assert\n\nGlobal flag controlling whether @mpiassert macros perform their checks. Set to false to disable all MPI assertions for performance. Default is true.\n\nSee also: set_assert, @mpiassert\n\n\n\n\n\n","category":"constant"},{"location":"api/safempi/#SafePETSc.SafeMPI.set_assert","page":"SafeMPI","title":"SafePETSc.SafeMPI.set_assert","text":"set_assert(x::Bool) -> nothing\n\nMPI Non-Collective\n\nEnable (true) or disable (false) MPI assertion checks via @mpiassert.\n\nExample\n\nSafeMPI.set_assert(false)  # Disable assertions\nSafeMPI.set_assert(true)   # Re-enable assertions\n\n\n\n\n\n","category":"function"},{"location":"guide/distributed_refs/#Distributed-Reference-Management","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"SafePETSc enables native Julia syntax for distributed linear algebra by implementing automatic distributed reference management via the SafeMPI module. This implementation detail ensures that distributed objects are properly cleaned up across all MPI ranks, allowing users to write natural Julia expressions like A * B + C without manual memory management.","category":"section"},{"location":"guide/distributed_refs/#The-Problem","page":"Distributed Reference Management","title":"The Problem","text":"In MPI-based parallel computing, objects like PETSc vectors and matrices exist on all ranks. Destroying such objects requires collective MPI calls—all ranks must participate. This creates challenges:\n\nPremature destruction: If one rank destroys an object while others still need it, the program crashes\nMemory leaks: If ranks don't coordinate cleanup, objects leak memory\nComplex coordination: Manual reference counting is error-prone","category":"section"},{"location":"guide/distributed_refs/#The-Solution:-DRef","page":"Distributed Reference Management","title":"The Solution: DRef","text":"SafePETSc uses DRef{T} (Distributed Reference) to automatically track object lifetimes:\n\nusing SafePETSc\n\n# Create a distributed vector (returns a DRef{_Vec{Float64}})\nv = Vec_uniform([1.0, 2.0, 3.0])\n\n# Use it normally\ny = v .+ 1.0\n\n# When v goes out of scope and is garbage collected,\n# SafePETSc coordinates cleanup across all ranks","category":"section"},{"location":"guide/distributed_refs/#How-It-Works","page":"Distributed Reference Management","title":"How It Works","text":"","category":"section"},{"location":"guide/distributed_refs/#Reference-Counting","page":"Distributed Reference Management","title":"Reference Counting","text":"Mirrored Counters: Each rank runs the same deterministic ID allocation, keeping a mirrored counter_pool and shared free_ids stack; all ranks recycle IDs identically without a designated root\nAutomatic Release: When a DRef is garbage collected, its finalizer enqueues the ID locally (no MPI in finalizers)\nCleanup Points: At check_and_destroy! calls (automatically invoked at object creation), SafePETSc:\nPeriodically triggers partial garbage collection (GC.gc(false)) so finalizers run\nDrains each rank's local release queue\nAllgathers counts and then Allgathervs the release IDs so every rank sees the same global sequence\nEach rank updates its mirrored counters identically and computes the same set of ready IDs\nAll ranks destroy ready objects simultaneously","category":"section"},{"location":"guide/distributed_refs/#Trait-Based-Opt-In","page":"Distributed Reference Management","title":"Trait-Based Opt-In","text":"Types must explicitly opt-in to distributed management:\n\n# Define your distributed type\nstruct MyDistributedObject\n    data::Vector{Float64}\n    # ... MPI-based fields\nend\n\n# Opt-in to distributed management\nSafeMPI.destroy_trait(::Type{MyDistributedObject}) = SafeMPI.CanDestroy()\n\n# Implement cleanup\nfunction SafeMPI.destroy_obj!(obj::MyDistributedObject)\n    # Perform collective cleanup (e.g., MPI_Free, PETSc destroy)\n    # This is called on ALL ranks simultaneously\n    cleanup_mpi_resources(obj)\nend\n\n# Now you can wrap it\nref = DRef(MyDistributedObject(...))","category":"section"},{"location":"guide/distributed_refs/#Controlling-Cleanup","page":"Distributed Reference Management","title":"Controlling Cleanup","text":"","category":"section"},{"location":"guide/distributed_refs/#Automatic-Cleanup-and-Pooling","page":"Distributed Reference Management","title":"Automatic Cleanup and Pooling","text":"Cleanup is triggered automatically at object creation. Every SafePETSc.default_check[] object creations (default: 10), a partial garbage collection (GC.gc(false)) runs to trigger finalizers, and pending releases are processed via MPI communication. This throttling reduces cleanup overhead.\n\nFor PETSc vectors, the default behavior is to return released vectors to a reuse pool instead of destroying them. Disable pooling with ENABLE_VEC_POOL[] = false or call clear_vec_pool!() to free pooled vectors.\n\nTo guarantee all objects are destroyed (not just those finalized by partial GC), call GC.gc(true) followed by SafeMPI.check_and_destroy!():\n\n# Force complete cleanup\nGC.gc(true)  # Full garbage collection\nSafeMPI.check_and_destroy!()  # Process all finalizers","category":"section"},{"location":"guide/distributed_refs/#Explicit-Cleanup","page":"Distributed Reference Management","title":"Explicit Cleanup","text":"You can manually trigger cleanup:\n\n# Trigger cleanup (does partial GC if throttle count reached)\nSafeMPI.check_and_destroy!()\n\n# With custom throttle count\nSafeMPI.check_and_destroy!(max_check_count=10)\n\nThe max_check_count parameter controls how often partial GC runs: check_and_destroy! only calls GC.gc(false) every max_check_count invocations, but MPI communication to process pending releases happens on every call.","category":"section"},{"location":"guide/distributed_refs/#Disabling-Assertions","page":"Distributed Reference Management","title":"Disabling Assertions","text":"For performance in production:\n\nSafeMPI.set_assert(false)  # Disable @mpiassert checks","category":"section"},{"location":"guide/distributed_refs/#Best-Practices","page":"Distributed Reference Management","title":"Best Practices","text":"","category":"section"},{"location":"guide/distributed_refs/#1.-Let-Scoping-Work-for-You","page":"Distributed Reference Management","title":"1. Let Scoping Work for You","text":"function compute_something()\n    A = Mat_uniform(...)\n    b = Vec_uniform(...)\n    x = A \\ b\n    # A, b, x cleaned up when function exits\n    return extract_result(x)\nend","category":"section"},{"location":"guide/distributed_refs/#2.-Explicit-Cleanup-in-Long-Running-Loops","page":"Distributed Reference Management","title":"2. Explicit Cleanup in Long-Running Loops","text":"for i in 1:1000000\n    v = Vec_uniform(data[i])\n    result[i] = compute(v)\n\n    # Periodic cleanup to avoid accumulation\n    if i % 100 == 0\n        SafeMPI.check_and_destroy!()\n    end\nend","category":"section"},{"location":"guide/distributed_refs/#Debugging","page":"Distributed Reference Management","title":"Debugging","text":"","category":"section"},{"location":"guide/distributed_refs/#Check-Reference-Counts","page":"Distributed Reference Management","title":"Check Reference Counts","text":"# Access the default manager\nmanager = SafeMPI.default_manager[]\n\n# Inspect state (mirrored on all ranks)\nprintln(io0(), \"Active objects: \", length(manager.counter_pool))\nprintln(io0(), \"Free IDs: \", length(manager.free_ids))","category":"section"},{"location":"guide/distributed_refs/#Enable-Verbose-Assertions","page":"Distributed Reference Management","title":"Enable Verbose Assertions","text":"# Assertions are enabled by default\nSafeMPI.enable_assert[]  # true\n\n# Use @mpiassert for collective checks\n@mpiassert all_data_valid \"Data validation failed\"","category":"section"},{"location":"guide/distributed_refs/#Performance-Considerations","page":"Distributed Reference Management","title":"Performance Considerations","text":"Cleanup Cost: check_and_destroy! uses collective Allgather/Allgatherv operations and periodically triggers partial garbage collection\nThrottling: Adjust SafePETSc.default_check[] to control how often partial garbage collection (GC.gc(false)) is triggered in check_and_destroy!() (default: 10). Higher values reduce GC overhead but may delay object finalization","category":"section"},{"location":"guide/distributed_refs/#See-Also","page":"Distributed Reference Management","title":"See Also","text":"SafePETSc.SafeMPI.DRef\nSafePETSc.SafeMPI.DistributedRefManager\nSafePETSc.SafeMPI.check_and_destroy!","category":"section"},{"location":"developer/#Developer-Guide","page":"Developer Guide","title":"Developer Guide","text":"This guide is for developers who want to contribute to SafePETSc or extend it with custom distributed types.","category":"section"},{"location":"developer/#Architecture-Overview","page":"Developer Guide","title":"Architecture Overview","text":"SafePETSc consists of two main layers:\n\nSafeMPI: Low-level distributed reference management\nSafePETSc: High-level PETSc wrappers using SafeMPI","category":"section"},{"location":"developer/#SafeMPI-Layer","page":"Developer Guide","title":"SafeMPI Layer","text":"The SafeMPI module implements reference counting across MPI ranks:\n\n┌─────────────────────────────────────────────┐\n│  User Code                                  │\n│  creates DRef-wrapped objects               │\n└─────────────┬───────────────────────────────┘\n              │\n┌─────────────▼────────────────────────────────┐\n│  SafeMPI.DRef{T}                             │\n│  - Wraps object                              │\n│  - Finalizer calls _release!                 │\n│  - Enqueues release ID locally (no MPI)      │\n└─────────────┬────────────────────────────────┘\n              │\n┌─────────────▼────────────────────────────────┐\n│  DistributedRefManager (mirrored on all ranks)│\n│  - Maintains identical counter_pool/free_ids │\n│  - Allgathers release IDs to update counters │\n│  - Pushes ready IDs back into `free_ids`     │\n└─────────────┬────────────────────────────────┘\n              │\n┌─────────────▼───────────────────────────────┐\n│  destroy_obj!(obj)                          │\n│  - Called on all ranks simultaneously       │\n│  - User-defined cleanup routine             │\n└─────────────────────────────────────────────┘","category":"section"},{"location":"developer/#SafePETSc-Layer","page":"Developer Guide","title":"SafePETSc Layer","text":"Wraps PETSc objects with DRef:\n\nstruct _Vec{T,Prefix}\n    v::PETSc.Vec{T}\n    row_partition::Vector{Int}\nend\n\nconst Vec{T,Prefix} = DRef{_Vec{T,Prefix}}\n\n# Opt-in to distributed management\nSafeMPI.destroy_trait(::Type{_Vec{T,Prefix}}) where {T,Prefix} = SafeMPI.CanDestroy()\n\n# Define cleanup\nfunction SafeMPI.destroy_obj!(x::_Vec{T,Prefix}) where {T,Prefix}\n    _destroy_petsc_vec!(x.v)\nend","category":"section"},{"location":"developer/#Adding-New-Distributed-Types","page":"Developer Guide","title":"Adding New Distributed Types","text":"To add your own distributed type:","category":"section"},{"location":"developer/#1.-Define-the-Internal-Type","page":"Developer Guide","title":"1. Define the Internal Type","text":"struct _MyDistributedType\n    # Your fields here\n    handle::Ptr{Cvoid}  # e.g., MPI handle\n    data::Vector{Float64}\n    # ... other fields\nend","category":"section"},{"location":"developer/#2.-Create-Type-Alias","page":"Developer Guide","title":"2. Create Type Alias","text":"const MyDistributedType = SafeMPI.DRef{_MyDistributedType}","category":"section"},{"location":"developer/#3.-Opt-In-to-Management","page":"Developer Guide","title":"3. Opt-In to Management","text":"SafeMPI.destroy_trait(::Type{_MyDistributedType}) = SafeMPI.CanDestroy()","category":"section"},{"location":"developer/#4.-Implement-Cleanup","page":"Developer Guide","title":"4. Implement Cleanup","text":"function SafeMPI.destroy_obj!(obj::_MyDistributedType)\n    # IMPORTANT: This is called on ALL ranks simultaneously\n    # Must be a collective operation\n\n    # Example: Free MPI resource\n    if obj.handle != C_NULL\n        MPI.Free(obj.handle)\n    end\n\n    # Clean up other resources\n    # ...\nend","category":"section"},{"location":"developer/#5.-Create-Constructor","page":"Developer Guide","title":"5. Create Constructor","text":"function MyDistributedType(data::Vector{Float64})\n    # Allocate distributed resource\n    handle = allocate_mpi_resource(data)\n\n    # Wrap in internal type\n    obj = _MyDistributedType(handle, data)\n\n    # Wrap in DRef (triggers cleanup coordination)\n    return DRef(obj)\nend","category":"section"},{"location":"developer/#Testing","page":"Developer Guide","title":"Testing","text":"","category":"section"},{"location":"developer/#Unit-Tests-Structure","page":"Developer Guide","title":"Unit Tests Structure","text":"SafePETSc uses a dual-file testing approach:\n\ntest/runtests.jl: Entry point that spawns MPI processes\ntest/test_*.jl: Individual test files run with MPI\n\nExample test file:\n\n# test/test_myfeature.jl\nusing SafePETSc\nusing Test\nusing MPI\n\nSafePETSc.Init()\n\n@testset \"My Feature\" begin\n    rank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n    # Test uniform distribution\n    v = Vec_uniform([1.0, 2.0, 3.0])\n    @test size(v) == (3,)\n\n    # Test operations\n    y = v .+ 1.0\n    @test eltype(y) == Float64\n\n    # Cleanup\n    SafeMPI.check_and_destroy!()\nend","category":"section"},{"location":"developer/#Running-Tests","page":"Developer Guide","title":"Running Tests","text":"# Run all tests\njulia --project=. -e 'using Pkg; Pkg.test()'\n\n# Run specific test\njulia --project=. -e 'using MPI; run(`$(MPI.mpiexec()) -n 4 $(Base.julia_cmd()) --project=. test/test_myfeature.jl`)'","category":"section"},{"location":"developer/#Coding-Guidelines","page":"Developer Guide","title":"Coding Guidelines","text":"","category":"section"},{"location":"developer/#Reference-Management","page":"Developer Guide","title":"Reference Management","text":"Always use DRef: Wrap distributed objects in DRef to ensure cleanup\nCleanup at creation: _make_ref automatically calls check_and_destroy!\nNo manual cleanup in operations: Avoid check_and_destroy! in regular functions\nCollective operations: destroy_obj! must be collective","category":"section"},{"location":"developer/#Error-Handling","page":"Developer Guide","title":"Error Handling","text":"Use @mpiassert: For collective error checking\nCoalesce assertions: Combine conditions into single @mpiassert\nInformative messages: Include context in error messages\n\n# Good: single assertion with multiple conditions\n@mpiassert (size(A, 2) == size(B, 1) &&\n            A.obj.col_partition == B.obj.row_partition) \"Matrix dimensions and partitions must match for multiplication\"\n\n# Less good: multiple assertions\n@mpiassert size(A, 2) == size(B, 1) \"Dimension mismatch\"\n@mpiassert A.obj.col_partition == B.obj.row_partition \"Partition mismatch\"","category":"section"},{"location":"developer/#PETSc-Interop","page":"Developer Guide","title":"PETSc Interop","text":"Use PETSc.@for_libpetsc: For multi-precision support\nGPU-friendly operations: Prefer bulk operations over element access\nConst for MATINITIALMATRIX: Use module constant MAT_INITIAL_MATRIX = Cint(0)\n\nPETSc.@for_libpetsc begin\n    function my_petsc_operation(mat::PETSc.Mat{$PetscScalar})\n        PETSc.@chk ccall((:PetscFunction, $libpetsc), ...)\n    end\nend","category":"section"},{"location":"developer/#Performance-Considerations","page":"Developer Guide","title":"Performance Considerations","text":"","category":"section"},{"location":"developer/#Cleanup-Overhead","page":"Developer Guide","title":"Cleanup Overhead","text":"check_and_destroy! uses collective Allgather/Allgatherv operations and periodically triggers partial GC\nDefault: partial GC every 10 object creations (controlled by SafePETSc.default_check[])\nTune default_check[] based on application: lower values = less memory, more overhead","category":"section"},{"location":"developer/#Memory-Management","page":"Developer Guide","title":"Memory Management","text":"Use DRef scoping to control lifetimes\nAvoid global DRef variables (prevent cleanup)\nConsider explicit cleanup in long loops","category":"section"},{"location":"developer/#GPU-Support","page":"Developer Guide","title":"GPU Support","text":"SafePETSc prioritizes GPU-friendly PETSc operations\nSet PETSc options for GPU: -mat_type aijcusparse -vec_type cuda\nAvoid element-wise access (causes GPU↔CPU transfers)","category":"section"},{"location":"developer/#Documentation","page":"Developer Guide","title":"Documentation","text":"","category":"section"},{"location":"developer/#Docstrings","page":"Developer Guide","title":"Docstrings","text":"Follow Julia documentation conventions:\n\n\"\"\"\n    my_function(x::Type; option=default) -> ReturnType\n\nBrief one-line description.\n\nExtended description with more details about the function's behavior,\nparameters, and return values.\n\n# Arguments\n- `x::Type`: Description of x\n- `option::Type=default`: Description of optional parameter\n\n# Returns\n- `ReturnType`: Description of return value\n\n# Examples\n\njulia result = my_function(input)\n\n\nSee also: [`related_function`](@ref), [`another_function`](@ref)\n\"\"\"\nfunction my_function(x; option=default)\n    # Implementation\nend","category":"section"},{"location":"developer/#Adding-Documentation-Pages","page":"Developer Guide","title":"Adding Documentation Pages","text":"Create markdown file in docs/src/\nAdd to pages in docs/make.jl\nBuild: julia --project=docs docs/make.jl","category":"section"},{"location":"developer/#Contributing","page":"Developer Guide","title":"Contributing","text":"","category":"section"},{"location":"developer/#Pull-Request-Process","page":"Developer Guide","title":"Pull Request Process","text":"Fork the repository\nCreate a feature branch\nAdd tests for new functionality\nUpdate documentation\nRun tests: julia --project=. -e 'using Pkg; Pkg.test()'\nSubmit pull request","category":"section"},{"location":"developer/#Code-Review-Checklist","page":"Developer Guide","title":"Code Review Checklist","text":"[ ] Tests pass\n[ ] Documentation updated\n[ ] Docstrings added for public API\n[ ] Reference management correct\n[ ] Collective operations properly coordinated\n[ ] Performance considerations addressed","category":"section"},{"location":"developer/#Debugging-Tips","page":"Developer Guide","title":"Debugging Tips","text":"","category":"section"},{"location":"developer/#MPI-Hangs","page":"Developer Guide","title":"MPI Hangs","text":"If program hangs, likely causes:\n\nNon-collective operation: One rank skipped a collective call\nUnbalanced branching: Ranks took different code paths\nMissing @mpiassert: Error on one rank, others waiting\n\nDebug with:\n\n# Add at suspicious points\nprintln(io0(), \"Reached checkpoint A\")\nMPI.Barrier(MPI.COMM_WORLD)","category":"section"},{"location":"developer/#Memory-Leaks","page":"Developer Guide","title":"Memory Leaks","text":"Check for:\n\nGlobal DRef variables\nSkipped check_and_destroy! in long loops\nCircular references preventing GC\n\nInspect manager state:\n\nmanager = SafeMPI.default_manager[]\nprintln(io0(), \"Active objects: \", length(manager.counter_pool))\nprintln(io0(), \"Pending releases: \", length(manager.pending_releases))","category":"section"},{"location":"developer/#Assertion-Failures","page":"Developer Guide","title":"Assertion Failures","text":"Enable verbose output:\n\n# Assertions enabled by default\nSafeMPI.enable_assert[]  # true\n\n# Check conditions\n@mpiassert condition \"Detailed error message\"","category":"section"},{"location":"developer/#Resources","page":"Developer Guide","title":"Resources","text":"PETSc Documentation\nMPI.jl Documentation\nDocumenter.jl Guide","category":"section"},{"location":"guide/solvers/#Linear-Solvers","page":"Linear Solvers","title":"Linear Solvers","text":"SafePETSc provides linear solver functionality through PETSc's KSP (Krylov Subspace) interface, wrapped with automatic memory management.","category":"section"},{"location":"guide/solvers/#Basic-Usage","page":"Linear Solvers","title":"Basic Usage","text":"","category":"section"},{"location":"guide/solvers/#Direct-Solve","page":"Linear Solvers","title":"Direct Solve","text":"The simplest way to solve a linear system:\n\n# Solve Ax = b\nx = A \\ b\n\n# Solve A^T x = b\nx = A' \\ b\n\nThis creates a solver internally, solves the system, and cleans up automatically.","category":"section"},{"location":"guide/solvers/#Multiple-Right-Hand-Sides","page":"Linear Solvers","title":"Multiple Right-Hand Sides","text":"# Matrix RHS: solve AX = B\nX = A \\ B\n\n# Transpose: solve A^T X = B\nX = A' \\ B\n\nNote: B and X must be dense matrices (MATMPIDENSE).","category":"section"},{"location":"guide/solvers/#Reusable-Solvers","page":"Linear Solvers","title":"Reusable Solvers","text":"For multiple solves with the same matrix, create a KSP object:\n\n# Create solver once\nksp = KSP(A)\n\n# Solve multiple systems\nx1 = zeros_like(b1)\nldiv!(ksp, x1, b1)\n\nx2 = zeros_like(b2)\nldiv!(ksp, x2, b2)\n\n# KSP is cleaned up automatically when ksp goes out of scope","category":"section"},{"location":"guide/solvers/#Benefits-of-Reuse","page":"Linear Solvers","title":"Benefits of Reuse","text":"Performance: Avoids repeated factorization/preconditioner setup\nMemory: Single solver object instead of multiple temporary solvers\nConfiguration: Set PETSc options once","category":"section"},{"location":"guide/solvers/#In-Place-Operations","page":"Linear Solvers","title":"In-Place Operations","text":"For pre-allocated result vectors/matrices:\n\n# Vector solve\nx = zeros_like(b)\nldiv!(x, A, b)  # x = A \\ b (creates solver internally)\n\n# With reusable solver\nldiv!(ksp, x, b)  # Reuse solver\n\n# Matrix solve\nX = zeros_like(B)\nldiv!(X, A, B)  # Solve AX = B\nldiv!(ksp, X, B)  # With reusable solver","category":"section"},{"location":"guide/solvers/#Right-Division","page":"Linear Solvers","title":"Right Division","text":"Solve systems where the unknown is on the left:\n\n# Solve x^T A = b^T (equivalent to A^T x = b)\nx_adj = b' / A  # Returns adjoint vector\n\n# Solve XA = B\nX = B / A\n\n# Transpose: solve XA^T = B\nX = B / A'\n\nNote: B and X must be dense matrices.","category":"section"},{"location":"guide/solvers/#Configuring-Solvers","page":"Linear Solvers","title":"Configuring Solvers","text":"","category":"section"},{"location":"guide/solvers/#PETSc-Options","page":"Linear Solvers","title":"PETSc Options","text":"Control solver behavior via PETSc options:\n\n# Global configuration\npetsc_options_insert_string(\"-ksp_type gmres\")\npetsc_options_insert_string(\"-ksp_rtol 1e-8\")\npetsc_options_insert_string(\"-pc_type bjacobi\")\n\n# With prefix for specific solvers\n# First define a custom prefix type (advanced)\nstruct MyPrefix end\nSafePETSc.prefix(::Type{MyPrefix}) = \"my_\"\n\npetsc_options_insert_string(\"-my_ksp_type cg\")\nA = Mat_uniform(data; Prefix=MyPrefix)\nksp = KSP(A)  # Will use CG\n\nCommon KSP options:\n\n-ksp_type: KSP type (cg, gmres, bcgs, etc.)\n-ksp_rtol: Relative tolerance\n-ksp_atol: Absolute tolerance\n-ksp_max_it: Maximum iterations\n-pc_type: Preconditioner (jacobi, bjacobi, ilu, etc.)","category":"section"},{"location":"guide/solvers/#Monitoring-Convergence","page":"Linear Solvers","title":"Monitoring Convergence","text":"petsc_options_insert_string(\"-ksp_monitor\")\npetsc_options_insert_string(\"-ksp_converged_reason\")\n\nx = A \\ b\n# PETSc will print convergence information","category":"section"},{"location":"guide/solvers/#KSP-Types","page":"Linear Solvers","title":"KSP Types","text":"SafePETSc supports all PETSc KSP types. Common choices:","category":"section"},{"location":"guide/solvers/#Direct-Methods","page":"Linear Solvers","title":"Direct Methods","text":"# For small to medium problems\npetsc_options_insert_string(\"-ksp_type preonly -pc_type lu\")\nx = A \\ b","category":"section"},{"location":"guide/solvers/#Iterative-Methods","page":"Linear Solvers","title":"Iterative Methods","text":"# Conjugate Gradient (symmetric positive definite)\npetsc_options_insert_string(\"-ksp_type cg -pc_type jacobi\")\n\n# GMRES (general nonsymmetric)\npetsc_options_insert_string(\"-ksp_type gmres -ksp_gmres_restart 30\")\n\n# BiCGStab\npetsc_options_insert_string(\"-ksp_type bcgs\")","category":"section"},{"location":"guide/solvers/#Examples","page":"Linear Solvers","title":"Examples","text":"","category":"section"},{"location":"guide/solvers/#Basic-Linear-System","page":"Linear Solvers","title":"Basic Linear System","text":"using SafePETSc\nusing MPI\n\nSafePETSc.Init()\n\n# Create a simple system\nn = 100\nA_dense = zeros(n, n)\nfor i in 1:n\n    A_dense[i, i] = 2.0\n    if i > 1\n        A_dense[i, i-1] = -1.0\n    end\n    if i < n\n        A_dense[i, i+1] = -1.0\n    end\nend\n\nA = Mat_uniform(A_dense)\nb = Vec_uniform(ones(n))\n\n# Solve\nx = A \\ b\n\n# Check residual\nr = b - A * x\n# (In practice, use PETSc's built-in convergence monitoring)","category":"section"},{"location":"guide/solvers/#Iterative-KSP-with-Monitoring","page":"Linear Solvers","title":"Iterative KSP with Monitoring","text":"using SafePETSc\n\nSafePETSc.Init()\n\n# Configure solver\npetsc_options_insert_string(\"-ksp_type cg\")\npetsc_options_insert_string(\"-ksp_rtol 1e-10\")\npetsc_options_insert_string(\"-ksp_monitor\")\npetsc_options_insert_string(\"-pc_type jacobi\")\n\n# Build system (e.g., Laplacian)\nn = 1000\ndiag = Vec_uniform(2.0 * ones(n))\noff = Vec_uniform(-1.0 * ones(n-1))\nA = spdiagm(-1 => off, 0 => diag, 1 => off)\n\nb = Vec_uniform(ones(n))\n\n# Solve (will print iteration info)\nx = A \\ b","category":"section"},{"location":"guide/solvers/#Multiple-Solves","page":"Linear Solvers","title":"Multiple Solves","text":"using SafePETSc\n\nSafePETSc.Init()\n\n# System matrix\nA = Mat_uniform(...)\n\n# Create solver once\nksp = KSP(A)\n\n# Solve many RHS vectors\nfor i in 1:100\n    b = Vec_uniform(rhs_data[i])\n    x = zeros_like(b)\n    ldiv!(ksp, x, b)\n    results[i] = extract_result(x)\nend\n\n# KSP automatically cleaned up","category":"section"},{"location":"guide/solvers/#Block-Solves","page":"Linear Solvers","title":"Block Solves","text":"using SafePETSc\n\nSafePETSc.Init()\n\n# System matrix\nA = Mat_uniform(...)\n\n# Multiple right-hand sides as columns of a dense matrix\nB = Mat_uniform(rhs_matrix)  # Must be dense\n\n# Solve all systems at once\nX = A \\ B\n\n# Each column of X is a solution","category":"section"},{"location":"guide/solvers/#Performance-Tips","page":"Linear Solvers","title":"Performance Tips","text":"Reuse KSP Objects: Create KSP once for multiple solves\nChoose Appropriate Method: Direct for small problems, iterative for large\nTune Preconditioner: Can dramatically affect convergence\nMonitor Convergence: Use -ksp_monitor to tune parameters\nGPU Acceleration: Set PETSc options for GPU execution\n\n# GPU configuration example\npetsc_options_insert_string(\"-mat_type aijcusparse\")\npetsc_options_insert_string(\"-vec_type cuda\")","category":"section"},{"location":"guide/solvers/#KSP-Properties","page":"Linear Solvers","title":"KSP Properties","text":"Check solver dimensions:\n\nksp = KSP(A)\n\nm, n = size(ksp)  # Matrix dimensions\nm = size(ksp, 1)  # Rows\nn = size(ksp, 2)  # Columns","category":"section"},{"location":"guide/solvers/#Troubleshooting","page":"Linear Solvers","title":"Troubleshooting","text":"","category":"section"},{"location":"guide/solvers/#Convergence-Issues","page":"Linear Solvers","title":"Convergence Issues","text":"# Increase iterations\npetsc_options_insert_string(\"-ksp_max_it 10000\")\n\n# Relax tolerance\npetsc_options_insert_string(\"-ksp_rtol 1e-6\")\n\n# Try different solver/preconditioner\npetsc_options_insert_string(\"-ksp_type gmres -pc_type asm\")\n\n# View solver details\npetsc_options_insert_string(\"-ksp_view\")","category":"section"},{"location":"guide/solvers/#Memory-Issues","page":"Linear Solvers","title":"Memory Issues","text":"# Use iterative method instead of direct\npetsc_options_insert_string(\"-ksp_type cg\")\n\n# Reduce GMRES restart\npetsc_options_insert_string(\"-ksp_gmres_restart 10\")","category":"section"},{"location":"guide/solvers/#Assertion-Failures","page":"Linear Solvers","title":"Assertion Failures","text":"Ensure:\n\nMatrix is square for \\ operator\nPartitions match (A.rowpartition == b.rowpartition)\nSame prefix on all objects","category":"section"},{"location":"guide/solvers/#See-Also","page":"Linear Solvers","title":"See Also","text":"SafePETSc.KSP","category":"section"},{"location":"api/solvers/#Solvers-API-Reference","page":"Solvers","title":"Solvers API Reference","text":"Linear solver functionality in SafePETSc.","category":"section"},{"location":"api/solvers/#Type","page":"Solvers","title":"Type","text":"","category":"section"},{"location":"api/solvers/#Initialization","page":"Solvers","title":"Initialization","text":"","category":"section"},{"location":"api/solvers/#Linear-Solves","page":"Solvers","title":"Linear Solves","text":"","category":"section"},{"location":"api/solvers/#Direct-Solve","page":"Solvers","title":"Direct Solve","text":"# Vector RHS\nx = A \\ b                              # Solve Ax = b\nx = A' \\ b                             # Solve A^T x = b\n\n# Matrix RHS (must be dense)\nX = A \\ B                              # Solve AX = B\nX = A' \\ B                             # Solve A^T X = B","category":"section"},{"location":"api/solvers/#In-Place-Solve","page":"Solvers","title":"In-Place Solve","text":"# Vector RHS\nLinearAlgebra.ldiv!(x, A, b)           # x = A \\ b\nLinearAlgebra.ldiv!(ksp, x, b)         # Using reusable solver\n\n# Matrix RHS (must be dense)\nLinearAlgebra.ldiv!(X, A, B)           # X = A \\ B\nLinearAlgebra.ldiv!(ksp, X, B)         # Using reusable solver","category":"section"},{"location":"api/solvers/#Right-Division","page":"Solvers","title":"Right Division","text":"# Vector\nx_adj = b' / A                         # Solve x^T A = b^T\n\n# Matrix (must be dense)\nX = B / A                              # Solve XA = B\nX = B / A'                             # Solve XA^T = B","category":"section"},{"location":"api/solvers/#Properties","page":"Solvers","title":"Properties","text":"m, n = size(ksp)                       # KSP matrix dimensions","category":"section"},{"location":"api/solvers/#SafePETSc.KSP","page":"Solvers","title":"SafePETSc.KSP","text":"KSP{T,Prefix}\n\nA PETSc KSP (Krylov Subspace) linear solver with element type T and prefix type Prefix, managed by SafePETSc's reference counting system.\n\nKSP{T,Prefix} is actually a type alias for DRef{_KSP{T,Prefix}}, meaning solvers are automatically tracked across MPI ranks and destroyed collectively when all ranks release their references.\n\nKSP objects can be reused for multiple linear systems with the same matrix, avoiding the cost of repeated factorization or preconditioner setup.\n\nConstruction\n\nSee the KSP constructor for creating solver instances.\n\nUsage\n\nKSP solvers can be used implicitly via the backslash operator, or explicitly for reuse:\n\n# Implicit (creates and destroys solver internally)\nx = A \\ b\n\n# Explicit (reuse solver for multiple solves)\nksp = KSP(A)\nx1 = similar(b)\nx2 = similar(b)\nLinearAlgebra.ldiv!(ksp, x1, b1)  # First solve\nLinearAlgebra.ldiv!(ksp, x2, b2)  # Second solve with same matrix\n\nSee also: Mat, Vec, the KSP constructor\n\n\n\n\n\n","category":"type"},{"location":"api/solvers/#SafePETSc.Init","page":"Solvers","title":"SafePETSc.Init","text":"Init() -> nothing\n\nMPI Collective\n\nEnsure MPI and PETSc are initialized in the recommended order (MPI first, then PETSc). Safe to call multiple times. Does not register custom finalizers; rely on library defaults for shutdown (MPI.jl finalizes at exit; PETSc may remain initialized).\n\nSets up PETSc options for prefix types (MPIDENSE and MPIAIJ).\n\n\n\n\n\n","category":"function"},{"location":"api/solvers/#SafePETSc.Initialized","page":"Solvers","title":"SafePETSc.Initialized","text":"Initialized() -> Bool\n\nMPI Non-Collective\n\nReturn true if both MPI and PETSc are initialized. This is a simple conjunction of MPI.Initialized() and PETSc.initialized for the active PETSc library.\n\n\n\n\n\n","category":"function"},{"location":"api/solvers/#SafePETSc.petsc_options_insert_string","page":"Solvers","title":"SafePETSc.petsc_options_insert_string","text":"petsc_options_insert_string(options_string::String)\n\nMPI Collective\n\nInsert command-line style options into PETSc's global options database.\n\nExample: petsc_options_insert_string(\"-dense_mat_type mpidense\")\n\nThis sets options that will be used by PETSc objects created with the corresponding prefix. PETSc must already be initialized by the caller.\n\n\n\n\n\n","category":"function"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"This guide will help you get started with SafePETSc.jl for distributed parallel computing with MPI and PETSc.","category":"section"},{"location":"getting_started/#Prerequisites","page":"Getting Started","title":"Prerequisites","text":"SafePETSc requires:\n\nJulia 1.6 or later\nMPI installation (OpenMPI, MPICH, etc.)\nPETSc installation","category":"section"},{"location":"getting_started/#Running-with-MPI","page":"Getting Started","title":"Running with MPI","text":"SafePETSc programs must be run with MPI. Use the MPI.jl wrapper to ensure compatibility:\n\n# Run with 4 MPI processes\njulia -e 'using MPI; run(`$(MPI.mpiexec()) -n 4 $(Base.julia_cmd()) --project=. your_script.jl`)'\n\nThis ensures the correct MPI implementation and Julia executable are used.","category":"section"},{"location":"getting_started/#Using-System-MPI-on-HPC-Clusters","page":"Getting Started","title":"Using System MPI on HPC Clusters","text":"On HPC clusters, you typically need to use the cluster's native MPI library (not the one Julia ships with) for optimal performance and compatibility with the job scheduler. Here's how to configure this:","category":"section"},{"location":"getting_started/#Step-1:-Load-the-MPI-Module-(Shell-Command)","page":"Getting Started","title":"Step 1: Load the MPI Module (Shell Command)","text":"First, load your cluster's MPI module. This is a shell command (run in your terminal):\n\n# Example for clusters using the module system\nmodule load openmpi  # or module load mpich, module load intel-mpi, etc.\n\nCheck which MPI was loaded:\n\n# Shell command\nwhich mpiexec","category":"section"},{"location":"getting_started/#Step-2:-Configure-Julia-to-Use-System-MPI","page":"Getting Started","title":"Step 2: Configure Julia to Use System MPI","text":"You need to tell Julia's MPI.jl package to use the system MPI library instead of its bundled version. This is done using MPIPreferences.jl.\n\nRun this Julia code (in a Julia REPL or as a script):\n\n# Julia code - run this once per project\nusing MPIPreferences\nMPIPreferences.use_system_binary()\n\nAlternatively, if you want to specify the MPI library explicitly:\n\n# Julia code - specify the exact MPI library path\nusing MPIPreferences\nMPIPreferences.use_system_binary(\n    mpiexec = \"/path/to/your/mpiexec\",  # Get this from 'which mpiexec'\n    vendor = \"OpenMPI\"  # or \"MPICH\", \"IntelMPI\", etc.\n)","category":"section"},{"location":"getting_started/#Step-3:-Rebuild-MPI.jl","page":"Getting Started","title":"Step 3: Rebuild MPI.jl","text":"After configuring MPIPreferences, you must rebuild MPI.jl. Run this Julia code:\n\n# Julia code - rebuild MPI.jl to use the system library\nusing Pkg\nPkg.build(\"MPI\"; verbose=true)","category":"section"},{"location":"getting_started/#Step-4:-Verify-the-Configuration","page":"Getting Started","title":"Step 4: Verify the Configuration","text":"Check that MPI.jl is now using the system MPI. Run this Julia code:\n\n# Julia code - verify MPI configuration\nusing MPI\nMPI.versioninfo()\n\nYou should see your cluster's MPI library listed (e.g., OpenMPI 4.1.x, not MPItrampoline).","category":"section"},{"location":"getting_started/#Step-5:-Run-Your-Code","page":"Getting Started","title":"Step 5: Run Your Code","text":"Now you can run your SafePETSc code. On clusters, you typically use the cluster's job scheduler:\n\n# Shell command - example SLURM job submission\nsbatch my_job.sh\n\nExample my_job.sh script:\n\n#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=16\n#SBATCH --time=1:00:00\n\n# Load MPI module\nmodule load openmpi\n\n# Run Julia with MPI\njulia -e 'using MPI; run(`$(MPI.mpiexec()) -n 32 $(Base.julia_cmd()) --project=. my_script.jl`)'\n\nOr for PBS/Torque:\n\n#!/bin/bash\n#PBS -l nodes=2:ppn=16\n#PBS -l walltime=1:00:00\n\ncd $PBS_O_WORKDIR\nmodule load openmpi\n\njulia -e 'using MPI; run(`$(MPI.mpiexec()) -n 32 $(Base.julia_cmd()) --project=. my_script.jl`)'","category":"section"},{"location":"getting_started/#Important-Notes","page":"Getting Started","title":"Important Notes","text":"One-time setup: Steps 1-3 only need to be done once per project/environment\nModule loading: You must load the MPI module in your job scripts (Step 1) every time you submit a job\nConsistency: Use the same MPI library that PETSc was built against on your cluster\nProject-specific: The MPI configuration is stored in your project's LocalPreferences.toml file","category":"section"},{"location":"getting_started/#Basic-Workflow","page":"Getting Started","title":"Basic Workflow","text":"","category":"section"},{"location":"getting_started/#1.-Initialize","page":"Getting Started","title":"1. Initialize","text":"Always start by initializing MPI and PETSc:\n\nusing SafePETSc\nusing MPI\n\nSafePETSc.Init()\n\nThis ensures both MPI and PETSc are properly initialized.","category":"section"},{"location":"getting_started/#2.-Create-Distributed-Objects","page":"Getting Started","title":"2. Create Distributed Objects","text":"SafePETSc provides two main patterns for creating distributed objects:","category":"section"},{"location":"getting_started/#Uniform-Distribution","page":"Getting Started","title":"Uniform Distribution","text":"Use when all ranks have the same data:\n\n# Same matrix on all ranks\nA = Mat_uniform([1.0 2.0; 3.0 4.0])\n\n# Same vector on all ranks\nv = Vec_uniform([1.0, 2.0])","category":"section"},{"location":"getting_started/#Sum-Distribution","page":"Getting Started","title":"Sum Distribution","text":"Use when ranks contribute different sparse data:\n\nusing SparseArrays\n\n# Each rank contributes sparse entries\n# Entries are summed across ranks\nA = Mat_sum(sparse([1], [1], [rank_value], 10, 10))\nv = Vec_sum(sparsevec([rank_id], [rank_value], 10))","category":"section"},{"location":"getting_started/#3.-Perform-Operations","page":"Getting Started","title":"3. Perform Operations","text":"# Matrix-vector multiplication\ny = A * v\n\n# In-place operations\ny .= A * v .+ 1.0\n\n# Linear solve\nx = A \\ b\n\n# Matrix operations\nC = A * B\nD = A'  # Transpose","category":"section"},{"location":"getting_started/#4.-Cleanup","page":"Getting Started","title":"4. Cleanup","text":"Objects are automatically cleaned up when they go out of scope. You can explicitly trigger cleanup:\n\nSafeMPI.check_and_destroy!()","category":"section"},{"location":"getting_started/#Complete-Example","page":"Getting Started","title":"Complete Example","text":"Here's a complete example that solves a linear system:\n\nusing SafePETSc\nusing MPI\nusing LinearAlgebra\n\n# Initialize\nSafePETSc.Init()\n\n# Get MPI info\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nnranks = MPI.Comm_size(comm)\n\n# Create a simple 2D Laplacian matrix (uniform on all ranks)\nn = 100\nA_dense = zeros(n, n)\nfor i in 1:n\n    A_dense[i, i] = 2.0\n    if i > 1\n        A_dense[i, i-1] = -1.0\n    end\n    if i < n\n        A_dense[i, i+1] = -1.0\n    end\nend\n\n# Create distributed PETSc matrix\nA = Mat_uniform(A_dense)\n\n# Create right-hand side\nb = Vec_uniform(ones(n))\n\n# Solve the system\nx = A \\ b\n\n# Print result (only on rank 0)\nprintln(io0(), \"System solved successfully\")\nprintln(io0(), \"Solution norm: \", norm(x))\n\n# Explicit cleanup (optional - happens automatically at scope exit)\nSafeMPI.check_and_destroy!()","category":"section"},{"location":"getting_started/#Running-the-Example","page":"Getting Started","title":"Running the Example","text":"Save the above code as example.jl and run:\n\njulia -e 'using MPI; run(`$(MPI.mpiexec()) -n 4 $(Base.julia_cmd()) --project=. example.jl`)'","category":"section"},{"location":"getting_started/#Next-Steps","page":"Getting Started","title":"Next Steps","text":"Learn about Distributed Reference Management\nExplore Vectors\nUnderstand Matrices\nUse Linear Solvers","category":"section"},{"location":"api/vectors/#Vectors-API-Reference","page":"Vectors","title":"Vectors API Reference","text":"Distributed vector operations in SafePETSc.","category":"section"},{"location":"api/vectors/#Type","page":"Vectors","title":"Type","text":"","category":"section"},{"location":"api/vectors/#Prefix-Types","page":"Vectors","title":"Prefix Types","text":"The Prefix type parameter controls PETSc configuration for vectors. See the Matrices API Reference for details on MPIAIJ, MPIDENSE, and prefix.","category":"section"},{"location":"api/vectors/#Constructors","page":"Vectors","title":"Constructors","text":"","category":"section"},{"location":"api/vectors/#Helper-Constructors","page":"Vectors","title":"Helper Constructors","text":"","category":"section"},{"location":"api/vectors/#Concatenation","page":"Vectors","title":"Concatenation","text":"Vectors can be concatenated using the same functions as matrices. See the Matrices API Reference for vcat and hcat.\n\nNote: Concatenating vectors returns Mat{T,Prefix} objects.","category":"section"},{"location":"api/vectors/#Partitioning","page":"Vectors","title":"Partitioning","text":"","category":"section"},{"location":"api/vectors/#Vector-Pooling","page":"Vectors","title":"Vector Pooling","text":"","category":"section"},{"location":"api/vectors/#Conversion-and-Display","page":"Vectors","title":"Conversion and Display","text":"Convert distributed vectors to Julia arrays for inspection and display:\n\nDisplay methods (automatically used by println, display, etc.):\n\nshow(io::IO, v::Vec) - Display vector contents\nshow(io::IO, mime::MIME, v::Vec) - Display with MIME type support","category":"section"},{"location":"api/vectors/#Utilities","page":"Vectors","title":"Utilities","text":"","category":"section"},{"location":"api/vectors/#Row-wise-Operations","page":"Vectors","title":"Row-wise Operations","text":"","category":"section"},{"location":"api/vectors/#Indexing","page":"Vectors","title":"Indexing","text":"Non-collective element and range access:","category":"section"},{"location":"api/vectors/#Operations","page":"Vectors","title":"Operations","text":"","category":"section"},{"location":"api/vectors/#Arithmetic","page":"Vectors","title":"Arithmetic","text":"Vectors support standard Julia arithmetic operations via broadcasting:\n\ny = x .+ 1.0        # Element-wise addition\ny = 2.0 .* x        # Scaling\nz = x .+ y          # Vector addition\ny .= x .+ 1.0       # In-place operation\n\nStandard operators are also overloaded:\n\nz = x + y           # Addition\nz = x - y           # Subtraction\nz = -x              # Negation","category":"section"},{"location":"api/vectors/#Linear-Algebra","page":"Vectors","title":"Linear Algebra","text":"y = A * x                              # Matrix-vector multiplication\nLinearAlgebra.mul!(y, A, x)            # In-place multiplication\nw = v' * A                             # Adjoint-vector times matrix\nLinearAlgebra.mul!(w, v', A)           # In-place","category":"section"},{"location":"api/vectors/#Properties","page":"Vectors","title":"Properties","text":"T = eltype(v)                          # Element type\nn = length(v)                          # Vector length\nn = size(v, 1)                         # Size in dimension 1","category":"section"},{"location":"api/vectors/#SafePETSc.Vec","page":"Vectors","title":"SafePETSc.Vec","text":"Vec{T,Prefix}\n\nA distributed PETSc vector with element type T and prefix type Prefix, managed by SafePETSc's reference counting system.\n\nVec{T,Prefix} is a type alias for DRef{_Vec{T,Prefix}} and is released collectively when all ranks release their references. By default, released PETSc vectors are returned to an internal pool for reuse rather than destroyed immediately. To force destruction instead of pooling, set ENABLE_VEC_POOL[] = false, or call clear_vec_pool!() to free pooled vectors.\n\nConstruction\n\nUse Vec_uniform or Vec_sum to create distributed vectors:\n\n# Create from uniform data (same on all ranks)\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\n\n# Create from sparse contributions (summed across ranks)\nusing SparseArrays\nv = Vec_sum(sparsevec([1, 3], [1.0, 3.0], 4))\n\nOperations\n\nVectors support standard arithmetic operations via broadcasting:\n\ny = x .+ 1.0        # Element-wise addition\ny .= 2.0 .* x       # In-place scaling\nz = x .+ y          # Vector addition\n\nMatrix-vector multiplication:\n\ny = A * x           # Matrix-vector product\nLinearAlgebra.mul!(y, A, x)  # In-place version\n\nSee also: Vec_uniform, Vec_sum, Mat, zeros_like, ENABLE_VEC_POOL, clear_vec_pool!\n\n\n\n\n\n","category":"type"},{"location":"api/vectors/#SafePETSc.Vec_uniform","page":"Vectors","title":"SafePETSc.Vec_uniform","text":"Vec_uniform(v::Vector{T}; row_partition=default_row_partition(length(v), MPI.Comm_size(MPI.COMM_WORLD)), Prefix::Type=MPIAIJ) -> Vec{T,Prefix}\n\nMPI Collective\n\nCreate a distributed PETSc vector from a Julia vector, asserting uniform distribution across ranks (on MPI.COMM_WORLD).\n\nv::Vector{T} must be identical on all ranks (mpi_uniform).\nrow_partition is a Vector{Int} of length nranks+1 with 1-based inclusive starts.\nPrefix is a type parameter for VecSetOptionsPrefix for PETSc options (default: MPIAIJ).\nReturns a Vec{T,Prefix} (aka DRef{_Vec{T,Prefix}}) managed collectively; by default vectors are returned to a reuse pool when released, not immediately destroyed. Use ENABLE_VEC_POOL[] = false or clear_vec_pool!() to force destruction.\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#SafePETSc.Vec_sum","page":"Vectors","title":"SafePETSc.Vec_sum","text":"Vec_sum(v::SparseVector{T}; row_partition=default_row_partition(length(v), MPI.Comm_size(MPI.COMM_WORLD)), Prefix::Type=MPIAIJ, own_rank_only=false) -> Vec{T,Prefix}\n\nMPI Collective\n\nCreate a distributed PETSc vector by summing sparse vectors across ranks (on MPI.COMM_WORLD).\n\nv::SparseVector{T} can differ across ranks; nonzeros are summed across all ranks.\nrow_partition is a Vector{Int} of length nranks+1 with 1-based inclusive starts.\nPrefix is a type parameter for VecSetOptionsPrefix for PETSc options (default: MPIAIJ).\nown_rank_only::Bool (default=false): if true, asserts that all nonzero indices fall within this rank's row partition.\nReturns a Vec{T,Prefix} managed collectively; by default vectors are returned to a reuse pool when released, not immediately destroyed. Use ENABLE_VEC_POOL[] = false or clear_vec_pool!() to force destruction.\n\nUses VecSetValues with ADD_VALUES to sum contributions across ranks.\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#SafePETSc.zeros_like","page":"Vectors","title":"SafePETSc.zeros_like","text":"zeros_like(x::Vec{T,Prefix}; T2::Type{S}=T, Prefix2::Type=Prefix) -> Vec{S,Prefix2}\n\nMPI Collective\n\nCreate a new distributed vector with the same size and partition as x, filled with zeros.\n\nArguments\n\nx: Template vector to match size and partition\nT2: Element type of the result (defaults to same type as x)\nPrefix2: Prefix type (defaults to same prefix as x)\n\nSee also: ones_like, fill_like, Vec_uniform\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#SafePETSc.ones_like","page":"Vectors","title":"SafePETSc.ones_like","text":"ones_like(x::Vec{T,Prefix}; T2::Type{S}=T, Prefix2::Type=Prefix) -> Vec{S,Prefix2}\n\nMPI Collective\n\nCreate a new distributed vector with the same size and partition as x, filled with ones.\n\nArguments\n\nx: Template vector to match size and partition\nT2: Element type of the result (defaults to same type as x)\nPrefix2: Prefix type (defaults to same prefix as x)\n\nSee also: zeros_like, fill_like, Vec_uniform\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#SafePETSc.fill_like","page":"Vectors","title":"SafePETSc.fill_like","text":"fill_like(x::Vec{T,Prefix}, val; T2::Type{S}=typeof(val), Prefix2::Type=Prefix) -> Vec{S,Prefix2}\n\nMPI Collective\n\nCreate a new distributed vector with the same size and partition as x, filled with val.\n\nArguments\n\nx: Template vector to match size and partition\nval: Value to fill the vector with\nT2: Element type of the result (defaults to type of val)\nPrefix2: Prefix type (defaults to same prefix as x)\n\nExample\n\ny = fill_like(x, 3.14)  # Create a vector like x, filled with 3.14\n\nSee also: zeros_like, ones_like, Vec_uniform\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#SafePETSc.default_row_partition","page":"Vectors","title":"SafePETSc.default_row_partition","text":"default_row_partition(n::Int, nranks::Int) -> Vector{Int}\n\nMPI Non-Collective\n\nCreate a default row partition that divides n rows equally among nranks.\n\nReturns a Vector{Int} of length nranks+1 where partition[i] is the start row (1-indexed) for rank i-1.\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#SafePETSc.ENABLE_VEC_POOL","page":"Vectors","title":"SafePETSc.ENABLE_VEC_POOL","text":"ENABLE_VEC_POOL\n\nGlobal flag to enable/disable vector pooling. Set to false to disable pooling.\n\n\n\n\n\n","category":"constant"},{"location":"api/vectors/#SafePETSc.clear_vec_pool!","page":"Vectors","title":"SafePETSc.clear_vec_pool!","text":"clear_vec_pool!()\n\nMPI Non-Collective\n\nClear all vectors from the pool, destroying them immediately. Useful for testing or explicit memory management.\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#SafePETSc.get_vec_pool_stats","page":"Vectors","title":"SafePETSc.get_vec_pool_stats","text":"get_vec_pool_stats() -> Dict\n\nMPI Non-Collective\n\nReturn statistics about the current vector pool state. Returns a dictionary with keys (nglobal, prefix, type) => count.\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#Base.Vector-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Vec{T, Prefix}} where {T, Prefix}}","page":"Vectors","title":"Base.Vector","text":"Vector(x::Vec{T,Prefix}) -> Vector{T}\n\nMPI Collective\n\nConvert a distributed PETSc Vec to a Julia Vector by gathering all data to all ranks. This is a collective operation - all ranks must call it and will receive the complete vector.\n\nThis is primarily used for display purposes or small vectors. For large vectors, this operation can be expensive as it gathers all data to all ranks.\n\n\n\n\n\nVector(vt::LinearAlgebra.Adjoint{T, <:Vec{T}}) -> LinearAlgebra.Adjoint{T, Vector{T}}\n\nMPI Collective\n\nConvert an adjoint of a distributed PETSc Vec to an adjoint Julia Vector. Equivalent to Vector(parent(vt))'.\n\nThis is a collective operation - all ranks must call it and will receive the complete adjoint vector.\n\n\n\n\n\n","category":"method"},{"location":"api/vectors/#SafePETSc.io0","page":"Vectors","title":"SafePETSc.io0","text":"io0(io=stdout; r::Set{Int}=Set{Int}([0]), dn=devnull)\n\nMPI Non-Collective\n\nReturn io if the current rank is in r, otherwise return dn.\n\nThis is useful for printing output only on specific ranks to avoid duplicate output.\n\nParameters\n\nio: The IO stream to use (default: stdout)\nr: Set of ranks that should produce output (default: Set{Int}([0]))\ndn: The IO stream to return for non-selected ranks (default: devnull)\n\nExamples\n\n# Print only on rank 0 (default)\nprintln(io0(), \"This prints only on rank 0\")\n\n# Print only on rank 2\nprintln(io0(r=Set([2])), \"This prints only on rank 2\")\n\n# Print on ranks 0 and 3\nprintln(io0(r=Set([0, 3])), \"This prints on ranks 0 and 3\")\n\n# Write to file only on rank 1\nopen(\"output.txt\", \"w\") do f\n    println(io0(f; r=Set([1])), \"This writes only on rank 1\")\nend\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#SafePETSc.own_row-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Vec{T, Prefix}} where {T, Prefix}}","page":"Vectors","title":"SafePETSc.own_row","text":"own_row(v::Vec{T,Prefix}) -> UnitRange{Int}\n\nMPI Non-Collective\n\nReturn the range of indices owned by the current rank for vector v.\n\nExample\n\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\nrange = own_row(v)  # e.g., 1:2 on rank 0\n\n\n\n\n\nown_row(A::Mat{T,Prefix}) -> UnitRange{Int}\n\nMPI Non-Collective\n\nReturn the range of row indices owned by the current rank for matrix A.\n\nExample\n\nA = Mat_uniform([1.0 2.0; 3.0 4.0; 5.0 6.0; 7.0 8.0])\nrange = own_row(A)  # e.g., 1:2 on rank 0\n\n\n\n\n\n","category":"method"},{"location":"api/vectors/#SafePETSc.map_rows","page":"Vectors","title":"SafePETSc.map_rows","text":"map_rows(f::Function, A::Union{Vec{T,Prefix},Mat{T,Prefix}}...; col_partition=nothing) -> Union{Vec{T,MPIDENSE},Mat{T,MPIDENSE}}\n\nMPI Collective\n\nApply a function f to corresponding rows across distributed PETSc vectors and matrices.\n\nSimilar to the native Julia pattern vcat((f.((eachrow.(A))...))...), but works with distributed PETSc objects. The function f is applied row-wise to each input, and the results are concatenated into a new distributed vector or matrix.\n\nArguments\n\nf::Function: Function to apply to each row. Should accept as many arguments as there are inputs.\nA...::Union{Vec{T,Prefix},Mat{T,Prefix}}: One or more distributed vectors or matrices. All inputs must have the same number of rows and compatible row partitions.\ncol_partition::Union{Vector{Int},Nothing}: Column partition for result matrix (default: use defaultrowpartition). Only used when f returns an adjoint vector (creating a matrix).\n\nReturn value\n\nAlways returns Vec{T,MPIDENSE} or Mat{T,MPIDENSE} (dense format). The return type depends on what f returns:\n\nIf f returns a scalar or Julia Vector → returns a Vec{T,MPIDENSE}\nIf f returns an adjoint Julia Vector (row vector) → returns a Mat{T,MPIDENSE}\n\nSize behavior\n\nIf inputs have m rows and f returns:\n\nA scalar or adjoint vector → result has m rows\nAn n-dimensional vector → result has m*n rows\n\nExamples\n\n# Example 1: Sum rows of a matrix\nB = Mat_uniform(randn(5, 3))\nsums = map_rows(sum, B)  # Returns Vec{Float64,MPIDENSE} with 5 elements\n\n# Example 2: Compute [sum, product] for each row (returns matrix)\nstats = map_rows(x -> [sum(x), prod(x)]', B)  # Returns 5×2 Mat{Float64,MPIDENSE}\n\n# Example 3: Combine matrix and vector row-wise\nC = Vec_uniform(randn(5))\ncombined = map_rows((x, y) -> [sum(x), prod(x), y[1]]', B, C)  # Returns 5×3 Mat{Float64,MPIDENSE}\n\nImplementation notes\n\nThis is a collective operation; all ranks must call it with compatible arguments\nThe function f is assumed to be homogeneous (always returns the same type of output)\nFor vectors, f receives a scalar value per row\nFor matrices, f receives a view of the row (similar to eachrow)\nThe result always uses MPIDENSE prefix regardless of input prefix\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Vec{T, Prefix}} where {T, Prefix}, Int64}","page":"Vectors","title":"Base.getindex","text":"Base.getindex(v::Vec{T}, i::Int) -> T\n\nMPI Non-Collective\n\nGet the value at index i from a distributed vector.\n\nThe index i must be wholly contained in the current rank's ownership range. If not, the function will abort with an error message and stack trace.\n\nExample\n\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\n# On rank that owns index 2:\nval = v[2]  # Returns 2.0\n\n\n\n\n\n","category":"method"},{"location":"api/vectors/#Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Vec{T, Prefix}} where {T, Prefix}, UnitRange{Int64}}","page":"Vectors","title":"Base.getindex","text":"Base.getindex(v::Vec{T}, range::UnitRange{Int}) -> Vector{T}\n\nMPI Non-Collective\n\nExtract a contiguous range of values from a distributed vector.\n\nThe range must be wholly contained in the current rank's ownership range. If not, the function will abort with an error message and stack trace.\n\nExample\n\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\n# On rank that owns indices 2:3:\nvals = v[2:3]  # Returns [2.0, 3.0]\n\n\n\n\n\n","category":"method"},{"location":"guide/mpi_programming/#MPI-Programming","page":"MPI Programming","title":"MPI Programming","text":"This chapter covers important considerations when writing MPI programs using SafePETSc, particularly regarding error handling and collective operations.","category":"section"},{"location":"guide/mpi_programming/#The-Challenge-of-Exceptions-in-MPI","page":"MPI Programming","title":"The Challenge of Exceptions in MPI","text":"MPI programming is fundamentally incompatible with traditional exception handling. When you write parallel code that runs across multiple MPI ranks, standard assertions or exceptions can cause serious problems:\n\nThe Problem: If one rank throws an exception while others continue execution, the MPI cluster becomes desynchronized and will hang. For example:\n\n# DANGEROUS: Don't do this in MPI code!\nusing SafePETSc\nSafePETSc.Init()\n\nx = Vec_uniform([1.0, 2.0, NaN, 4.0])  # NaN only on some ranks\n\n# This will hang! Some ranks will pass, others will fail\n@assert all(isfinite.(Vector(x)))  # ❌ Causes hang if ranks disagree\n\nIn the example above, if some ranks have finite values but others have NaN, some ranks will assert while others continue. The MPI cluster becomes desynchronized and will deadlock.","category":"section"},{"location":"guide/mpi_programming/#Safe-Exception-Handling","page":"MPI Programming","title":"Safe Exception Handling","text":"To handle errors safely in MPI programs, exceptions must be collective operations that either fail on all ranks simultaneously or pass on all ranks. SafePETSc provides several tools for this.","category":"section"},{"location":"guide/mpi_programming/#Using-@mpiassert-for-Collective-Assertions","page":"MPI Programming","title":"Using @mpiassert for Collective Assertions","text":"The SafeMPI.@mpiassert macro provides a collective assertion mechanism:\n\nusing SafePETSc\nusing SafePETSc.SafeMPI\nSafePETSc.Init()\n\nx = Vec_uniform([1.0, 2.0, 3.0, 4.0])\n\n# Safe: All ranks check together\nSafeMPI.@mpiassert all(isfinite.(Vector(x))) \"Vector contains non-finite values\"\n\nHow @mpiassert works:\n\nEach rank evaluates the condition locally\nAll ranks communicate to determine if ANY rank failed\nIf any rank's condition is false, ALL ranks throw an error simultaneously\nIf all ranks' conditions are true, ALL ranks continue\n\nImportant: @mpiassert is a collective operation and therefore slower than regular assertions. Use it only when necessary for correctness in MPI contexts.","category":"section"},{"location":"guide/mpi_programming/#Using-mpi_any-for-Conditional-Logic","page":"MPI Programming","title":"Using mpi_any for Conditional Logic","text":"When you need to make decisions based on conditions that might differ across ranks, use mpi_any:\n\nusing SafePETSc\nusing SafePETSc.SafeMPI\nSafePETSc.Init()\n\n# Each rank computes some local condition\nlocal_has_error = some_local_check()\n\n# Collective operation: true if ANY rank has an error\nany_rank_has_error = mpi_any(local_has_error)\n\nif any_rank_has_error\n    # All ranks execute this branch together\n    println(io0(), \"Error detected on at least one rank\")\n    # Handle error collectively\nelse\n    # All ranks execute this branch together\n    println(io0(), \"All ranks are healthy\")\nend\n\nThis ensures all ranks make the same decision, preventing desynchronization.\n\nSee the SafeMPI API Reference for more details.","category":"section"},{"location":"guide/mpi_programming/#Using-mpi_uniform-to-Verify-Consistency","page":"MPI Programming","title":"Using mpi_uniform to Verify Consistency","text":"The mpi_uniform function checks whether a value is identical across all ranks:\n\nusing SafePETSc\nusing SafePETSc.SafeMPI\nSafePETSc.Init()\n\n# Create a matrix that should be the same on all ranks\nA = [1.0 2.0; 3.0 4.0]\n\n# Verify it's actually uniform across all ranks\nSafeMPI.@mpiassert mpi_uniform(A) \"Matrix A is not uniform across ranks\"\n\n# Safe to use A as a uniform matrix\nA_petsc = Mat_uniform(A)\n\nThis is particularly useful for debugging distributed algorithms where you expect certain values to be synchronized.\n\nSee the SafeMPI API Reference for more details.","category":"section"},{"location":"guide/mpi_programming/#Best-Practices","page":"MPI Programming","title":"Best Practices","text":"Never use standard @assert or throw in MPI code unless you are certain all ranks will agree on the outcome\nUse @mpiassert for correctness checks that involve distributed data:\nSafeMPI.@mpiassert size(A) == size(B) \"Matrix dimensions must match\"\nUse mpi_any for error detection when local conditions might differ:\nif mpi_any(local_error_condition)\n    # Handle error collectively on all ranks\nend\nUse mpi_uniform to verify assumptions about distributed data:\nSafeMPI.@mpiassert mpi_uniform(config) \"Configuration must be uniform\"\nRemember that collective operations are slow - use them judiciously. They require communication between all ranks, so they can impact performance if used excessively.","category":"section"},{"location":"guide/mpi_programming/#Example:-Safe-Error-Handling-Pattern","page":"MPI Programming","title":"Example: Safe Error Handling Pattern","text":"Here's a complete example showing safe error handling in an MPI context:\n\nusing SafePETSc\nusing SafePETSc.SafeMPI\nSafePETSc.Init()\n\nfunction safe_computation(x::Vec)\n    # Convert to local array for checking\n    x_local = Vector(x)\n\n    # Check for problems locally\n    local_has_nan = any(isnan, x_local)\n    local_has_inf = any(isinf, x_local)\n\n    # Collective check: any rank has problems?\n    if mpi_any(local_has_nan)\n        error(\"NaN detected in vector on at least one rank\")\n    end\n\n    if mpi_any(local_has_inf)\n        error(\"Inf detected in vector on at least one rank\")\n    end\n\n    # All ranks confirmed data is good, proceed with computation\n    result = x .* 2.0\n    return result\nend\n\n# Usage\nx = Vec_uniform([1.0, 2.0, 3.0, 4.0])\ny = safe_computation(x)  # Safe: all ranks execute together","category":"section"},{"location":"guide/mpi_programming/#Common-Pitfalls-to-Avoid","page":"MPI Programming","title":"Common Pitfalls to Avoid","text":"","category":"section"},{"location":"guide/mpi_programming/#Pitfall-1:-Rank-Dependent-Assertions","page":"MPI Programming","title":"Pitfall 1: Rank-Dependent Assertions","text":"# ❌ WRONG: Will hang if condition differs by rank\nif MPI.Comm_rank(MPI.COMM_WORLD) == 0\n    @assert check_something()  # Only rank 0 might assert!\nend\n\n# ✓ CORRECT: Use collective operations\nlocal_check = (MPI.Comm_rank(MPI.COMM_WORLD) == 0) ? check_something() : true\nSafeMPI.@mpiassert local_check \"Check failed on rank 0\"","category":"section"},{"location":"guide/mpi_programming/#Pitfall-2:-File-I/O-Errors","page":"MPI Programming","title":"Pitfall 2: File I/O Errors","text":"# ❌ WRONG: File might exist on some ranks but not others\n@assert isfile(\"config.txt\")  # Might differ by rank!\n\n# ✓ CORRECT: Use collective check\nSafeMPI.@mpiassert isfile(\"config.txt\") \"config.txt not found\"","category":"section"},{"location":"guide/mpi_programming/#Pitfall-3:-Floating-Point-Comparisons","page":"MPI Programming","title":"Pitfall 3: Floating-Point Comparisons","text":"# ❌ WRONG: Floating-point round-off might differ by rank\n@assert computed_value ≈ expected_value\n\n# ✓ CORRECT: Use collective assertion\nSafeMPI.@mpiassert computed_value ≈ expected_value \"Value mismatch detected\"","category":"section"},{"location":"guide/mpi_programming/#Summary","page":"MPI Programming","title":"Summary","text":"MPI programming requires careful handling of exceptions and error conditions:\n\nUse @mpiassert for collective assertions that must pass or fail on all ranks together\nUse mpi_any to make collective decisions based on local conditions\nUse mpi_uniform to verify data consistency across ranks\nNever use standard assertions or exceptions that might execute differently on different ranks\nRemember that collective operations have performance costs - use them wisely but don't hesitate to use them for correctness\n\nBy following these patterns, you can write robust MPI programs that won't hang or deadlock due to desynchronized exception handling.","category":"section"},{"location":"api/matrices/#Matrices-API-Reference","page":"Matrices","title":"Matrices API Reference","text":"Distributed matrix operations in SafePETSc.","category":"section"},{"location":"api/matrices/#Type","page":"Matrices","title":"Type","text":"","category":"section"},{"location":"api/matrices/#Prefix-Types","page":"Matrices","title":"Prefix Types","text":"The Prefix type parameter determines matrix storage format and PETSc configuration:","category":"section"},{"location":"api/matrices/#Constructors","page":"Matrices","title":"Constructors","text":"","category":"section"},{"location":"api/matrices/#Concatenation","page":"Matrices","title":"Concatenation","text":"","category":"section"},{"location":"api/matrices/#Sparse-Diagonal-Matrices","page":"Matrices","title":"Sparse Diagonal Matrices","text":"","category":"section"},{"location":"api/matrices/#Conversion-and-Display","page":"Matrices","title":"Conversion and Display","text":"Convert distributed matrices to Julia arrays for inspection and display:\n\nDisplay methods (automatically used by println, display, etc.):\n\nshow(io::IO, A::Mat) - Display matrix contents (uses dense or sparse format based on type)\nshow(io::IO, mime::MIME, A::Mat) - Display with MIME type support","category":"section"},{"location":"api/matrices/#Utilities","page":"Matrices","title":"Utilities","text":"","category":"section"},{"location":"api/matrices/#Row-wise-Operations","page":"Matrices","title":"Row-wise Operations","text":"See map_rows in the Vectors API - works with both vectors and matrices.","category":"section"},{"location":"api/matrices/#Indexing","page":"Matrices","title":"Indexing","text":"Non-collective element and range access:","category":"section"},{"location":"api/matrices/#Operations","page":"Matrices","title":"Operations","text":"","category":"section"},{"location":"api/matrices/#Linear-Algebra","page":"Matrices","title":"Linear Algebra","text":"# Matrix-vector multiplication\ny = A * x\nLinearAlgebra.mul!(y, A, x)\n\n# Matrix-matrix multiplication\nC = A * B\nLinearAlgebra.mul!(C, A, B)\n\n# Transpose\nB = A'\nB = Mat(A')                            # Materialize transpose\nLinearAlgebra.transpose!(B, A)         # In-place transpose\n\n# Adjoint-vector multiplication\nw = v' * A\nLinearAlgebra.mul!(w, v', A)","category":"section"},{"location":"api/matrices/#Properties","page":"Matrices","title":"Properties","text":"T = eltype(A)                          # Element type\nm, n = size(A)                         # Dimensions\nm = size(A, 1)                         # Rows\nn = size(A, 2)                         # Columns","category":"section"},{"location":"api/matrices/#Iteration","page":"Matrices","title":"Iteration","text":"# Iterate over rows (works for both dense and sparse matrices)\nfor row in eachrow(A)\n    # For dense (MPIDENSE): row is a view of the matrix row\n    # For sparse (MPIAIJ): row is a SparseVector efficiently preserving sparsity\n    process(row)\nend","category":"section"},{"location":"api/matrices/#Block-Matrix-Products","page":"Matrices","title":"Block Matrix Products","text":"","category":"section"},{"location":"api/matrices/#SafePETSc.Mat","page":"Matrices","title":"SafePETSc.Mat","text":"Mat{T,Prefix}\n\nA distributed PETSc matrix with element type T and prefix type Prefix, managed by SafePETSc's reference counting system.\n\nMat{T,Prefix} is actually a type alias for DRef{_Mat{T,Prefix}}, meaning matrices are automatically tracked across MPI ranks and destroyed collectively when all ranks release their references.\n\nConstruction\n\nUse Mat_uniform or Mat_sum to create distributed matrices:\n\n# Create from uniform data (same on all ranks)\nA = Mat_uniform([1.0 2.0; 3.0 4.0])\n\n# Create from sparse contributions (summed across ranks)\nusing SparseArrays\nA = Mat_sum(sparse([1, 2], [1, 2], [1.0, 4.0], 2, 2))\n\nOperations\n\nMatrices support standard linear algebra operations:\n\n# Matrix-vector multiplication\ny = A * x\n\n# Matrix-matrix multiplication\nC = A * B\n\n# Matrix transpose\nB = A'\nB = Mat(A')  # Materialize transpose\n\n# Linear solve\nx = A \\ b\n\n# Concatenation\nC = vcat(A, B)  # or cat(A, B; dims=1)\nD = hcat(A, B)  # or cat(A, B; dims=2)\nE = blockdiag(A, B)\n\n# Diagonal matrix from vectors\nusing SparseArrays\nA = spdiagm(0 => diag_vec, 1 => upper_diag)\n\nSee also: Mat_uniform, Mat_sum, Vec, KSP\n\n\n\n\n\n","category":"type"},{"location":"api/matrices/#SafePETSc.MPIAIJ","page":"Matrices","title":"SafePETSc.MPIAIJ","text":"MPIAIJ\n\nPrefix type for sparse matrices and general vectors (default).\n\nString Prefix\n\nThe string prefix is \"MPIAIJ_\", which is prepended to PETSc option names.\n\nDefault PETSc Types\n\nMatrices: mpiaij (MPI sparse matrix, compressed row storage)\nVectors: mpi (standard MPI vector)\n\nUsage\n\nUse MPIAIJ for:\n\nSparse matrices with few nonzeros per row\nMemory-efficient storage of large sparse systems\nIterative solvers and sparse linear algebra\nGeneral-purpose vector operations\n\nThis is the default prefix type when not specified.\n\nExample\n\n# Create sparse matrix (MPIAIJ is the default)\nA = Mat_uniform(sparse([1.0 0.0; 0.0 2.0]))\n\n# Explicitly specify MPIAIJ prefix\nB = Mat_uniform(data; Prefix=MPIAIJ)\n\n# Configure iterative solver for sparse matrices\npetsc_options_insert_string(\"-MPIAIJ_ksp_type gmres\")\n\nSee also: MPIDENSE, Mat, Vec\n\n\n\n\n\n","category":"type"},{"location":"api/matrices/#SafePETSc.MPIDENSE","page":"Matrices","title":"SafePETSc.MPIDENSE","text":"MPIDENSE\n\nPrefix type for dense matrix operations and associated vectors.\n\nString Prefix\n\nThe string prefix is \"MPIDENSE_\", which is prepended to PETSc option names.\n\nDefault PETSc Types\n\nMatrices: mpidense (MPI dense matrix, row-major storage)\nVectors: mpi (standard MPI vector)\n\nUsage\n\nUse MPIDENSE for:\n\nDense matrices where all elements are stored\nOperations requiring dense storage (e.g., eachrow)\nDirect solvers and dense linear algebra\n\nExample\n\n# Create dense matrix\nA = Mat_uniform([1.0 2.0; 3.0 4.0]; Prefix=MPIDENSE)\n\n# Configure GPU acceleration for dense matrices\npetsc_options_insert_string(\"-MPIDENSE_mat_type mpidense\")\n\nSee also: MPIAIJ, Mat, Vec\n\n\n\n\n\n","category":"type"},{"location":"api/matrices/#SafePETSc.prefix","page":"Matrices","title":"SafePETSc.prefix","text":"prefix(::Type{<:Prefix}) -> String\n\nReturn the string prefix for a given prefix type.\n\nThe string prefix is prepended to PETSc option names. For example, with prefix type MPIDENSE, the option -mat_type mpidense becomes -MPIDENSE_mat_type mpidense.\n\nExamples\n\nprefix(MPIDENSE)  # Returns \"MPIDENSE_\"\nprefix(MPIAIJ)    # Returns \"MPIAIJ_\"\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#SafePETSc.Mat_uniform","page":"Matrices","title":"SafePETSc.Mat_uniform","text":"Mat_uniform(A::Matrix{T}; row_partition=default_row_partition(size(A, 1), MPI.Comm_size(MPI.COMM_WORLD)), col_partition=default_row_partition(size(A, 2), MPI.Comm_size(MPI.COMM_WORLD)), Prefix::Type=MPIAIJ) -> DRef{Mat{T,Prefix}}\n\nMPI Collective\n\nCreate a distributed PETSc matrix from a Julia matrix, asserting uniform distribution across ranks (on MPI.COMM_WORLD).\n\nA::Matrix{T} must be identical on all ranks (mpi_uniform).\nrow_partition is a Vector{Int} of length nranks+1 where partition[i] is the start row (1-indexed) for rank i-1.\ncol_partition is a Vector{Int} of length nranks+1 where partition[i] is the start column (1-indexed) for rank i-1.\nPrefix is a type parameter for MatSetOptionsPrefix() to set matrix-specific command-line options (default: MPIAIJ).\nReturns a DRef that will destroy the PETSc Mat collectively when all ranks release their reference.\n\n\n\n\n\nMat_uniform(A::SparseMatrixCSC{T}; row_partition=default_row_partition(size(A, 1), MPI.Comm_size(MPI.COMM_WORLD)), col_partition=default_row_partition(size(A, 2), MPI.Comm_size(MPI.COMM_WORLD)), Prefix::Type=MPIAIJ) -> DRef{Mat{T,Prefix}}\n\nMPI Collective\n\nCreate a distributed PETSc matrix from a sparse Julia matrix, asserting uniform distribution across ranks (on MPI.COMM_WORLD).\n\nA::SparseMatrixCSC{T} must be identical on all ranks (mpi_uniform).\nrow_partition is a Vector{Int} of length nranks+1 where partition[i] is the start row (1-indexed) for rank i-1.\ncol_partition is a Vector{Int} of length nranks+1 where partition[i] is the start column (1-indexed) for rank i-1.\nPrefix is a type parameter for MatSetOptionsPrefix() to set matrix-specific command-line options (default: MPIAIJ).\nReturns a DRef that will destroy the PETSc Mat collectively when all ranks release their reference.\n\nEach rank inserts only the values from its assigned row partition using INSERT_VALUES mode.\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#SafePETSc.Mat_sum","page":"Matrices","title":"SafePETSc.Mat_sum","text":"Mat_sum(A::SparseMatrixCSC{T}; row_partition=default_row_partition(size(A, 1), MPI.Comm_size(MPI.COMM_WORLD)), col_partition=default_row_partition(size(A, 2), MPI.Comm_size(MPI.COMM_WORLD)), Prefix::Type=MPIAIJ, own_rank_only=false) -> DRef{Mat{T,Prefix}}\n\nMPI Collective\n\nCreate a distributed PETSc matrix by summing sparse matrices across ranks (on MPI.COMM_WORLD).\n\nA::SparseMatrixCSC{T} can differ across ranks; nonzero entries are summed across all ranks.\nrow_partition is a Vector{Int} of length nranks+1 where partition[i] is the start row (1-indexed) for rank i-1.\ncol_partition is a Vector{Int} of length nranks+1 where partition[i] is the start column (1-indexed) for rank i-1.\nPrefix is a type parameter for MatSetOptionsPrefix() to set matrix-specific command-line options (default: MPIAIJ).\nown_rank_only::Bool (default=false): if true, asserts that all nonzero entries fall within this rank's row partition.\nReturns a DRef that will destroy the PETSc Mat collectively when all ranks release their reference.\n\nUses MatSetValues with ADD_VALUES mode to sum contributions from all ranks.\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#Base.cat","page":"Matrices","title":"Base.cat","text":"Base.cat(As::Union{Vec{T,Prefix},Mat{T,Prefix}}...; dims) -> Union{Vec{T,Prefix}, Mat{T,Prefix}}\n\nMPI Collective\n\nConcatenate distributed PETSc vectors and/or matrices along dimension dims.\n\nArguments\n\nAs::Union{Vec{T,Prefix},Mat{T,Prefix}}...: One or more vectors or matrices with the same element type T\ndims: Concatenation dimension (1 for vertical/vcat, 2 for horizontal/hcat)\n\nReturn Type\n\nReturns Vec{T,Prefix} when dims=1 and result has a single column (vertical stacking of vectors)\nReturns Mat{T,Prefix} otherwise (horizontal concatenation or matrix inputs)\n\nRequirements\n\nAll inputs must:\n\nHave the same element type T\nHave compatible sizes and partitions for the concatenation dimension\nFor dims=1 (vcat): same number of columns and column partition\nFor dims=2 (hcat): same number of rows and row partition\n\nAutomatic Prefix Selection\n\nThe output Prefix type is automatically determined to ensure correctness:\n\nMPIDENSE if any input has Prefix=MPIDENSE (dense format required)\nMPIDENSE if concatenating vectors horizontally with width > 1 (e.g., hcat(x, y))\nOtherwise, preserves the first input's Prefix\n\nThis ensures that operations like hcat(vec1, vec2) produce dense matrices as expected, since vectors are inherently dense and horizontal concatenation creates a dense result.\n\nImplementation\n\nThe concatenation is performed by:\n\nEach rank extracts its owned rows from each input as a Julia sparse matrix\nStandard Julia cat is applied locally on each rank\nThe results are combined across ranks using Vec_sum (for single-column results) or Mat_sum (otherwise)\n\nExamples\n\n# Vertical concatenation (stacking) - returns Vec\nx = Vec_uniform([1.0, 2.0, 3.0])\ny = Vec_uniform([4.0, 5.0, 6.0])\nv = vcat(x, y)  # Returns Vec{Float64,MPIAIJ} with 6 elements\n\n# Horizontal concatenation - returns Mat\nM = hcat(x, y)  # Returns Mat{Float64,MPIDENSE} of size 3×2\n\n# Matrix concatenation - returns Mat\nA = Mat_uniform(sparse([1 2; 3 4]))\nB = Mat_uniform(sparse([5 6; 7 8]))\nC = vcat(A, B)  # Returns Mat{Float64,MPIAIJ} of size 4×2\n\nSee also: vcat, hcat, Vec_sum, Mat_sum\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#Base.vcat","page":"Matrices","title":"Base.vcat","text":"Base.vcat(As::Union{Vec{T,Prefix},Mat{T,Prefix}}...) -> Union{Vec{T,Prefix}, Mat{T,Prefix}}\n\nMPI Collective\n\nVertically concatenate (stack) distributed PETSc vectors and/or matrices.\n\nEquivalent to cat(As...; dims=1). Stacks inputs vertically, increasing the number of rows while keeping the number of columns constant.\n\nReturn Type\n\nReturns Vec{T,Prefix} when concatenating only vectors (single-column result)\nReturns Mat{T,Prefix} when concatenating matrices or when result has multiple columns\n\nRequirements\n\nAll inputs must have the same number of columns and the same column partition.\n\nPrefix Selection\n\nTypically preserves the input Prefix (e.g., MPIAIJ for vectors)\nUpgrades to MPIDENSE if any input is MPIDENSE\n\nExamples\n\n# Concatenating vectors returns a Vec\nx = Vec_uniform([1.0, 2.0])\ny = Vec_uniform([3.0, 4.0])\nv = vcat(x, y)  # Vec{Float64,MPIAIJ} with 4 elements\n\n# Concatenating matrices returns a Mat\nA = Mat_uniform(sparse([1 2; 3 4]))\nB = Mat_uniform(sparse([5 6; 7 8]))\nC = vcat(A, B)  # Mat{Float64,MPIAIJ} of size 4×2\n\nSee also: cat, hcat\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#Base.hcat","page":"Matrices","title":"Base.hcat","text":"Base.hcat(As::Union{Vec{T,Prefix},Mat{T,Prefix}}...) -> Mat{T,Prefix}\n\nMPI Collective\n\nHorizontally concatenate (place side-by-side) distributed PETSc vectors and/or matrices.\n\nEquivalent to cat(As...; dims=2). Concatenates inputs horizontally, increasing the number of columns while keeping the number of rows constant.\n\nRequirements\n\nAll inputs must have the same number of rows and the same row partition.\n\nPrefix Selection\n\nAutomatically upgrades to MPIDENSE when concatenating vectors (width > 1)\nUpgrades to MPIDENSE if any input is MPIDENSE\nOtherwise preserves the input Prefix\n\nThe automatic upgrade for vectors is important because vectors are inherently dense, and horizontal concatenation of vectors produces a dense matrix.\n\nExamples\n\nx = Vec_uniform([1.0, 2.0, 3.0])\ny = Vec_uniform([4.0, 5.0, 6.0])\nM = hcat(x, y)  # 3×2 Mat{Float64,MPIDENSE} - auto-upgraded!\n\nA = Mat_uniform(sparse([1; 2; 3]))\nB = Mat_uniform(sparse([4; 5; 6]))\nC = hcat(A, B)  # 3×2 matrix with MPIDENSE\n\nSee also: cat, vcat\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#SparseArrays.blockdiag","page":"Matrices","title":"SparseArrays.blockdiag","text":"blockdiag(As::Mat{T,Prefix}...) -> Mat{T,Prefix}\n\nMPI Collective\n\nCreate a block diagonal matrix from distributed PETSc matrices.\n\nThe result is a matrix with the input matrices along the diagonal and zeros elsewhere. All matrices must have the same prefix and element type.\n\nExample\n\n# If A is m×n and B is p×q, then blockdiag(A, B) is (m+p)×(n+q)\nC = blockdiag(A, B)\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#SparseArrays.spdiagm","page":"Matrices","title":"SparseArrays.spdiagm","text":"spdiagm(kv::Pair{<:Integer, <:Vec{T,Prefix}}...) -> Mat{T,Prefix}\nspdiagm(m::Integer, n::Integer, kv::Pair{<:Integer, <:Vec{T,Prefix}}...) -> Mat{T,Prefix}\n\nMPI Collective\n\nCreate a sparse diagonal matrix from distributed PETSc vectors.\n\nEach pair k => v places the vector v on the k-th diagonal:\n\nk = 0: main diagonal\nk > 0: superdiagonal\nk < 0: subdiagonal\n\nAll vectors must have the same element type T. The matrix dimensions are inferred from the diagonal positions and vector lengths, or can be specified explicitly.\n\nOptional Keyword Arguments\n\nprefix: Matrix prefix type to use for the result. Defaults to the input vector's prefix. Use this to create a matrix with a different prefix than the input vectors (e.g., create MPIAIJ from MPIDENSE vectors, or vice versa).\nrow_partition: Override the default equal-row partitioning (length nranks+1, start at 1, end at m+1, non-decreasing). Defaults to default_row_partition(m, nranks).\ncol_partition: Override the default equal-column partitioning (length nranks+1, start at 1, end at n+1, non-decreasing). Defaults to default_row_partition(n, nranks).\n\nExamples\n\n# Create a tridiagonal matrix\nA = spdiagm(-1 => lower, 0 => diag, 1 => upper)\n\n# Create a 100×100 matrix with specified vectors on diagonals\nB = spdiagm(100, 100, 0 => v1, 1 => v2)\n\n# Create MPIAIJ (sparse) matrix from MPIDENSE vector\nv_dense = Vec_uniform(data; Prefix=MPIDENSE)\nA_sparse = spdiagm(0 => v_dense; prefix=MPIAIJ)\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#Base.Matrix-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}}","page":"Matrices","title":"Base.Matrix","text":"Matrix(x::Mat{T,Prefix}) -> Matrix{T}\n\nMPI Collective\n\nConvert a distributed PETSc Mat to a Julia Matrix by gathering all data to all ranks. This is a collective operation - all ranks must call it and will receive the complete matrix.\n\nFor dense matrices, this uses efficient MatDenseGetArrayRead. For other matrix types, it uses MatGetRow to extract each row.\n\nThis is primarily used for display purposes or small matrices. For large matrices, this operation can be expensive as it gathers all data to all ranks.\n\n\n\n\n\nBase.Matrix(At::LinearAlgebra.Adjoint{T, <:Mat{T}}) -> Matrix{T}\n\nMPI Collective\n\nConvert an adjoint of a distributed PETSc Mat to an adjoint Julia Matrix. Equivalent to Matrix(parent(At))'.\n\nThis is a collective operation - all ranks must call it and will receive the complete matrix transpose.\n\n\n\n\n\n","category":"method"},{"location":"api/matrices/#SparseArrays.sparse-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}}","page":"Matrices","title":"SparseArrays.sparse","text":"SparseArrays.sparse(x::Mat{T,Prefix}) -> SparseMatrixCSC{T, Int}\n\nMPI Collective\n\nConvert a distributed PETSc Mat to a Julia SparseMatrixCSC by gathering all data to all ranks. This is a collective operation - all ranks must call it and will receive the complete sparse matrix.\n\nUses MatGetRow to extract the sparse structure efficiently, preserving sparsity.\n\nThis is primarily used for display purposes or small matrices. For large matrices, this operation can be expensive as it gathers all data to all ranks.\n\n\n\n\n\nSparseArrays.sparse(At::LinearAlgebra.Adjoint{T, <:Mat{T}}) -> SparseMatrixCSC{T, Int}\n\nMPI Collective\n\nConvert an adjoint of a distributed PETSc Mat to an adjoint Julia SparseMatrixCSC. Equivalent to sparse(parent(At))'.\n\nThis is a collective operation - all ranks must call it and will receive the complete sparse matrix transpose.\n\n\n\n\n\n","category":"method"},{"location":"api/matrices/#SafePETSc.is_dense","page":"Matrices","title":"SafePETSc.is_dense","text":"is_dense(x::Mat{T,Prefix}) -> Bool\n\nMPI Non-Collective\n\nCheck if a PETSc matrix is a dense matrix type.\n\nThis checks the PETSc matrix type string and returns true if it contains \"dense\" (case-insensitive). This handles various dense types like \"seqdense\", \"mpidense\", and vendor-specific dense matrix types.\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#SafePETSc.own_row-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}}","page":"Matrices","title":"SafePETSc.own_row","text":"own_row(v::Vec{T,Prefix}) -> UnitRange{Int}\n\nMPI Non-Collective\n\nReturn the range of indices owned by the current rank for vector v.\n\nExample\n\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\nrange = own_row(v)  # e.g., 1:2 on rank 0\n\n\n\n\n\nown_row(A::Mat{T,Prefix}) -> UnitRange{Int}\n\nMPI Non-Collective\n\nReturn the range of row indices owned by the current rank for matrix A.\n\nExample\n\nA = Mat_uniform([1.0 2.0; 3.0 4.0; 5.0 6.0; 7.0 8.0])\nrange = own_row(A)  # e.g., 1:2 on rank 0\n\n\n\n\n\n","category":"method"},{"location":"api/matrices/#Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, Colon, Int64}","page":"Matrices","title":"Base.getindex","text":"Base.getindex(A::Mat{T,Prefix}, ::Colon, k::Int) -> Vec{T,Prefix}\n\nMPI Non-Collective\n\nExtract column k from matrix A, returning a distributed vector.\n\nEach rank extracts its owned rows from column k. The resulting vector has the same row partition as matrix A.\n\nUses efficient bulk operations: MatDenseGetArrayRead for dense matrices, MatGetRow for sparse matrices.\n\nExample\n\nA = Mat_uniform([1.0 2.0 3.0; 4.0 5.0 6.0])\nv = A[:, 2]  # Extract second column: [2.0, 5.0]\n\n\n\n\n\n","category":"method"},{"location":"api/matrices/#Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, Int64, Int64}","page":"Matrices","title":"Base.getindex","text":"Base.getindex(A::Mat{T}, i::Int, j::Int) -> T\n\nMPI Non-Collective\n\nGet the value at position (i, j) from a distributed matrix.\n\nThe row index i must be wholly contained in the current rank's row ownership range. If not, the function will abort with an error message and stack trace.\n\nThis is a non-collective operation.\n\nExample\n\nA = Mat_uniform([1.0 2.0; 3.0 4.0])\n# On rank that owns row 1:\nval = A[1, 2]  # Returns 2.0\n\n\n\n\n\n","category":"method"},{"location":"api/matrices/#Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, UnitRange{Int64}, Int64}","page":"Matrices","title":"Base.getindex","text":"Base.getindex(A::Mat{T}, range_i::UnitRange{Int}, j::Int) -> Vector{T}\n\nMPI Non-Collective\n\nExtract a contiguous range of rows from column j of a distributed matrix.\n\nThe row range must be wholly contained in the current rank's row ownership range. If not, the function will abort with an error message and stack trace.\n\nThis is a non-collective operation.\n\nExample\n\nA = Mat_uniform([1.0 2.0; 3.0 4.0; 5.0 6.0])\n# On rank that owns rows 1:2:\nvals = A[1:2, 2]  # Returns [2.0, 4.0]\n\n\n\n\n\n","category":"method"},{"location":"api/matrices/#Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, Int64, UnitRange{Int64}}","page":"Matrices","title":"Base.getindex","text":"Base.getindex(A::Mat{T}, i::Int, range_j::UnitRange{Int}) -> Vector{T}\n\nMPI Non-Collective\n\nExtract a contiguous range of columns from row i of a distributed matrix.\n\nThe row index i must be wholly contained in the current rank's row ownership range. If not, the function will abort with an error message and stack trace.\n\nThis is a non-collective operation.\n\nExample\n\nA = Mat_uniform([1.0 2.0 3.0; 4.0 5.0 6.0])\n# On rank that owns row 1:\nvals = A[1, 2:3]  # Returns [2.0, 3.0]\n\n\n\n\n\n","category":"method"},{"location":"api/matrices/#Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, UnitRange{Int64}, UnitRange{Int64}}","page":"Matrices","title":"Base.getindex","text":"Base.getindex(A::Mat{T}, range_i::UnitRange{Int}, range_j::UnitRange{Int}) -> Union{Matrix{T}, SparseMatrixCSC{T}}\n\nMPI Non-Collective\n\nExtract a submatrix from a distributed matrix.\n\nThe row range must be wholly contained in the current rank's row ownership range. If not, the function will abort with an error message and stack trace.\n\nReturns a dense Matrix if A is dense, otherwise returns a SparseMatrixCSC.\n\nThis is a non-collective operation.\n\nExample\n\nA = Mat_uniform([1.0 2.0 3.0; 4.0 5.0 6.0; 7.0 8.0 9.0])\n# On rank that owns rows 1:2:\nsubmat = A[1:2, 2:3]  # Returns [2.0 3.0; 5.0 6.0]\n\n\n\n\n\n","category":"method"},{"location":"api/matrices/#SafePETSc.BlockProduct","page":"Matrices","title":"SafePETSc.BlockProduct","text":"BlockProduct{T,Prefix}\n\nRepresents a product of block matrices with pre-allocated storage for efficient recomputation.\n\nA block matrix is a Julia Matrix where each element is a Mat, Mat', Vec, Vec', scalar, or nothing.\n\nFields\n\nprod::Vector{Matrix{BlockElement{T,Prefix}}}: The sequence of block matrices to multiply\nresult::Union{Matrix{BlockElement{T,Prefix}}, Nothing}: Pre-allocated result (allocated on first calculate! call)\nintermediates::Vector{Matrix{BlockElement{T,Prefix}}}: Pre-allocated intermediate results for chained products\n\nType Parameters\n\nT: Element type (e.g., Float64)\nPrefix: PETSc prefix type (e.g., MPIAIJ, MPIDENSE) - must match all contained objects\n\nConstructor\n\nBlockProduct(prod::Vector{Matrix}; Prefix::Type=MPIAIJ)\n\nValidates dimensions and creates a BlockProduct. Actual allocation of result and intermediates happens lazily on the first call to calculate!.\n\nExample\n\n# Create block matrices\nA = [M1 M2; M3 M4]  # 2x2 block of Mat{Float64,MPIAIJ}\nB = [N1 N2; N3 N4]\n\n# Create product (no allocation yet)\nbp = BlockProduct([A, B])\n\n# Compute A * B (allocates result on first call)\nC = calculate!(bp)\n\n# Subsequent calls reuse allocations\nC2 = calculate!(bp)\n\n\n\n\n\n","category":"type"},{"location":"api/matrices/#SafePETSc.calculate!","page":"Matrices","title":"SafePETSc.calculate!","text":"calculate!(bp::BlockProduct)\n\nMPI Collective\n\nRecompute the product after modifying input matrices/vectors, reusing cached PETSc objects.\n\nAfter the user modifies entries in bp.prod[k][i,j] matrices or vectors, calling this function updates bp.result using in-place operations on the cached intermediate results.\n\nThis avoids allocating new PETSc Mat/Vec objects.\n\nReturns the updated result as a block matrix (Julia Matrix of BlockElements).\n\nExample\n\n# Create and compute initial product\nbp = BlockProduct([A, B])\nresult1 = bp.result\n\n# Modify input matrix entries\n# (modify A[1,1] entries here)\n\n# Recompute with cached objects\ncalculate!(bp)\nresult2 = bp.result\n\n# result2 has updated values but same PETSc object identity\n\n\n\n\n\n","category":"function"},{"location":"guide/matrices/#Matrices","page":"Matrices","title":"Matrices","text":"SafePETSc provides distributed matrices through the Mat{T,Prefix} type, which wraps PETSc's distributed matrix functionality with GPU-friendly operations and automatic memory management.","category":"section"},{"location":"guide/matrices/#Creating-Matrices","page":"Matrices","title":"Creating Matrices","text":"","category":"section"},{"location":"guide/matrices/#Uniform-Distribution","page":"Matrices","title":"Uniform Distribution","text":"Use Mat_uniform when all ranks have the same data:\n\n# Create from dense matrix\nA = Mat_uniform([1.0 2.0; 3.0 4.0])\n\n# With custom partitions\nrow_part = [1, 2, 3]  # 2 ranks\ncol_part = [1, 2, 3]\nA = Mat_uniform(data; row_partition=row_part, col_partition=col_part)\n\n# With custom prefix type (advanced)\nA = Mat_uniform(data; Prefix=MPIDENSE)","category":"section"},{"location":"guide/matrices/#Sum-Distribution","page":"Matrices","title":"Sum Distribution","text":"Use Mat_sum when ranks contribute sparse entries:\n\nusing SparseArrays\n\n# Each rank contributes different sparse entries\n# All contributions are summed\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nI = [1, rank+1]\nJ = [1, rank+1]\nV = [1.0, 2.0]\nA = Mat_sum(sparse(I, J, V, 10, 10))\n\n# Assert local ownership for validation\nA = Mat_sum(sparse_local; own_rank_only=true)","category":"section"},{"location":"guide/matrices/#Matrix-Operations","page":"Matrices","title":"Matrix Operations","text":"","category":"section"},{"location":"guide/matrices/#Linear-Algebra","page":"Matrices","title":"Linear Algebra","text":"# Matrix-vector multiplication\ny = A * x\n\n# In-place\nmul!(y, A, x)\n\n# Matrix-matrix multiplication\nC = A * B\n\n# In-place\nmul!(C, A, B)\n\n# Transpose\nB = A'\nB = Mat(A')  # Materialize transpose\n\n# In-place transpose (reuses B)\ntranspose!(B, A)","category":"section"},{"location":"guide/matrices/#Concatenation","page":"Matrices","title":"Concatenation","text":"# Vertical concatenation (stacking)\nC = vcat(A, B)\nC = cat(A, B; dims=1)\n\n# Horizontal concatenation (side-by-side)\nD = hcat(A, B)\nD = cat(A, B; dims=2)\n\n# Block diagonal\nE = blockdiag(A, B, C)\n\n# Concatenating vectors to form matrices\nx = Vec_uniform([1.0, 2.0, 3.0])\ny = Vec_uniform([4.0, 5.0, 6.0])\nM = hcat(x, y)  # Creates 3×2 Mat{Float64,MPIDENSE}\n\nnote: Automatic Prefix Selection\nThe concatenation functions automatically determine the output Prefix type:Upgrades to MPIDENSE when concatenating vectors horizontally (e.g., hcat(x, y))\nUpgrades to MPIDENSE if any input matrix has Prefix=MPIDENSE\nOtherwise, preserves the first input's PrefixThis ensures correctness since vectors are inherently dense, and horizontal concatenation of vectors produces a dense matrix. Vertical concatenation of vectors (vcat) preserves the sparse format since the result is still a single column.","category":"section"},{"location":"guide/matrices/#Sparse-Diagonal-Matrices","page":"Matrices","title":"Sparse Diagonal Matrices","text":"using SparseArrays\n\n# Create diagonal matrix from vectors\ndiag_vec = Vec_uniform(ones(100))\nupper_vec = Vec_uniform(ones(99))\nlower_vec = Vec_uniform(ones(99))\n\n# Tridiagonal matrix\nA = spdiagm(-1 => lower_vec, 0 => diag_vec, 1 => upper_vec)\n\n# Explicit dimensions\nA = spdiagm(100, 100, 0 => diag_vec, 1 => upper_vec)\n\n# Control output prefix type\nv_dense = Vec_uniform(data; Prefix=MPIDENSE)\nA_sparse = spdiagm(0 => v_dense; prefix=MPIAIJ)  # Create sparse from dense\n\nnote: Prefix Parameter\nThe prefix keyword argument allows you to override the output matrix prefix type. By default, spdiagm returns a matrix with the same prefix as the input vectors, but you can specify a different prefix to convert between dense and sparse formats. This is useful when you need a sparse diagonal matrix from dense vectors, or vice versa.","category":"section"},{"location":"guide/matrices/#Transpose-Operations","page":"Matrices","title":"Transpose Operations","text":"SafePETSc uses PETSc's efficient transpose operations:\n\n# Create transpose (new matrix)\nB = Mat(A')\n\n# Reuse transpose storage\nB = Mat(A')  # Initial creation\n# ... later, after A changes:\ntranspose!(B, A)  # Reuse B's storage\n\nNote: For transpose! to work correctly with PETSc's reuse mechanism, B should have been created as a transpose of A initially.","category":"section"},{"location":"guide/matrices/#Properties","page":"Matrices","title":"Properties","text":"# Element type\nT = eltype(A)\n\n# Size\nm, n = size(A)\nm = size(A, 1)\nn = size(A, 2)\n\n# Partition information\nrow_part = A.obj.row_partition\ncol_part = A.obj.col_partition","category":"section"},{"location":"guide/matrices/#Row-Ownership-and-Indexing","page":"Matrices","title":"Row Ownership and Indexing","text":"","category":"section"},{"location":"guide/matrices/#Determining-Owned-Rows","page":"Matrices","title":"Determining Owned Rows","text":"Use own_row() to find which row indices are owned by the current rank:\n\nA = Mat_uniform([1.0 2.0; 3.0 4.0; 5.0 6.0; 7.0 8.0])\n\n# Get ownership range for this rank\nowned = own_row(A)  # e.g., 1:2 on rank 0, 3:4 on rank 1\n\nprintln(io0(), \"Rank $(MPI.Comm_rank(MPI.COMM_WORLD)) owns rows: $owned\")","category":"section"},{"location":"guide/matrices/#Indexing-Matrices","page":"Matrices","title":"Indexing Matrices","text":"Important: You can only index rows that are owned by the current rank. Attempting to access non-owned rows will result in an error.\n\nSafePETSc supports several indexing patterns:\n\nA = Mat_uniform([1.0 2.0 3.0; 4.0 5.0 6.0; 7.0 8.0 9.0; 10.0 11.0 12.0])\nowned = own_row(A)\n\n# ✓ Single element (owned row, any column)\nif 2 in owned\n    val = A[2, 3]  # Returns 6.0 on the rank that owns row 2\nend\n\n# ✓ Extract column (all ranks get their owned portion)\ncol_vec = A[:, 2]  # Returns distributed Vec with owned rows from column 2\n\n# ✓ Row slice (owned row, column range)\nif 3 in owned\n    row_slice = A[3, 1:2]  # Returns [7.0, 8.0] on the rank that owns row 3\nend\n\n# ✓ Column slice (owned row range, single column)\nif owned == 1:2\n    col_slice = A[1:2, 2]  # Returns [2.0, 5.0] on the rank that owns these rows\nend\n\n# ✓ Submatrix (owned row range, column range)\nif owned == 1:2\n    submat = A[1:2, 2:3]  # Returns 2×2 Matrix on the rank that owns these rows\nend\n\n# ❌ WRONG - Accessing non-owned rows causes an error\nval = A[4, 1]  # ERROR if rank doesn't own row 4!\n\nIndexing is non-collective - each rank can independently access its owned rows without coordination.","category":"section"},{"location":"guide/matrices/#Use-Cases-for-Indexing","page":"Matrices","title":"Use Cases for Indexing","text":"Indexing is useful when you need to:\n\nExtract specific local values from owned rows\nExtract columns as distributed vectors\nImplement custom local operations\nInterface with non-PETSc code on owned data\n\n# Extract owned portion for local processing\nA = Mat_uniform(randn(100, 50))\nowned = own_row(A)\n\n# Get local submatrix\nlocal_submat = A[owned, 1:10]  # First 10 columns of owned rows\n\n# Process locally\nlocal_norms = [norm(local_submat[i, :]) for i in 1:length(owned)]\n\n# Aggregate across ranks if needed\nmax_norm = MPI.Allreduce(maximum(local_norms), max, MPI.COMM_WORLD)","category":"section"},{"location":"guide/matrices/#Partitioning","page":"Matrices","title":"Partitioning","text":"Matrices have both row and column partitions.","category":"section"},{"location":"guide/matrices/#Default-Partitioning","page":"Matrices","title":"Default Partitioning","text":"m, n = 100, 80\nnranks = MPI.Comm_size(MPI.COMM_WORLD)\n\nrow_part = default_row_partition(m, nranks)\ncol_part = default_row_partition(n, nranks)","category":"section"},{"location":"guide/matrices/#Requirements","page":"Matrices","title":"Requirements","text":"Row operations require matching row partitions\nColumn operations require matching column partitions\nMatrix multiplication: C = A * B requires A.col_partition == B.row_partition","category":"section"},{"location":"guide/matrices/#Row-wise-Operations-with-map_rows","page":"Matrices","title":"Row-wise Operations with map_rows","text":"The map_rows() function applies a function to each row of distributed matrices or vectors, enabling powerful row-wise transformations.","category":"section"},{"location":"guide/matrices/#Basic-Usage-with-Matrices","page":"Matrices","title":"Basic Usage with Matrices","text":"# Apply function to each row of a matrix\nA = Mat_uniform([1.0 2.0 3.0; 4.0 5.0 6.0])\n\n# Compute row sums (returns Vec)\nrow_sums = map_rows(sum, A)  # Returns Vec([6.0, 15.0])\n\n# Compute statistics per row (returns Mat)\nstats = map_rows(row -> [sum(row), prod(row)]', A)\n# Returns 2×2 Mat: [[6.0, 6.0]; [15.0, 48.0]]\n\nNote: For matrices, the function receives a view of each row (like eachrow).","category":"section"},{"location":"guide/matrices/#Output-Types","page":"Matrices","title":"Output Types","text":"The return type depends on what your function returns:\n\nScalar → Returns a Vec with m rows (one value per row)\nVector → Returns a Vec with expanded rows (m*n total elements)\nAdjoint Vector (row vector) → Returns a Mat with m rows\n\nB = Mat_uniform([1.0 2.0; 3.0 4.0; 5.0 6.0])\n\n# Scalar output: Vec with 3 elements\nmeans = map_rows(mean, B)\n\n# Vector output: Vec with 3*2 = 6 elements\ndoubled = map_rows(row -> [row[1], row[2]], B)\n\n# Matrix output: Mat with 3 rows, 2 columns\nstats = map_rows(row -> [minimum(row), maximum(row)]', B)","category":"section"},{"location":"guide/matrices/#Combining-Matrices-and-Vectors","page":"Matrices","title":"Combining Matrices and Vectors","text":"Process matrices and vectors together row-wise:\n\nB = Mat_uniform(randn(5, 3))\nC = Vec_uniform(randn(5))\n\n# Combine matrix rows with corresponding vector elements\nresult = map_rows((mat_row, vec_row) -> [sum(mat_row), prod(mat_row), vec_row[1]]', B, C)\n# Returns 5×3 matrix with [row_sum, row_product, vec_value] per row\n\nImportant: All inputs must have compatible row partitions.","category":"section"},{"location":"guide/matrices/#Real-World-Example","page":"Matrices","title":"Real-World Example","text":"using Statistics\n\n# Data matrix: each row is an observation\ndata = Mat_uniform(randn(1000, 50))\n\n# Compute statistics for each observation\nobservation_stats = map_rows(data) do row\n    [mean(row), std(row), minimum(row), maximum(row)]'\nend\n# Returns 1000×4 matrix with statistics per observation\n\n# Convert to Julia matrix for analysis if small\nif size(observation_stats, 1) < 10000\n    stats_array = Matrix(observation_stats)\n    # Further analysis with standard Julia tools\nend","category":"section"},{"location":"guide/matrices/#Performance-Notes","page":"Matrices","title":"Performance Notes","text":"map_rows() is a collective operation - all ranks must call it\nThe function is applied only to locally owned rows on each rank\nResults are automatically assembled into a new distributed object\nMore efficient than extracting rows individually and processing\nWorks efficiently with both dense and sparse matrices","category":"section"},{"location":"guide/matrices/#Advanced-Features","page":"Matrices","title":"Advanced Features","text":"","category":"section"},{"location":"guide/matrices/#Iterating-Over-Matrix-Rows","page":"Matrices","title":"Iterating Over Matrix Rows","text":"For dense (MPIDENSE) matrices, eachrow returns views:\n\n# Iterate over local rows efficiently\nfor row in eachrow(A)\n    # row is a view of the matrix row (SubArray)\n    process(row)\nend\n\nFor sparse (MPIAIJ) matrices, eachrow returns sparse vectors:\n\n# Iterate over local rows of a sparse matrix\nfor row in eachrow(A_sparse)\n    # row is a SparseVector efficiently preserving sparsity\n    process(row)\nend\n\nBoth implementations are efficient: dense iteration uses a single MatDenseGetArrayRead call, while sparse iteration uses MatGetRow for each row without wasteful dense conversions.","category":"section"},{"location":"guide/matrices/#PETSc-Options-and-the-Prefix-Type-Parameter","page":"Matrices","title":"PETSc Options and the Prefix Type Parameter","text":"SafePETSc matrices have a Prefix type parameter (e.g., Mat{Float64,MPIAIJ}) that determines both the matrix storage format and PETSc configuration. SafePETSc provides two built-in prefix types:","category":"section"},{"location":"guide/matrices/#Built-in-Prefix-Types","page":"Matrices","title":"Built-in Prefix Types","text":"MPIAIJ (default): For sparse matrices\nString prefix: \"MPIAIJ_\"\nDefault PETSc matrix type: mpiaij (MPI sparse matrix, compressed row storage)\nUse for: Sparse linear algebra, iterative solvers\nMemory efficient for matrices with few nonzeros per row\nMPIDENSE: For dense matrices\nString prefix: \"MPIDENSE_\"\nDefault PETSc matrix type: mpidense (MPI dense matrix, row-major storage)\nUse for: Dense linear algebra, direct solvers, operations like eachrow\nStores all matrix elements\n\nImportant: Unlike vectors (which are always dense internally), the Prefix parameter fundamentally changes matrix storage format. Choose MPIDENSE when you need dense storage, and MPIAIJ for sparse matrices.","category":"section"},{"location":"guide/matrices/#Setting-PETSc-Options","page":"Matrices","title":"Setting PETSc Options","text":"Configure PETSc behavior for matrices with a specific prefix:\n\n# Configure GPU-accelerated dense matrices\npetsc_options_insert_string(\"-MPIDENSE_mat_type mpidense\")\nA = Mat_uniform(data; Prefix=MPIDENSE)\n\n# Configure sparse matrices with custom solver\npetsc_options_insert_string(\"-MPIAIJ_mat_type mpiaij\")\nB = Mat_uniform(sparse_data; Prefix=MPIAIJ)\n\nThe string prefix (e.g., \"MPIDENSE_\", \"MPIAIJ_\") is automatically prepended to option names when PETSc processes options for matrices with that prefix type.","category":"section"},{"location":"guide/matrices/#Examples","page":"Matrices","title":"Examples","text":"","category":"section"},{"location":"guide/matrices/#Assemble-a-Sparse-Matrix","page":"Matrices","title":"Assemble a Sparse Matrix","text":"using SafePETSc\nusing SparseArrays\nusing MPI\n\nSafePETSc.Init()\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nnranks = MPI.Comm_size(MPI.COMM_WORLD)\n\nn = 100\n\n# Each rank builds a local piece\nrow_part = default_row_partition(n, nranks)\nlo = row_part[rank + 1]\nhi = row_part[rank + 2] - 1\n\n# Build local sparse matrix (only owned rows)\nI = Int[]\nJ = Int[]\nV = Float64[]\n\nfor i in lo:hi\n    # Diagonal\n    push!(I, i)\n    push!(J, i)\n    push!(V, 2.0)\n\n    # Off-diagonal\n    if i > 1\n        push!(I, i)\n        push!(J, i-1)\n        push!(V, -1.0)\n    end\n    if i < n\n        push!(I, i)\n        push!(J, i+1)\n        push!(V, -1.0)\n    end\nend\n\nlocal_sparse = sparse(I, J, V, n, n)\n\n# Assemble global matrix\nA = Mat_sum(local_sparse; own_rank_only=true)","category":"section"},{"location":"guide/matrices/#Build-Block-Matrices","page":"Matrices","title":"Build Block Matrices","text":"# Create blocks\nA11 = Mat_uniform(...)\nA12 = Mat_uniform(...)\nA21 = Mat_uniform(...)\nA22 = Mat_uniform(...)\n\n# Assemble block matrix\nA = vcat(hcat(A11, A12), hcat(A21, A22))\n\n# Or equivalently\nA = [A11 A12; A21 A22]  # (if block-matrix syntax is supported)","category":"section"},{"location":"guide/matrices/#Tridiagonal-System","page":"Matrices","title":"Tridiagonal System","text":"n = 1000\n\n# Create diagonal vectors\ndiag = Vec_uniform(2.0 * ones(n))\nupper = Vec_uniform(-1.0 * ones(n-1))\nlower = Vec_uniform(-1.0 * ones(n-1))\n\n# Build tridiagonal matrix\nA = spdiagm(-1 => lower, 0 => diag, 1 => upper)\n\n# Create RHS\nb = Vec_uniform(ones(n))\n\n# Solve\nx = A \\ b","category":"section"},{"location":"guide/matrices/#Performance-Tips","page":"Matrices","title":"Performance Tips","text":"Use Native Operations: Prefer PETSc operations over element access\nBatch Assembly: Build sparse matrices locally, then sum once\nAppropriate Matrix Type: Use dense vs. sparse based on structure\nReuse KSP Objects: Create KSP once, reuse for multiple solves\nGPU Configuration: Set PETSc options for GPU matrices\n\n# Good: bulk assembly\nlocal_matrix = sparse(I, J, V, m, n)\nA = Mat_sum(local_matrix)\n\n# Less good: element-by-element (if it were supported)\n# A = Mat_sum(...)\n# for each element\n#     set_value(A, i, j, val)  # Repeated MPI calls","category":"section"},{"location":"guide/matrices/#Converting-to-Julia-Arrays","page":"Matrices","title":"Converting to Julia Arrays","text":"You can convert distributed Mat objects to native Julia arrays for interoperability, analysis, or export. SafePETSc provides two conversion options depending on matrix structure.","category":"section"},{"location":"guide/matrices/#Dense-Matrix-Conversion","page":"Matrices","title":"Dense Matrix Conversion","text":"Use Matrix() to convert to a dense Julia array:\n\n# Create a distributed matrix\nA = Mat_uniform([1.0 2.0; 3.0 4.0])\n\n# Convert to Julia Matrix\nA_dense = Matrix(A)  # Returns Matrix{Float64}","category":"section"},{"location":"guide/matrices/#Sparse-Matrix-Conversion","page":"Matrices","title":"Sparse Matrix Conversion","text":"Use sparse() to convert to a sparse CSC matrix (preserves sparsity):\n\nusing SparseArrays\n\n# Create sparse matrix\nn = 10_000\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [2.0*ones(n); -ones(n-1); -ones(n-1)]\nA = Mat_sum(sparse(I, J, V, n, n))\n\n# Convert to CSC format (preserves sparsity)\nA_csc = sparse(A)  # Returns SparseMatrixCSC{Float64, Int}\n\n# Don't use Matrix() for sparse matrices!\nA_dense = Matrix(A)  # Creates 10000×10000 dense array - wasteful!","category":"section"},{"location":"guide/matrices/#Checking-Matrix-Type-with-is_dense","page":"Matrices","title":"Checking Matrix Type with is_dense","text":"Use is_dense() to determine if a matrix is stored in dense format:\n\nusing SparseArrays\n\nA_dense = Mat_uniform([1.0 2.0; 3.0 4.0])\nA_sparse = Mat_uniform(sparse([1, 2], [1, 2], [1.0, 4.0], 10, 10))\n\nis_dense(A_dense)   # Returns true (matrix type contains \"dense\")\nis_dense(A_sparse)  # Returns false (matrix type is sparse)\n\n# Use appropriate conversion\nif is_dense(A)\n    A_julia = Matrix(A)      # Convert to dense\nelse\n    A_julia = sparse(A)      # Convert to sparse CSC\nend\n\nThe is_dense() function checks the PETSc matrix type string and returns true if it contains \"dense\" (case-insensitive). This handles various dense types like \"seqdense\", \"mpidense\", and vendor-specific dense matrix types.","category":"section"},{"location":"guide/matrices/#Important:-Collective-Operation","page":"Matrices","title":"Important: Collective Operation","text":"Both conversion functions are collective operations - all ranks must call them:\n\n# ✓ CORRECT - All ranks participate\nA_julia = Matrix(A)  # All ranks get the complete matrix\n\n# ❌ WRONG - Will hang MPI!\nif rank == 0\n    A_julia = Matrix(A)  # Only rank 0 calls, others wait forever\nend\n\nAfter conversion, all ranks receive the complete matrix. The data is gathered from all ranks using MPI collective operations.","category":"section"},{"location":"guide/matrices/#When-to-Use-Conversions","page":"Matrices","title":"When to Use Conversions","text":"Good use cases:\n\nInteroperability: Pass data to packages that don't support PETSc\nSmall-scale analysis: Compute eigenvalues, determinants, etc.\nData export: Save results to files\nVisualization: Convert for plotting libraries\n\nusing LinearAlgebra\n\n# Solve distributed system\nA = Mat_uniform([2.0 1.0; 1.0 3.0])\nb = Vec_uniform([1.0, 2.0])\nx = A \\ b\n\n# Convert for analysis (small matrix, so conversion is cheap)\nA_julia = Matrix(A)\nλ = eigvals(A_julia)       # Compute eigenvalues\ndet_A = det(A_julia)       # Compute determinant\n\nprintln(io0(), \"Eigenvalues: \", λ)\nprintln(io0(), \"Determinant: \", det_A)\n\nAvoid conversions for:\n\nLarge matrices: Gathers all data to all ranks (very expensive!)\nIntermediate computations: Keep data in PETSc format\nDense conversion of sparse matrices: Use sparse() instead","category":"section"},{"location":"guide/matrices/#Performance-Considerations","page":"Matrices","title":"Performance Considerations","text":"Conversion performance scales with:\n\nMatrix size: Larger matrices take longer to gather\nRank count: More ranks means more communication\nSparsity: Sparse conversions are more efficient than dense for large sparse matrices\n\n# Small: fast conversion (< 1ms)\nA_small = Mat_uniform(ones(100, 100))\nA_julia = Matrix(A_small)\n\n# Large sparse: use sparse() not Matrix()\nn = 1_000_000\nA_large_sparse = Mat_sum(sparse(..., n, n))\nA_csc = sparse(A_large_sparse)    # Efficient\n# A_dense = Matrix(A_large_sparse)  # Would allocate n×n dense array!\n\nSee Converting to Native Julia Arrays for more details and examples.","category":"section"},{"location":"guide/matrices/#Compatibility-Notes","page":"Matrices","title":"Compatibility Notes","text":"Transpose Reuse: transpose!(B, A) requires that B was created via Mat(A') or has a compatible precursor\nMatrix Multiplication Reuse: mul!(C, A, B) requires pre-allocated C with correct partitions\nDense Operations: Some operations (e.g., \\ with matrix RHS) require dense matrices","category":"section"},{"location":"guide/matrices/#See-Also","page":"Matrices","title":"See Also","text":"Mat_uniform\nMat_sum\nspdiagm\nvcat, hcat, blockdiag\nSafePETSc.is_dense\nInput/Output and Display - Display and conversion operations","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Markdown\nusing Pkg\nusing SafePETSc\nv = string(pkgversion(SafePETSc))\nmd\"# SafePETSc.jl $v\"\n\nSafePETSc is a Julia package that makes distributed PETSc linear algebra feel like native Julia. Instead of writing verbose PETSc C API calls with explicit pointer management and multi-step operations, you can write natural Julia expressions like A * B + C, A \\ b, or y .= 2 .* x .+ 3.\n\nThe Problem: PETSc is powerful but cumbersome. Multiplying two matrices takes many lines of code, requires careful pointer management, and errors cause segfaults and bus errors. Decomposing algebraic expressions into sequences of low-level operations is error-prone and verbose.\n\nThe Solution: SafePETSc implements Julia's array interface for distributed PETSc objects. You get arithmetic operators (+, -, *, \\, /), broadcasting (y .= f.(x)), standard constructors (spdiagm, vcat, hcat, blockdiag), and familiar iteration patterns (eachrow). The package handles the complexity of PETSc's C API and manages object lifecycles automatically through distributed reference management.","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Julia-Native Syntax: Use A * B, A \\ b, A + B, A' just like regular Julia matrices\nBroadcasting: Full support for .+, .*, .= and function broadcasting f.(x)\nStandard Constructors: spdiagm, vcat, hcat, blockdiag work on distributed matrices\nLinear Algebra: Matrix multiplication, solving (\\, /), transpose, in-place operations\nAutomatic Memory Management: Objects are cleaned up automatically across MPI ranks\nVector Pooling: Temporary vectors are reused for efficiency (configurable with ENABLE_VEC_POOL[])\nIteration: Use eachrow(A) for row-wise processing","category":"section"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"using SafePETSc\nusing MPI\n\n# Initialize MPI and PETSc\nSafePETSc.Init()\n\n# Create a distributed matrix\nA = Mat_uniform([2.0 1.0; 1.0 3.0])\n\n# Create a distributed vector\nb = Vec_uniform([1.0, 2.0])\n\n# Solve the linear system\nx = A \\ b\n\n# Display the solution (only on rank 0)\nprintln(io0(), x)","category":"section"},{"location":"#Core-Types","page":"Home","title":"Core Types","text":"SafePETSc provides three main types for distributed linear algebra:\n\nVec{T,Prefix}: Distributed vectors with Julia array-like operations (broadcasting, arithmetic, etc.) and automatic pooling for efficiency\nMat{T,Prefix}: Distributed matrices with arithmetic operators (+, -, *, \\), broadcasting, transpose (A'), and GPU-friendly operations\nKSP{T,Prefix}: Linear solver objects that can be reused for multiple solves with the same matrix","category":"section"},{"location":"#The-Prefix-Type-Parameter","page":"Home","title":"The Prefix Type Parameter","text":"All SafePETSc types take a Prefix type parameter that controls PETSc object configuration. SafePETSc provides two built-in prefix types:\n\nMPIAIJ (default): For sparse matrices and general vectors\nString prefix: \"MPIAIJ_\"\nDefault PETSc matrix type: mpiaij (MPI sparse matrix)\nDefault PETSc vector type: mpi (standard MPI vector)\nUse for: Sparse linear algebra, general computations\nMPIDENSE: For dense matrices\nString prefix: \"MPIDENSE_\"\nDefault PETSc matrix type: mpidense (MPI dense matrix)\nDefault PETSc vector type: mpi (standard MPI vector)\nUse for: Dense linear algebra, operations requiring dense storage (e.g., eachrow)\n\nThe string prefix is used when setting PETSc options:\n\n# Configure GPU acceleration for dense matrices\npetsc_options_insert_string(\"-MPIDENSE_mat_type mpidense\")\n\n# Create matrix with that prefix\nA = Mat_uniform(data; Prefix=MPIDENSE)\n\nNote: All PETSc vectors are inherently dense (they store all elements), so Vec{T,MPIAIJ} and Vec{T,MPIDENSE} differ only in their PETSc options prefix, not their internal storage format.","category":"section"},{"location":"#Memory-Management","page":"Home","title":"Memory Management","text":"Objects are automatically cleaned up when they go out of scope through distributed garbage collection. For faster cleanup in memory-intensive applications, you can manually trigger cleanup:\n\nGC.gc()                          # Run Julia's garbage collector\nSafeMPI.check_and_destroy!()     # Perform distributed cleanup","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(\"SafePETSc\")","category":"section"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"See the Getting Started guide for a tutorial on using SafePETSc.","category":"section"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"}]
}
