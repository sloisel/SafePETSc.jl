<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Matrices · SafePETSc.jl 0.1.16</title><meta name="title" content="Matrices · SafePETSc.jl 0.1.16"/><meta property="og:title" content="Matrices · SafePETSc.jl 0.1.16"/><meta property="twitter:title" content="Matrices · SafePETSc.jl 0.1.16"/><meta name="description" content="Documentation for SafePETSc.jl 0.1.16."/><meta property="og:description" content="Documentation for SafePETSc.jl 0.1.16."/><meta property="twitter:description" content="Documentation for SafePETSc.jl 0.1.16."/><meta property="og:url" content="https://sloisel.github.io/SafePETSc.jl/api/matrices/"/><meta property="twitter:url" content="https://sloisel.github.io/SafePETSc.jl/api/matrices/"/><link rel="canonical" href="https://sloisel.github.io/SafePETSc.jl/api/matrices/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">SafePETSc.jl 0.1.16</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../getting_started/">Getting Started</a></li><li><span class="tocitem">User Guide</span><ul><li><a class="tocitem" href="../../guide/vectors/">Vectors</a></li><li><a class="tocitem" href="../../guide/matrices/">Matrices</a></li><li><a class="tocitem" href="../../guide/solvers/">Linear Solvers</a></li><li><a class="tocitem" href="../../guide/strumpack/">STRUMPACK Support</a></li><li><a class="tocitem" href="../../guide/io/">Input/Output and Display</a></li><li><a class="tocitem" href="../../guide/mpi_programming/">MPI Programming</a></li><li><a class="tocitem" href="../../guide/distributed_refs/">Distributed Reference Management</a></li></ul></li><li><span class="tocitem">API Reference</span><ul><li><a class="tocitem" href="../safempi/">SafeMPI</a></li><li><a class="tocitem" href="../vectors/">Vectors</a></li><li class="is-active"><a class="tocitem" href>Matrices</a><ul class="internal"><li><a class="tocitem" href="#Type"><span>Type</span></a></li><li><a class="tocitem" href="#Prefix-Types"><span>Prefix Types</span></a></li><li><a class="tocitem" href="#Constructors"><span>Constructors</span></a></li><li><a class="tocitem" href="#Concatenation"><span>Concatenation</span></a></li><li><a class="tocitem" href="#Sparse-Diagonal-Matrices"><span>Sparse Diagonal Matrices</span></a></li><li><a class="tocitem" href="#Conversion-and-Display"><span>Conversion and Display</span></a></li><li><a class="tocitem" href="#Utilities"><span>Utilities</span></a></li><li><a class="tocitem" href="#Row-wise-Operations"><span>Row-wise Operations</span></a></li><li><a class="tocitem" href="#Indexing"><span>Indexing</span></a></li><li><a class="tocitem" href="#Operations"><span>Operations</span></a></li><li><a class="tocitem" href="#Block-Matrix-Products"><span>Block Matrix Products</span></a></li></ul></li><li><a class="tocitem" href="../solvers/">Solvers</a></li></ul></li><li><a class="tocitem" href="../../developer/">Developer Guide</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API Reference</a></li><li class="is-active"><a href>Matrices</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Matrices</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Matrices-API-Reference"><a class="docs-heading-anchor" href="#Matrices-API-Reference">Matrices API Reference</a><a id="Matrices-API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Matrices-API-Reference" title="Permalink"></a></h1><p>Distributed matrix operations in SafePETSc.</p><h2 id="Type"><a class="docs-heading-anchor" href="#Type">Type</a><a id="Type-1"></a><a class="docs-heading-anchor-permalink" href="#Type" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="SafePETSc.Mat"><a class="docstring-binding" href="#SafePETSc.Mat"><code>SafePETSc.Mat</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">Mat{T,Prefix}</code></pre><p>A distributed PETSc matrix with element type <code>T</code> and prefix type <code>Prefix</code>, managed by SafePETSc&#39;s reference counting system.</p><p><code>Mat{T,Prefix}</code> is actually a type alias for <code>DRef{_Mat{T,Prefix}}</code>, meaning matrices are automatically tracked across MPI ranks and destroyed collectively when all ranks release their references.</p><p><strong>Construction</strong></p><p>Use <a href="#SafePETSc.Mat_uniform"><code>Mat_uniform</code></a> or <a href="#SafePETSc.Mat_sum"><code>Mat_sum</code></a> to create distributed matrices:</p><pre><code class="language-julia hljs"># Create from uniform data (same on all ranks)
A = Mat_uniform([1.0 2.0; 3.0 4.0])

# Create from sparse contributions (summed across ranks)
using SparseArrays
A = Mat_sum(sparse([1, 2], [1, 2], [1.0, 4.0], 2, 2))</code></pre><p><strong>Operations</strong></p><p>Matrices support standard linear algebra operations:</p><pre><code class="language-julia hljs"># Matrix-vector multiplication
y = A * x

# Matrix-matrix multiplication
C = A * B

# Matrix transpose
B = A&#39;
B = Mat(A&#39;)  # Materialize transpose

# Linear solve
x = A \ b

# Concatenation
C = vcat(A, B)  # or cat(A, B; dims=1)
D = hcat(A, B)  # or cat(A, B; dims=2)
E = blockdiag(A, B)

# Diagonal matrix from vectors
using SparseArrays
A = spdiagm(0 =&gt; diag_vec, 1 =&gt; upper_diag)</code></pre><p>See also: <a href="#SafePETSc.Mat_uniform"><code>Mat_uniform</code></a>, <a href="#SafePETSc.Mat_sum"><code>Mat_sum</code></a>, <a href="../vectors/#SafePETSc.Vec"><code>Vec</code></a>, <a href="../solvers/#SafePETSc.KSP"><code>KSP</code></a></p></div></section></details></article><h2 id="Prefix-Types"><a class="docs-heading-anchor" href="#Prefix-Types">Prefix Types</a><a id="Prefix-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Prefix-Types" title="Permalink"></a></h2><p>The <code>Prefix</code> type parameter determines matrix storage format and PETSc configuration:</p><article><details class="docstring" open="true"><summary id="SafePETSc.MPIAIJ"><a class="docstring-binding" href="#SafePETSc.MPIAIJ"><code>SafePETSc.MPIAIJ</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">MPIAIJ</code></pre><p>Prefix type for sparse matrices and general vectors (default).</p><p><strong>String Prefix</strong></p><p>The string prefix is <code>&quot;MPIAIJ_&quot;</code>, which is prepended to PETSc option names.</p><p><strong>Default PETSc Types</strong></p><ul><li>Matrices: <code>mpiaij</code> (MPI sparse matrix, compressed row storage)</li><li>Vectors: <code>mpi</code> (standard MPI vector)</li></ul><p><strong>Usage</strong></p><p>Use <code>MPIAIJ</code> for:</p><ul><li>Sparse matrices with few nonzeros per row</li><li>Memory-efficient storage of large sparse systems</li><li>Iterative solvers and sparse linear algebra</li><li>General-purpose vector operations</li></ul><p>This is the default prefix type when not specified.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Create sparse matrix (MPIAIJ is the default)
A = Mat_uniform(sparse([1.0 0.0; 0.0 2.0]))

# Explicitly specify MPIAIJ prefix
B = Mat_uniform(data; Prefix=MPIAIJ)

# Configure iterative solver for sparse matrices
petsc_options_insert_string(&quot;-MPIAIJ_ksp_type gmres&quot;)</code></pre><p>See also: <a href="#SafePETSc.MPIDENSE"><code>MPIDENSE</code></a>, <a href="#SafePETSc.Mat"><code>Mat</code></a>, <a href="../vectors/#SafePETSc.Vec"><code>Vec</code></a></p></div></section></details></article><article><details class="docstring" open="true"><summary id="SafePETSc.MPIDENSE"><a class="docstring-binding" href="#SafePETSc.MPIDENSE"><code>SafePETSc.MPIDENSE</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">MPIDENSE</code></pre><p>Prefix type for dense matrix operations and associated vectors.</p><p><strong>String Prefix</strong></p><p>The string prefix is <code>&quot;MPIDENSE_&quot;</code>, which is prepended to PETSc option names.</p><p><strong>Default PETSc Types</strong></p><ul><li>Matrices: <code>mpidense</code> (MPI dense matrix, row-major storage)</li><li>Vectors: <code>mpi</code> (standard MPI vector)</li></ul><p><strong>Usage</strong></p><p>Use <code>MPIDENSE</code> for:</p><ul><li>Dense matrices where all elements are stored</li><li>Operations requiring dense storage (e.g., <code>eachrow</code>)</li><li>Direct solvers and dense linear algebra</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Create dense matrix
A = Mat_uniform([1.0 2.0; 3.0 4.0]; Prefix=MPIDENSE)

# Configure GPU acceleration for dense matrices
petsc_options_insert_string(&quot;-MPIDENSE_mat_type mpidense&quot;)</code></pre><p>See also: <a href="#SafePETSc.MPIAIJ"><code>MPIAIJ</code></a>, <a href="#SafePETSc.Mat"><code>Mat</code></a>, <a href="../vectors/#SafePETSc.Vec"><code>Vec</code></a></p></div></section></details></article><article><details class="docstring" open="true"><summary id="SafePETSc.prefix"><a class="docstring-binding" href="#SafePETSc.prefix"><code>SafePETSc.prefix</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">prefix(::Type{&lt;:Prefix}) -&gt; String</code></pre><p>Return the string prefix for a given prefix type.</p><p>The string prefix is prepended to PETSc option names. For example, with prefix type <code>MPIDENSE</code>, the option <code>-mat_type mpidense</code> becomes <code>-MPIDENSE_mat_type mpidense</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">prefix(MPIDENSE)  # Returns &quot;MPIDENSE_&quot;
prefix(MPIAIJ)    # Returns &quot;MPIAIJ_&quot;</code></pre></div></section></details></article><h2 id="Constructors"><a class="docs-heading-anchor" href="#Constructors">Constructors</a><a id="Constructors-1"></a><a class="docs-heading-anchor-permalink" href="#Constructors" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="SafePETSc.Mat_uniform"><a class="docstring-binding" href="#SafePETSc.Mat_uniform"><code>SafePETSc.Mat_uniform</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">Mat_uniform(A::Matrix{T}; row_partition=default_row_partition(size(A, 1), MPI.Comm_size(MPI.COMM_WORLD)), col_partition=default_row_partition(size(A, 2), MPI.Comm_size(MPI.COMM_WORLD)), Prefix::Type=MPIAIJ) -&gt; DRef{Mat{T,Prefix}}</code></pre><p><strong>MPI Collective</strong></p><p>Create a distributed PETSc matrix from a Julia matrix, asserting uniform distribution across ranks (on MPI.COMM_WORLD).</p><ul><li><code>A::Matrix{T}</code> must be identical on all ranks (<code>mpi_uniform</code>).</li><li><code>row_partition</code> is a Vector{Int} of length nranks+1 where partition[i] is the start row (1-indexed) for rank i-1.</li><li><code>col_partition</code> is a Vector{Int} of length nranks+1 where partition[i] is the start column (1-indexed) for rank i-1.</li><li><code>Prefix</code> is a type parameter for MatSetOptionsPrefix() to set matrix-specific command-line options (default: MPIAIJ).</li><li>Returns a DRef that will destroy the PETSc Mat collectively when all ranks release their reference.</li></ul></div></section><section><div><pre><code class="language-julia hljs">Mat_uniform(A::SparseMatrixCSC{T}; row_partition=default_row_partition(size(A, 1), MPI.Comm_size(MPI.COMM_WORLD)), col_partition=default_row_partition(size(A, 2), MPI.Comm_size(MPI.COMM_WORLD)), Prefix::Type=MPIAIJ) -&gt; DRef{Mat{T,Prefix}}</code></pre><p><strong>MPI Collective</strong></p><p>Create a distributed PETSc matrix from a sparse Julia matrix, asserting uniform distribution across ranks (on MPI.COMM_WORLD).</p><ul><li><code>A::SparseMatrixCSC{T}</code> must be identical on all ranks (<code>mpi_uniform</code>).</li><li><code>row_partition</code> is a Vector{Int} of length nranks+1 where partition[i] is the start row (1-indexed) for rank i-1.</li><li><code>col_partition</code> is a Vector{Int} of length nranks+1 where partition[i] is the start column (1-indexed) for rank i-1.</li><li><code>Prefix</code> is a type parameter for MatSetOptionsPrefix() to set matrix-specific command-line options (default: MPIAIJ).</li><li>Returns a DRef that will destroy the PETSc Mat collectively when all ranks release their reference.</li></ul><p>Each rank inserts only the values from its assigned row partition using INSERT_VALUES mode.</p></div></section></details></article><article><details class="docstring" open="true"><summary id="SafePETSc.Mat_sum"><a class="docstring-binding" href="#SafePETSc.Mat_sum"><code>SafePETSc.Mat_sum</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">Mat_sum(A::SparseMatrixCSC{T}; row_partition=default_row_partition(size(A, 1), MPI.Comm_size(MPI.COMM_WORLD)), col_partition=default_row_partition(size(A, 2), MPI.Comm_size(MPI.COMM_WORLD)), Prefix::Type=MPIAIJ, own_rank_only=false) -&gt; DRef{Mat{T,Prefix}}</code></pre><p><strong>MPI Collective</strong></p><p>Create a distributed PETSc matrix by summing sparse matrices across ranks (on MPI.COMM_WORLD).</p><ul><li><code>A::SparseMatrixCSC{T}</code> can differ across ranks; nonzero entries are summed across all ranks.</li><li><code>row_partition</code> is a Vector{Int} of length nranks+1 where partition[i] is the start row (1-indexed) for rank i-1.</li><li><code>col_partition</code> is a Vector{Int} of length nranks+1 where partition[i] is the start column (1-indexed) for rank i-1.</li><li><code>Prefix</code> is a type parameter for MatSetOptionsPrefix() to set matrix-specific command-line options (default: MPIAIJ).</li><li><code>own_rank_only::Bool</code> (default=false): if true, asserts that all nonzero entries fall within this rank&#39;s row partition.</li><li>Returns a DRef that will destroy the PETSc Mat collectively when all ranks release their reference.</li></ul><p>Uses MatSetValues with ADD_VALUES mode to sum contributions from all ranks.</p></div></section></details></article><h2 id="Concatenation"><a class="docs-heading-anchor" href="#Concatenation">Concatenation</a><a id="Concatenation-1"></a><a class="docs-heading-anchor-permalink" href="#Concatenation" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="Base.cat"><a class="docstring-binding" href="#Base.cat"><code>Base.cat</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">Base.cat(As::Union{Vec{T},Mat{T,Prefix}}...; dims) -&gt; Union{Vec{T}, Mat{T,Prefix}}</code></pre><p><strong>MPI Collective</strong></p><p>Concatenate distributed PETSc vectors and/or matrices along dimension <code>dims</code>.</p><p><strong>Arguments</strong></p><ul><li><code>As::Union{Vec{T},Mat{T,Prefix}}...</code>: One or more vectors or matrices with the same element type <code>T</code></li><li><code>dims</code>: Concatenation dimension (1 for vertical/vcat, 2 for horizontal/hcat)</li></ul><p><strong>Return Type</strong></p><ul><li>Returns <code>Vec{T}</code> when <code>dims=1</code> and result has a single column (vertical stacking of vectors)</li><li>Returns <code>Mat{T,Prefix}</code> otherwise (horizontal concatenation or matrix inputs)</li></ul><p><strong>Requirements</strong></p><p>All inputs must:</p><ul><li>Have the same element type <code>T</code></li><li>Have compatible sizes and partitions for the concatenation dimension<ul><li>For <code>dims=1</code> (vcat): same number of columns and column partition</li><li>For <code>dims=2</code> (hcat): same number of rows and row partition</li></ul></li></ul><p><strong>Automatic Prefix Selection</strong></p><p>The output <code>Prefix</code> type is automatically determined to ensure correctness:</p><ul><li><strong>MPIDENSE</strong> if any input has <code>Prefix=MPIDENSE</code> (dense format required)</li><li><strong>MPIDENSE</strong> if concatenating vectors horizontally with width &gt; 1 (e.g., <code>hcat(x, y)</code>)</li><li>Otherwise, preserves the first input&#39;s <code>Prefix</code></li></ul><p>This ensures that operations like <code>hcat(vec1, vec2)</code> produce dense matrices as expected, since vectors are inherently dense and horizontal concatenation creates a dense result.</p><p><strong>Implementation</strong></p><p>The concatenation is performed by:</p><ol><li>Each rank extracts its owned rows from each input as a Julia sparse matrix</li><li>Standard Julia <code>cat</code> is applied locally on each rank</li><li>The results are combined across ranks using <code>Vec_sum</code> (for single-column results) or <code>Mat_sum</code> (otherwise)</li></ol><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Vertical concatenation (stacking) - returns Vec
x = Vec_uniform([1.0, 2.0, 3.0])
y = Vec_uniform([4.0, 5.0, 6.0])
v = vcat(x, y)  # Returns Vec{Float64} with 6 elements

# Horizontal concatenation - returns Mat
M = hcat(x, y)  # Returns Mat{Float64,MPIDENSE} of size 3×2

# Matrix concatenation - returns Mat
A = Mat_uniform(sparse([1 2; 3 4]))
B = Mat_uniform(sparse([5 6; 7 8]))
C = vcat(A, B)  # Returns Mat{Float64,MPIAIJ} of size 4×2</code></pre><p>See also: <a href="#Base.vcat"><code>vcat</code></a>, <a href="#Base.hcat"><code>hcat</code></a>, <a href="../vectors/#SafePETSc.Vec_sum"><code>Vec_sum</code></a>, <a href="#SafePETSc.Mat_sum"><code>Mat_sum</code></a></p></div></section></details></article><article><details class="docstring" open="true"><summary id="Base.vcat"><a class="docstring-binding" href="#Base.vcat"><code>Base.vcat</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">Base.vcat(As::Union{Vec{T},Mat{T,Prefix}}...) -&gt; Union{Vec{T},Mat{T,Prefix}}</code></pre><p><strong>MPI Collective</strong></p><p>Vertically concatenate (stack) distributed PETSc vectors and/or matrices.</p><p>Equivalent to <code>cat(As...; dims=1)</code>. Stacks inputs vertically, increasing the number of rows while keeping the number of columns constant.</p><p><strong>Return Type</strong></p><ul><li>Returns <code>Vec{T}</code> when concatenating only vectors (single-column result)</li><li>Returns <code>Mat{T,Prefix}</code> when concatenating matrices or when result has multiple columns</li></ul><p><strong>Requirements</strong></p><p>All inputs must have the same number of columns and the same column partition.</p><p><strong>Prefix Selection</strong></p><ul><li>Typically preserves the input <code>Prefix</code> (e.g., <code>MPIAIJ</code> for vectors)</li><li>Upgrades to <code>MPIDENSE</code> if any input is <code>MPIDENSE</code></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Concatenating vectors returns a Vec
x = Vec_uniform([1.0, 2.0])
y = Vec_uniform([3.0, 4.0])
v = vcat(x, y)  # Vec{Float64} with 4 elements

# Concatenating matrices returns a Mat
A = Mat_uniform(sparse([1 2; 3 4]))
B = Mat_uniform(sparse([5 6; 7 8]))
C = vcat(A, B)  # Mat{Float64,MPIAIJ} of size 4×2</code></pre><p>See also: <a href="#Base.cat"><code>cat</code></a>, <a href="#Base.hcat"><code>hcat</code></a></p></div></section></details></article><article><details class="docstring" open="true"><summary id="Base.hcat"><a class="docstring-binding" href="#Base.hcat"><code>Base.hcat</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">Base.hcat(As::Union{Vec{T},Mat{T,Prefix}}...) -&gt; Mat{T,Prefix}</code></pre><p><strong>MPI Collective</strong></p><p>Horizontally concatenate (place side-by-side) distributed PETSc vectors and/or matrices.</p><p>Equivalent to <code>cat(As...; dims=2)</code>. Concatenates inputs horizontally, increasing the number of columns while keeping the number of rows constant.</p><p><strong>Requirements</strong></p><p>All inputs must have the same number of rows and the same row partition.</p><p><strong>Prefix Selection</strong></p><ul><li><strong>Automatically upgrades to <code>MPIDENSE</code></strong> when concatenating vectors (width &gt; 1)</li><li>Upgrades to <code>MPIDENSE</code> if any input is <code>MPIDENSE</code></li><li>Otherwise preserves the input <code>Prefix</code></li></ul><p>The automatic upgrade for vectors is important because vectors are inherently dense, and horizontal concatenation of vectors produces a dense matrix.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = Vec_uniform([1.0, 2.0, 3.0])
y = Vec_uniform([4.0, 5.0, 6.0])
M = hcat(x, y)  # 3×2 Mat{Float64,MPIDENSE} - auto-upgraded!

A = Mat_uniform(sparse([1; 2; 3]))
B = Mat_uniform(sparse([4; 5; 6]))
C = hcat(A, B)  # 3×2 matrix with MPIDENSE</code></pre><p>See also: <a href="#Base.cat"><code>cat</code></a>, <a href="#Base.vcat"><code>vcat</code></a></p></div></section></details></article><article><details class="docstring" open="true"><summary id="SparseArrays.blockdiag"><a class="docstring-binding" href="#SparseArrays.blockdiag"><code>SparseArrays.blockdiag</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">blockdiag(As::Mat{T,Prefix}...) -&gt; Mat{T,Prefix}</code></pre><p><strong>MPI Collective</strong></p><p>Create a block diagonal matrix from distributed PETSc matrices.</p><p>The result is a matrix with the input matrices along the diagonal and zeros elsewhere. All matrices must have the same prefix and element type.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs"># If A is m×n and B is p×q, then blockdiag(A, B) is (m+p)×(n+q)
C = blockdiag(A, B)</code></pre></div></section></details></article><h2 id="Sparse-Diagonal-Matrices"><a class="docs-heading-anchor" href="#Sparse-Diagonal-Matrices">Sparse Diagonal Matrices</a><a id="Sparse-Diagonal-Matrices-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-Diagonal-Matrices" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="SparseArrays.spdiagm"><a class="docstring-binding" href="#SparseArrays.spdiagm"><code>SparseArrays.spdiagm</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">spdiagm(kv::Pair{&lt;:Integer, &lt;:Vec{T}}...; prefix=MPIAIJ) -&gt; Mat{T,Prefix}
spdiagm(m::Integer, n::Integer, kv::Pair{&lt;:Integer, &lt;:Vec{T}}...; prefix=MPIAIJ) -&gt; Mat{T,Prefix}</code></pre><p><strong>MPI Collective</strong></p><p>Create a sparse diagonal matrix from distributed PETSc vectors.</p><p>Each pair <code>k =&gt; v</code> places the vector <code>v</code> on the <code>k</code>-th diagonal:</p><ul><li><code>k = 0</code>: main diagonal</li><li><code>k &gt; 0</code>: superdiagonal</li><li><code>k &lt; 0</code>: subdiagonal</li></ul><p>All vectors must have the same element type <code>T</code>. The matrix dimensions are inferred from the diagonal positions and vector lengths, or can be specified explicitly.</p><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>prefix</code>: Matrix prefix type to use for the result. Defaults to MPIAIJ (sparse).</li><li><code>row_partition</code>: Override the default equal-row partitioning (length <code>nranks+1</code>, start at 1, end at <code>m+1</code>, non-decreasing). Defaults to <code>default_row_partition(m, nranks)</code>.</li><li><code>col_partition</code>: Override the default equal-column partitioning (length <code>nranks+1</code>, start at 1, end at <code>n+1</code>, non-decreasing). Defaults to <code>default_row_partition(n, nranks)</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Create a tridiagonal matrix
A = spdiagm(-1 =&gt; lower, 0 =&gt; diag, 1 =&gt; upper)

# Create a 100×100 matrix with specified vectors on diagonals
B = spdiagm(100, 100, 0 =&gt; v1, 1 =&gt; v2)

# Create MPIAIJ (sparse) matrix
v = Vec_uniform(data)
A_sparse = spdiagm(0 =&gt; v; prefix=MPIAIJ)</code></pre></div></section></details></article><h2 id="Conversion-and-Display"><a class="docs-heading-anchor" href="#Conversion-and-Display">Conversion and Display</a><a id="Conversion-and-Display-1"></a><a class="docs-heading-anchor-permalink" href="#Conversion-and-Display" title="Permalink"></a></h2><p>Convert distributed matrices to Julia arrays for inspection and display:</p><article><details class="docstring" open="true"><summary id="Base.Matrix-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}}"><a class="docstring-binding" href="#Base.Matrix-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}}"><code>Base.Matrix</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">Matrix(x::Mat{T,Prefix}) -&gt; Matrix{T}</code></pre><p><strong>MPI Collective</strong></p><p>Convert a distributed PETSc Mat to a Julia Matrix by gathering all data to all ranks. This is a collective operation - all ranks must call it and will receive the complete matrix.</p><p>For dense matrices, this uses efficient MatDenseGetArrayRead. For other matrix types, it uses MatGetRow to extract each row.</p><p>This is primarily used for display purposes or small matrices. For large matrices, this operation can be expensive as it gathers all data to all ranks.</p></div></section><section><div><pre><code class="language-julia hljs">Base.Matrix(At::LinearAlgebra.Adjoint{T, &lt;:Mat{T}}) -&gt; Matrix{T}</code></pre><p><strong>MPI Collective</strong></p><p>Convert an adjoint of a distributed PETSc Mat to an adjoint Julia Matrix. Equivalent to <code>Matrix(parent(At))&#39;</code>.</p><p>This is a collective operation - all ranks must call it and will receive the complete matrix transpose.</p></div></section></details></article><article><details class="docstring" open="true"><summary id="SparseArrays.sparse-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}}"><a class="docstring-binding" href="#SparseArrays.sparse-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}}"><code>SparseArrays.sparse</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">SparseArrays.sparse(x::Mat{T,Prefix}) -&gt; SparseMatrixCSC{T, Int}</code></pre><p><strong>MPI Collective</strong></p><p>Convert a distributed PETSc Mat to a Julia SparseMatrixCSC by gathering all data to all ranks. This is a collective operation - all ranks must call it and will receive the complete sparse matrix.</p><p>Uses MatGetRow to extract the sparse structure efficiently, preserving sparsity.</p><p>This is primarily used for display purposes or small matrices. For large matrices, this operation can be expensive as it gathers all data to all ranks.</p></div></section><section><div><pre><code class="language-julia hljs">SparseArrays.sparse(At::LinearAlgebra.Adjoint{T, &lt;:Mat{T}}) -&gt; SparseMatrixCSC{T, Int}</code></pre><p><strong>MPI Collective</strong></p><p>Convert an adjoint of a distributed PETSc Mat to an adjoint Julia SparseMatrixCSC. Equivalent to <code>sparse(parent(At))&#39;</code>.</p><p>This is a collective operation - all ranks must call it and will receive the complete sparse matrix transpose.</p></div></section></details></article><article><details class="docstring" open="true"><summary id="SafePETSc.J"><a class="docstring-binding" href="#SafePETSc.J"><code>SafePETSc.J</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">J(x)</code></pre><p><strong>MPI Collective</strong> (when applied to PETSc types)</p><p>Universal conversion function that converts PETSc types to their native Julia equivalents. For non-PETSc types, returns the input unchanged.</p><p>This function provides a uniform interface for converting any SafePETSc object to standard Julia types, automatically choosing the appropriate conversion based on the input type.</p><p><strong>Behavior by Type</strong></p><ul><li><code>Vec{T}</code> → <code>Vector{T}</code> (via <code>Vector(v)</code>)</li><li><code>Mat{T}</code> (dense) → <code>Matrix{T}</code> (via <code>Matrix(A)</code>)</li><li><code>Mat{T}</code> (sparse) → <code>SparseMatrixCSC{T,Int}</code> (via <code>sparse(A)</code>)</li><li><code>Adjoint{T, Vec}</code> → <code>Adjoint{T, Vector{T}}</code></li><li><code>Adjoint{T, Mat}</code> → <code>Adjoint{T, Matrix{T}}</code> or <code>Adjoint{T, SparseMatrixCSC}</code></li><li><code>Pair</code> → converts the value while preserving the key</li><li>Other types → returned unchanged</li></ul><p><strong>Warning</strong></p><p>When applied to PETSc types (<code>Vec</code>, <code>Mat</code>), this is a <strong>collective operation</strong> - all MPI ranks must call it. The result is gathered to all ranks.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Convert Vec to Vector
v = Vec_uniform([1.0, 2.0, 3.0])
v_julia = J(v)  # Vector{Float64}

# Convert dense Mat to Matrix
A = Mat_uniform([1.0 2.0; 3.0 4.0])
A_julia = J(A)  # Matrix{Float64}

# Convert sparse Mat to SparseMatrixCSC
using SparseArrays
B = Mat_uniform(sparse([1.0 0.0; 0.0 2.0]))
B_julia = J(B)  # SparseMatrixCSC{Float64, Int}

# Scalars pass through unchanged
x = J(3.14)  # 3.14

# Useful in generic code
function compare_to_julia(petsc_result, julia_func, args...)
    expected = julia_func(J.(args)...)  # Convert all args
    actual = J(petsc_result)
    return norm(actual - expected)
end</code></pre><p>See also: <a href="../vectors/#Base.Vector-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Vec{T}} where T}"><code>Vector</code></a>, <a href="#Base.Matrix-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}}"><code>Matrix</code></a>, <a href="#SparseArrays.sparse-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}}"><code>sparse</code></a></p></div></section><section><div><pre><code class="language-julia hljs">J(v::Vec{T}) -&gt; Vector{T}</code></pre><p><strong>MPI Collective</strong></p><p>Convert a distributed PETSc Vec to a Julia Vector by gathering all data to all ranks. This is a collective operation - all ranks must call it and will receive the complete vector.</p><p>Equivalent to <code>Vector(v)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">v = Vec_uniform([1.0, 2.0, 3.0, 4.0])
v_julia = J(v)  # Returns Vector{Float64}</code></pre></div></section><section><div><pre><code class="language-julia hljs">J(vt::LinearAlgebra.Adjoint{T, &lt;:Vec{T}}) -&gt; Adjoint{T, Vector{T}}</code></pre><p><strong>MPI Collective</strong></p><p>Convert an adjoint of a distributed PETSc Vec to an adjoint Julia Vector. This is a collective operation - all ranks must call it and will receive the complete adjoint vector.</p><p>Equivalent to <code>Vector(vt)</code>.</p></div></section><section><div><pre><code class="language-julia hljs">J(A::Mat{T,Prefix}) -&gt; Union{Matrix{T}, SparseMatrixCSC{T, Int}}</code></pre><p><strong>MPI Collective</strong></p><p>Convert a distributed PETSc Mat to a Julia array type (Matrix or SparseMatrixCSC). Dense matrices are converted to Matrix, sparse matrices to SparseMatrixCSC.</p><p>This is a collective operation - all ranks must call it and will receive the complete matrix.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = Mat_uniform([1.0 2.0; 3.0 4.0])
A_julia = J(A)  # Returns Matrix{Float64}

B = Mat_uniform(sparse([1.0 0.0; 0.0 2.0]))
B_julia = J(B)  # Returns SparseMatrixCSC{Float64, Int}</code></pre></div></section><section><div><pre><code class="language-julia hljs">J(At::LinearAlgebra.Adjoint{T, &lt;:Mat{T}}) -&gt; Union{Adjoint{T, Matrix{T}}, Adjoint{T, SparseMatrixCSC{T, Int}}}</code></pre><p><strong>MPI Collective</strong></p><p>Convert an adjoint of a distributed PETSc Mat to a Julia adjoint array type. Uses the same logic as J(A) but preserves the adjoint wrapper.</p><p>This is a collective operation - all ranks must call it and will receive the complete adjoint matrix.</p></div></section></details></article><article><details class="docstring" open="true"><summary id="SafePETSc.is_dense"><a class="docstring-binding" href="#SafePETSc.is_dense"><code>SafePETSc.is_dense</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">is_dense(x::Mat{T,Prefix}) -&gt; Bool</code></pre><p><strong>MPI Non-Collective</strong></p><p>Check if a PETSc matrix is a dense matrix type.</p><p>This checks the PETSc matrix type string and returns true if it contains &quot;dense&quot; (case-insensitive). This handles various dense types like &quot;seqdense&quot;, &quot;mpidense&quot;, and vendor-specific dense matrix types.</p></div></section></details></article><p>Display methods (automatically used by <code>println</code>, <code>display</code>, etc.):</p><ul><li><code>show(io::IO, A::Mat)</code> - Display matrix contents (uses dense or sparse format based on type)</li><li><code>show(io::IO, mime::MIME, A::Mat)</code> - Display with MIME type support</li></ul><h2 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="SafePETSc.own_row-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}}"><a class="docstring-binding" href="#SafePETSc.own_row-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}}"><code>SafePETSc.own_row</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">own_row(v::Vec{T}) -&gt; UnitRange{Int}</code></pre><p><strong>MPI Non-Collective</strong></p><p>Return the range of indices owned by the current rank for vector v.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">v = Vec_uniform([1.0, 2.0, 3.0, 4.0])
range = own_row(v)  # e.g., 1:2 on rank 0</code></pre></div></section><section><div><pre><code class="language-julia hljs">own_row(A::Mat{T,Prefix}) -&gt; UnitRange{Int}</code></pre><p><strong>MPI Non-Collective</strong></p><p>Return the range of row indices owned by the current rank for matrix A.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = Mat_uniform([1.0 2.0; 3.0 4.0; 5.0 6.0; 7.0 8.0])
range = own_row(A)  # e.g., 1:2 on rank 0</code></pre></div></section></details></article><h2 id="Row-wise-Operations"><a class="docs-heading-anchor" href="#Row-wise-Operations">Row-wise Operations</a><a id="Row-wise-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Row-wise-Operations" title="Permalink"></a></h2><p>See <a href="../vectors/#SafePETSc.map_rows"><code>map_rows</code></a> in the <a href="../vectors/#Row-wise-Operations">Vectors API</a> - works with both vectors and matrices.</p><h2 id="Indexing"><a class="docs-heading-anchor" href="#Indexing">Indexing</a><a id="Indexing-1"></a><a class="docs-heading-anchor-permalink" href="#Indexing" title="Permalink"></a></h2><p>Non-collective element and range access:</p><article><details class="docstring" open="true"><summary id="Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, Colon, Int64}"><a class="docstring-binding" href="#Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, Colon, Int64}"><code>Base.getindex</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">Base.getindex(A::Mat{T,Prefix}, ::Colon, k::Int) -&gt; Vec{T}</code></pre><p><strong>MPI Non-Collective</strong></p><p>Extract column k from matrix A, returning a distributed vector.</p><p>Each rank extracts its owned rows from column k. The resulting vector has the same row partition as matrix A.</p><p>Uses efficient bulk operations: MatDenseGetArrayRead for dense matrices, MatGetRow for sparse matrices.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = Mat_uniform([1.0 2.0 3.0; 4.0 5.0 6.0])
v = A[:, 2]  # Extract second column: [2.0, 5.0]</code></pre></div></section></details></article><article><details class="docstring" open="true"><summary id="Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, Int64, Int64}"><a class="docstring-binding" href="#Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, Int64, Int64}"><code>Base.getindex</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">Base.getindex(A::Mat{T}, i::Int, j::Int) -&gt; T</code></pre><p><strong>MPI Non-Collective</strong></p><p>Get the value at position (i, j) from a distributed matrix.</p><p>The row index i must be wholly contained in the current rank&#39;s row ownership range. If not, the function will abort with an error message and stack trace.</p><p>This is a non-collective operation.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = Mat_uniform([1.0 2.0; 3.0 4.0])
# On rank that owns row 1:
val = A[1, 2]  # Returns 2.0</code></pre></div></section></details></article><article><details class="docstring" open="true"><summary id="Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, UnitRange{Int64}, Int64}"><a class="docstring-binding" href="#Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, UnitRange{Int64}, Int64}"><code>Base.getindex</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">Base.getindex(A::Mat{T}, range_i::UnitRange{Int}, j::Int) -&gt; Vector{T}</code></pre><p><strong>MPI Non-Collective</strong></p><p>Extract a contiguous range of rows from column j of a distributed matrix.</p><p>The row range must be wholly contained in the current rank&#39;s row ownership range. If not, the function will abort with an error message and stack trace.</p><p>This is a non-collective operation.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = Mat_uniform([1.0 2.0; 3.0 4.0; 5.0 6.0])
# On rank that owns rows 1:2:
vals = A[1:2, 2]  # Returns [2.0, 4.0]</code></pre></div></section></details></article><article><details class="docstring" open="true"><summary id="Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, Int64, UnitRange{Int64}}"><a class="docstring-binding" href="#Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, Int64, UnitRange{Int64}}"><code>Base.getindex</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">Base.getindex(A::Mat{T}, i::Int, range_j::UnitRange{Int}) -&gt; Vector{T}</code></pre><p><strong>MPI Non-Collective</strong></p><p>Extract a contiguous range of columns from row i of a distributed matrix.</p><p>The row index i must be wholly contained in the current rank&#39;s row ownership range. If not, the function will abort with an error message and stack trace.</p><p>This is a non-collective operation.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = Mat_uniform([1.0 2.0 3.0; 4.0 5.0 6.0])
# On rank that owns row 1:
vals = A[1, 2:3]  # Returns [2.0, 3.0]</code></pre></div></section></details></article><article><details class="docstring" open="true"><summary id="Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, UnitRange{Int64}, UnitRange{Int64}}"><a class="docstring-binding" href="#Base.getindex-Tuple{SafePETSc.SafeMPI.DRef{SafePETSc._Mat{T, Prefix}} where {T, Prefix}, UnitRange{Int64}, UnitRange{Int64}}"><code>Base.getindex</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">Base.getindex(A::Mat{T}, range_i::UnitRange{Int}, range_j::UnitRange{Int}) -&gt; Union{Matrix{T}, SparseMatrixCSC{T}}</code></pre><p><strong>MPI Non-Collective</strong></p><p>Extract a submatrix from a distributed matrix.</p><p>The row range must be wholly contained in the current rank&#39;s row ownership range. If not, the function will abort with an error message and stack trace.</p><p>Returns a dense Matrix if A is dense, otherwise returns a SparseMatrixCSC.</p><p>This is a non-collective operation.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = Mat_uniform([1.0 2.0 3.0; 4.0 5.0 6.0; 7.0 8.0 9.0])
# On rank that owns rows 1:2:
submat = A[1:2, 2:3]  # Returns [2.0 3.0; 5.0 6.0]</code></pre></div></section></details></article><h2 id="Operations"><a class="docs-heading-anchor" href="#Operations">Operations</a><a id="Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Operations" title="Permalink"></a></h2><h3 id="Linear-Algebra"><a class="docs-heading-anchor" href="#Linear-Algebra">Linear Algebra</a><a id="Linear-Algebra-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Algebra" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Matrix-vector multiplication
y = A * x
LinearAlgebra.mul!(y, A, x)

# Matrix-matrix multiplication
C = A * B
LinearAlgebra.mul!(C, A, B)

# Transpose
B = A&#39;
B = Mat(A&#39;)                            # Materialize transpose
LinearAlgebra.transpose!(B, A)         # In-place transpose

# Adjoint-vector multiplication
w = v&#39; * A
LinearAlgebra.mul!(w, v&#39;, A)</code></pre><h3 id="Properties"><a class="docs-heading-anchor" href="#Properties">Properties</a><a id="Properties-1"></a><a class="docs-heading-anchor-permalink" href="#Properties" title="Permalink"></a></h3><pre><code class="language-julia hljs">T = eltype(A)                          # Element type
m, n = size(A)                         # Dimensions
m = size(A, 1)                         # Rows
n = size(A, 2)                         # Columns</code></pre><h3 id="Iteration"><a class="docs-heading-anchor" href="#Iteration">Iteration</a><a id="Iteration-1"></a><a class="docs-heading-anchor-permalink" href="#Iteration" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Iterate over rows (works for both dense and sparse matrices)
for row in eachrow(A)
    # For dense (MPIDENSE): row is a view of the matrix row
    # For sparse (MPIAIJ): row is a SparseVector efficiently preserving sparsity
    process(row)
end</code></pre><h2 id="Block-Matrix-Products"><a class="docs-heading-anchor" href="#Block-Matrix-Products">Block Matrix Products</a><a id="Block-Matrix-Products-1"></a><a class="docs-heading-anchor-permalink" href="#Block-Matrix-Products" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="SafePETSc.BlockProduct"><a class="docstring-binding" href="#SafePETSc.BlockProduct"><code>SafePETSc.BlockProduct</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">BlockProduct{T,Prefix}</code></pre><p>Represents a product of block matrices with pre-allocated storage for efficient recomputation.</p><p>A block matrix is a Julia <code>Matrix</code> where each element is a <code>Mat</code>, <code>Mat&#39;</code>, <code>Vec</code>, <code>Vec&#39;</code>, scalar, or <code>nothing</code>.</p><p><strong>Fields</strong></p><ul><li><code>prod::Vector{Matrix{BlockElement{T,Prefix}}}</code>: The sequence of block matrices to multiply</li><li><code>result::Union{Matrix{BlockElement{T,Prefix}}, Nothing}</code>: Pre-allocated result (allocated on first <code>calculate!</code> call)</li><li><code>intermediates::Vector{Matrix{BlockElement{T,Prefix}}}</code>: Pre-allocated intermediate results for chained products</li></ul><p><strong>Type Parameters</strong></p><ul><li><code>T</code>: Element type (e.g., Float64)</li><li><code>Prefix</code>: PETSc prefix type (e.g., MPIAIJ, MPIDENSE) - must match all contained objects</li></ul><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">BlockProduct(prod::Vector{Matrix}; Prefix::Type=MPIAIJ)</code></pre><p>Validates dimensions and creates a BlockProduct. Actual allocation of result and intermediates happens lazily on the first call to <code>calculate!</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Create block matrices
A = [M1 M2; M3 M4]  # 2x2 block of Mat{Float64,MPIAIJ}
B = [N1 N2; N3 N4]

# Create product (no allocation yet)
bp = BlockProduct([A, B])

# Compute A * B (allocates result on first call)
C = calculate!(bp)

# Subsequent calls reuse allocations
C2 = calculate!(bp)</code></pre></div></section></details></article><article><details class="docstring" open="true"><summary id="SafePETSc.calculate!"><a class="docstring-binding" href="#SafePETSc.calculate!"><code>SafePETSc.calculate!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">calculate!(bp::BlockProduct)</code></pre><p><strong>MPI Collective</strong></p><p>Recompute the product after modifying input matrices/vectors, reusing cached PETSc objects.</p><p>After the user modifies entries in bp.prod[k][i,j] matrices or vectors, calling this function updates bp.result using in-place operations on the cached intermediate results.</p><p>This avoids allocating new PETSc Mat/Vec objects.</p><p>Returns the updated result as a block matrix (Julia Matrix of BlockElements).</p><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Create and compute initial product
bp = BlockProduct([A, B])
result1 = bp.result

# Modify input matrix entries
# (modify A[1,1] entries here)

# Recompute with cached objects
calculate!(bp)
result2 = bp.result

# result2 has updated values but same PETSc object identity</code></pre></div></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../vectors/">« Vectors</a><a class="docs-footer-nextpage" href="../solvers/">Solvers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Thursday 27 November 2025 23:11">Thursday 27 November 2025</span>. Using Julia version 1.12.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
