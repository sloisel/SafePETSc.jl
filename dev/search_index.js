var documenterSearchIndex = {"docs":
[{"location":"guide/vectors/#Vectors","page":"Vectors","title":"Vectors","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"SafePETSc provides distributed vectors through the Vec{T} type, which wraps PETSc's distributed vector functionality with automatic memory management.","category":"page"},{"location":"guide/vectors/#Creating-Vectors","page":"Vectors","title":"Creating Vectors","text":"","category":"section"},{"location":"guide/vectors/#Uniform-Distribution","page":"Vectors","title":"Uniform Distribution","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"Use Vec_uniform when all ranks have the same data:","category":"page"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"# Create a vector from uniform data\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\n\n# With custom partition\npartition = [1, 3, 5]  # rank 0: rows 1-2, rank 1: rows 3-4\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0]; row_partition=partition)\n\n# With PETSc options prefix\nv = Vec_uniform([1.0, 2.0]; prefix=\"my_vec_\")","category":"page"},{"location":"guide/vectors/#Sum-Distribution","page":"Vectors","title":"Sum Distribution","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"Use Vec_sum when ranks contribute sparse entries:","category":"page"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"using SparseArrays\n\n# Each rank contributes different entries\n# All contributions are summed\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nindices = [rank * 2 + 1, rank * 2 + 2]\nvalues = [1.0, 2.0]\nv = Vec_sum(sparsevec(indices, values, 10))","category":"page"},{"location":"guide/vectors/#Helper-Constructors","page":"Vectors","title":"Helper Constructors","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"Create vectors similar to existing ones:","category":"page"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"# Zero vector with same size/partition as x\ny = zeros_like(x)\n\n# Ones vector\ny = ones_like(x)\n\n# Filled with specific value\ny = fill_like(x, 3.14)\n\n# With different element type\ny = zeros_like(x; T2=Float32)","category":"page"},{"location":"guide/vectors/#Vector-Operations","page":"Vectors","title":"Vector Operations","text":"","category":"section"},{"location":"guide/vectors/#Broadcasting","page":"Vectors","title":"Broadcasting","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"Vectors support Julia's broadcasting syntax:","category":"page"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"# Element-wise operations\ny = x .+ 1.0\ny = 2.0 .* x\ny = x .^ 2\n\n# Vector-vector operations\nz = x .+ y\nz = x .* y\n\n# In-place operations\ny .= x .+ 1.0\ny .= 2.0 .* x .+ y","category":"page"},{"location":"guide/vectors/#Arithmetic","page":"Vectors","title":"Arithmetic","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"# Addition and subtraction\nz = x + y\nz = x - y\n\n# Mixed with scalars\nz = x + 1.0\nz = 2.0 - x\n\n# Unary operations\nz = -x\nz = +x","category":"page"},{"location":"guide/vectors/#Linear-Algebra","page":"Vectors","title":"Linear Algebra","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"# Adjoint (transpose)\nx_adj = x'\n\n# Adjoint-matrix multiplication\nresult = x' * A  # Returns adjoint vector","category":"page"},{"location":"guide/vectors/#Partitioning","page":"Vectors","title":"Partitioning","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"Vectors are partitioned across ranks to distribute work and memory.","category":"page"},{"location":"guide/vectors/#Default-Partitioning","page":"Vectors","title":"Default Partitioning","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"# Equal distribution\nn = 100\nnranks = MPI.Comm_size(MPI.COMM_WORLD)\npartition = default_row_partition(n, nranks)\n\n# For n=100, nranks=4:\n# partition = [1, 26, 51, 76, 101]\n# rank 0: rows 1-25 (25 elements)\n# rank 1: rows 26-50 (25 elements)\n# rank 2: rows 51-75 (25 elements)\n# rank 3: rows 76-100 (25 elements)","category":"page"},{"location":"guide/vectors/#Custom-Partitioning","page":"Vectors","title":"Custom Partitioning","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"# Unequal distribution\npartition = [1, 10, 30, 101]  # Different sizes per rank\nv = Vec_uniform(data; row_partition=partition)","category":"page"},{"location":"guide/vectors/#Partition-Requirements","page":"Vectors","title":"Partition Requirements","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"Length nranks + 1\nFirst element is 1\nLast element is n + 1 (where n is vector length)\nStrictly increasing","category":"page"},{"location":"guide/vectors/#Properties","page":"Vectors","title":"Properties","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"# Element type\nT = eltype(v)  # e.g., Float64\n\n# Size\nn = length(v)\nn = size(v, 1)\n\n# Partition information\npartition = v.obj.row_partition\nprefix = v.obj.prefix","category":"page"},{"location":"guide/vectors/#PETSc-Options","page":"Vectors","title":"PETSc Options","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"Use prefixes to configure PETSc behavior:","category":"page"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"# Set options globally\npetsc_options_insert_string(\"-my_vec_type cuda\")\n\n# Create vector with prefix\nv = Vec_uniform(data; prefix=\"my_vec_\")\n\n# Now PETSc will use the \"cuda\" type for this vector","category":"page"},{"location":"guide/vectors/#Examples","page":"Vectors","title":"Examples","text":"","category":"section"},{"location":"guide/vectors/#Parallel-Computation","page":"Vectors","title":"Parallel Computation","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"using SafePETSc\nusing MPI\n\nSafePETSc.Init()\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nnranks = MPI.Comm_size(MPI.COMM_WORLD)\n\n# Each rank creates local data\nn_global = 1000\npartition = default_row_partition(n_global, nranks)\nlo = partition[rank + 1]\nhi = partition[rank + 2] - 1\n\n# Generate data based on rank\ndata = collect(range(1.0, length=n_global))\n\n# Create distributed vector\nv = Vec_uniform(data)\n\n# Compute: y = 2x + 1\ny = 2.0 .* v .+ 1.0\n\nif rank == 0\n    println(\"Computation complete\")\nend","category":"page"},{"location":"guide/vectors/#Sparse-Contributions","page":"Vectors","title":"Sparse Contributions","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"using SafePETSc\nusing SparseArrays\nusing MPI\n\nSafePETSc.Init()\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nn = 100\n\n# Each rank contributes to different parts\n# Use own_rank_only=true to assert local contributions\nlo = rank * 25 + 1\nhi = (rank + 1) * 25\nindices = collect(lo:hi)\nvalues = ones(length(indices)) * (rank + 1)\n\nv = Vec_sum(sparsevec(indices, values, n); own_rank_only=true)","category":"page"},{"location":"guide/vectors/#Performance-Tips","page":"Vectors","title":"Performance Tips","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"Use Broadcasting: In-place broadcasting (y .= ...) avoids allocations\nBatch Operations: Combine multiple operations in one broadcast\nAvoid Extraction: Keep data in distributed vectors; don't extract to Julia arrays\nGPU-Aware: Set PETSc options for GPU execution","category":"page"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"# Good: in-place, batched\ny .= 2.0 .* x .+ 3.0 .* z .+ 1.0\n\n# Less good: multiple allocations\ny = 2.0 * x\ny = y + 3.0 * z\ny = y + 1.0","category":"page"},{"location":"guide/vectors/#See-Also","page":"Vectors","title":"See Also","text":"","category":"section"},{"location":"guide/vectors/","page":"Vectors","title":"Vectors","text":"Vec_uniform\nVec_sum\nzeros_like\nones_like\nfill_like","category":"page"},{"location":"api/safempi/#SafeMPI-API-Reference","page":"SafeMPI","title":"SafeMPI API Reference","text":"","category":"section"},{"location":"api/safempi/","page":"SafeMPI","title":"SafeMPI","text":"The SafeMPI module provides distributed reference management for MPI-based parallel computing.","category":"page"},{"location":"api/safempi/#Core-Types","page":"SafeMPI","title":"Core Types","text":"","category":"section"},{"location":"api/safempi/#SafePETSc.SafeMPI.DRef","page":"SafeMPI","title":"SafePETSc.SafeMPI.DRef","text":"DRef{T}\n\nA distributed reference to an object of type T that is managed across MPI ranks.\n\nWhen all ranks have released their references (via garbage collection or explicit release!), the object is collectively destroyed on all ranks using the type's destroy_obj! method.\n\nConstructor\n\nDRef(obj::T; manager=default_manager[]) -> DRef{T}\n\nCreate a distributed reference to obj. The type T must opt-in to distributed management by defining destroy_trait(::Type{T}) = CanDestroy() and implementing destroy_obj!(obj::T).\n\nFinalizers automatically call release!() when the DRef is garbage collected, so manual cleanup is optional. Call check_and_destroy!() to perform the actual collective destruction.\n\nExample\n\n# Define a type that can be managed\nstruct MyDistributedObject\n    data::Vector{Float64}\nend\n\nSafeMPI.destroy_trait(::Type{MyDistributedObject}) = SafeMPI.CanDestroy()\nSafeMPI.destroy_obj!(obj::MyDistributedObject) = println(\"Destroying object\")\n\n# Create a distributed reference\nref = DRef(MyDistributedObject([1.0, 2.0, 3.0]))\n# ref.obj accesses the underlying object\n# When ref is garbage collected and check_and_destroy!() is called, the object is destroyed\n\nSee also: DistributedRefManager, check_and_destroy!, destroy_trait\n\n\n\n\n\n","category":"type"},{"location":"api/safempi/#SafePETSc.SafeMPI.DistributedRefManager","page":"SafeMPI","title":"SafePETSc.SafeMPI.DistributedRefManager","text":"DistributedRefManager\n\nManages reference counting and collective destruction of distributed objects across MPI ranks.\n\nRank 0 acts as the coordinator, tracking reference counts in counter_pool. Other ranks send release messages to rank 0 using MPI tag RELEASE_TAG = 1001. Released IDs are recycled via free_ids to prevent unbounded growth in long-running applications.\n\nFields\n\ndestroy_early::Bool: If false (default), objects are destroyed when all ranks release them. If true, objects are destroyed as soon as any rank releases. Warning: Setting this to true is safe for pure SPMD code, but unsafe when rank-dependent flow control leads to different local storage of PETSc data structures across ranks.\n\nSee also: DRef, check_and_destroy!, default_manager\n\n\n\n\n\n","category":"type"},{"location":"api/safempi/#Reference-Management","page":"SafeMPI","title":"Reference Management","text":"","category":"section"},{"location":"api/safempi/#SafePETSc.SafeMPI.check_and_destroy!","page":"SafeMPI","title":"SafePETSc.SafeMPI.check_and_destroy!","text":"check_and_destroy!(manager=default_manager[]; max_check_count::Integer=1)\n\nPerform garbage collection and process pending object releases, destroying objects when all ranks have released their references.\n\nThis function must be called explicitly to allow controlled cleanup points in the application. It performs a full garbage collection to trigger finalizers, then processes all pending release messages and collectively destroys objects that are ready.\n\nThe max_check_count parameter controls throttling: the function only performs cleanup every max_check_count calls. This reduces overhead in tight loops.\n\nExample\n\nSafeMPI.check_and_destroy!()  # Process releases immediately\nSafeMPI.check_and_destroy!(max_check_count=10)  # Only cleanup every 10th call\n\nSee also: DRef, DistributedRefManager\n\n\n\n\n\n","category":"function"},{"location":"api/safempi/#SafePETSc.SafeMPI.destroy_obj!","page":"SafeMPI","title":"SafePETSc.SafeMPI.destroy_obj!","text":"destroy_obj!(obj)\n\nTrait method called to collectively destroy an object when all ranks have released their references. Types that opt-in to distributed reference management must implement this method.\n\nExample\n\nSafeMPI.destroy_obj!(obj::MyType) = begin\n    # Perform collective cleanup (e.g., free MPI/PETSc resources)\n    cleanup_resources(obj)\nend\n\nSee also: DRef, destroy_trait\n\n\n\n\n\n","category":"function"},{"location":"api/safempi/#SafePETSc.SafeMPI.default_manager","page":"SafeMPI","title":"SafePETSc.SafeMPI.default_manager","text":"default_manager\n\nThe default DistributedRefManager instance used by all DRef objects unless explicitly overridden. Automatically initialized when the module loads.\n\n\n\n\n\n","category":"constant"},{"location":"api/safempi/#Trait-System","page":"SafeMPI","title":"Trait System","text":"","category":"section"},{"location":"api/safempi/#SafePETSc.SafeMPI.DestroySupport","page":"SafeMPI","title":"SafePETSc.SafeMPI.DestroySupport","text":"DestroySupport\n\nAbstract type for the trait system controlling which types can be managed by DRef. See CanDestroy and CannotDestroy.\n\n\n\n\n\n","category":"type"},{"location":"api/safempi/#SafePETSc.SafeMPI.CanDestroy","page":"SafeMPI","title":"SafePETSc.SafeMPI.CanDestroy","text":"CanDestroy <: DestroySupport\n\nTrait indicating that a type can be managed by DRef and supports collective destruction. Types must opt-in by defining destroy_trait(::Type{YourType}) = CanDestroy().\n\n\n\n\n\n","category":"type"},{"location":"api/safempi/#SafePETSc.SafeMPI.CannotDestroy","page":"SafeMPI","title":"SafePETSc.SafeMPI.CannotDestroy","text":"CannotDestroy <: DestroySupport\n\nTrait indicating that a type cannot be managed by DRef (default for all types).\n\n\n\n\n\n","category":"type"},{"location":"api/safempi/#SafePETSc.SafeMPI.destroy_trait","page":"SafeMPI","title":"SafePETSc.SafeMPI.destroy_trait","text":"destroy_trait(::Type) -> DestroySupport\n\nTrait function determining whether a type can be managed by DRef.\n\nReturns CanDestroy() for types that opt-in to distributed reference management, or CannotDestroy() for types that don't support it (default).\n\nExample\n\n# Opt-in a custom type\nSafeMPI.destroy_trait(::Type{MyType}) = SafeMPI.CanDestroy()\n\n\n\n\n\n","category":"function"},{"location":"api/safempi/#MPI-Utilities","page":"SafeMPI","title":"MPI Utilities","text":"","category":"section"},{"location":"api/safempi/#SafePETSc.SafeMPI.mpi_any","page":"SafeMPI","title":"SafePETSc.SafeMPI.mpi_any","text":"mpi_any(local_bool::Bool, comm=MPI.COMM_WORLD) -> Bool\n\nCollective logical OR reduction across all ranks in comm.\n\nReturns true on all ranks if any rank has local_bool == true, otherwise returns false on all ranks. This is useful for checking whether any rank encountered an error or special condition.\n\nExample\n\nlocal_error = (x < 0)  # Some local condition\nif SafeMPI.mpi_any(local_error)\n    # At least one rank has an error, all ranks enter this branch\n    error(\"Error detected on at least one rank\")\nend\n\nSee also: @mpiassert\n\n\n\n\n\n","category":"function"},{"location":"api/safempi/#SafePETSc.SafeMPI.mpierror","page":"SafeMPI","title":"SafePETSc.SafeMPI.mpierror","text":"mpierror(msg::AbstractString, trace::Bool; comm=MPI.COMM_WORLD, code::Integer=1)\n\nBest-effort MPI-wide error terminator that avoids hangs:\n\nPrints [rank N] ERROR: msg on each process that reaches it\nIf trace is true, prints a backtrace\nIf MPI is initialized, aborts the communicator to cleanly stop all ranks (avoids deadlocks if other ranks are not in the same code path)\nFalls back to exit(code) if MPI is not initialized or already finalized\n\n\n\n\n\n","category":"function"},{"location":"api/safempi/#SafePETSc.SafeMPI.@mpiassert","page":"SafeMPI","title":"SafePETSc.SafeMPI.@mpiassert","text":"@mpiassert cond [message]\n\nMPI-aware assertion that checks cond on all ranks and triggers collective error handling if any rank fails the assertion.\n\nEach rank evaluates cond locally. If any rank has cond == false, all ranks are notified via mpi_any() and collectively enter error handling via mpierror(). Only ranks where the assertion failed will print a backtrace.\n\nThe assertion is skipped entirely if enable_assert[] == false (see set_assert).\n\nArguments\n\ncond: Boolean expression to check (assertion passes when cond == true)\nmessage: Optional custom error message (defaults to auto-generated message with file/line info)\n\nExample\n\n# Assert that all ranks have the same value\n@mpiassert SafeMPI.mpi_uniform(A) \"Matrix A must be uniform across ranks\"\n\n# Assert a local condition that must hold on all ranks\n@mpiassert n > 0 \"Array size must be positive\"\n\nSee also: mpi_any, mpierror, set_assert\n\n\n\n\n\n","category":"macro"},{"location":"api/safempi/#Configuration","page":"SafeMPI","title":"Configuration","text":"","category":"section"},{"location":"api/safempi/#SafePETSc.SafeMPI.enable_assert","page":"SafeMPI","title":"SafePETSc.SafeMPI.enable_assert","text":"enable_assert\n\nGlobal flag controlling whether @mpiassert macros perform their checks. Set to false to disable all MPI assertions for performance. Default is true.\n\nSee also: set_assert, @mpiassert\n\n\n\n\n\n","category":"constant"},{"location":"api/safempi/#SafePETSc.SafeMPI.set_assert","page":"SafeMPI","title":"SafePETSc.SafeMPI.set_assert","text":"set_assert(x::Bool) -> nothing\n\nEnable (true) or disable (false) MPI assertion checks via @mpiassert.\n\nExample\n\nSafeMPI.set_assert(false)  # Disable assertions\nSafeMPI.set_assert(true)   # Re-enable assertions\n\n\n\n\n\n","category":"function"},{"location":"guide/distributed_refs/#Distributed-Reference-Management","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"SafePETSc's core feature is automatic distributed reference management via the SafeMPI module. This ensures that distributed objects are properly cleaned up across all MPI ranks.","category":"page"},{"location":"guide/distributed_refs/#The-Problem","page":"Distributed Reference Management","title":"The Problem","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"In MPI-based parallel computing, objects like PETSc vectors and matrices exist on all ranks. Destroying such objects requires collective MPI calls—all ranks must participate. This creates challenges:","category":"page"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"Premature destruction: If one rank destroys an object while others still need it, the program crashes\nMemory leaks: If ranks don't coordinate cleanup, objects leak memory\nComplex coordination: Manual reference counting is error-prone","category":"page"},{"location":"guide/distributed_refs/#The-Solution:-DRef","page":"Distributed Reference Management","title":"The Solution: DRef","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"SafePETSc uses DRef{T} (Distributed Reference) to automatically track object lifetimes:","category":"page"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"using SafePETSc\n\n# Create a distributed vector (returns a DRef{_Vec{Float64}})\nv = Vec_uniform([1.0, 2.0, 3.0])\n\n# Use it normally\ny = v .+ 1.0\n\n# When v goes out of scope and is garbage collected,\n# SafePETSc coordinates cleanup across all ranks","category":"page"},{"location":"guide/distributed_refs/#How-It-Works","page":"Distributed Reference Management","title":"How It Works","text":"","category":"section"},{"location":"guide/distributed_refs/#Reference-Counting","page":"Distributed Reference Management","title":"Reference Counting","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"Rank 0 as Coordinator: Rank 0 maintains a reference count for each distributed object\nAutomatic Release: When a DRef is garbage collected, its finalizer sends a \"release\" message to rank 0\nCleanup Points: At check_and_destroy! calls (automatically inserted at object creation), SafePETSc:\nTriggers garbage collection\nProcesses release messages\nIdentifies objects where all ranks have released their references\nBroadcasts destruction commands\nAll ranks destroy the object simultaneously","category":"page"},{"location":"guide/distributed_refs/#Architecture","page":"Distributed Reference Management","title":"Architecture","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"Rank 0 (Coordinator)          Other Ranks\n┌─────────────────┐          ┌─────────────────┐\n│ counter_pool    │◄─────────│ Send release    │\n│ {id → count}    │ MPI msgs │ messages        │\n│                 │          │                 │\n│ When count ==   │          │                 │\n│ nranks:         │          │                 │\n│ Broadcast       │─────────►│ Receive &       │\n│ destroy command │          │ destroy object  │\n└─────────────────┘          └─────────────────┘","category":"page"},{"location":"guide/distributed_refs/#Trait-Based-Opt-In","page":"Distributed Reference Management","title":"Trait-Based Opt-In","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"Types must explicitly opt-in to distributed management:","category":"page"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"# Define your distributed type\nstruct MyDistributedObject\n    data::Vector{Float64}\n    # ... MPI-based fields\nend\n\n# Opt-in to distributed management\nSafeMPI.destroy_trait(::Type{MyDistributedObject}) = SafeMPI.CanDestroy()\n\n# Implement cleanup\nfunction SafeMPI.destroy_obj!(obj::MyDistributedObject)\n    # Perform collective cleanup (e.g., MPI_Free, PETSc destroy)\n    # This is called on ALL ranks simultaneously\n    cleanup_mpi_resources(obj)\nend\n\n# Now you can wrap it\nref = DRef(MyDistributedObject(...))","category":"page"},{"location":"guide/distributed_refs/#Controlling-Cleanup","page":"Distributed Reference Management","title":"Controlling Cleanup","text":"","category":"section"},{"location":"guide/distributed_refs/#Automatic-Cleanup","page":"Distributed Reference Management","title":"Automatic Cleanup","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"Cleanup happens automatically at object creation:","category":"page"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"# Every 10th object creation triggers cleanup\nv1 = Vec_uniform([1.0, 2.0])  # May trigger cleanup\nv2 = Vec_uniform([3.0, 4.0])  # May trigger cleanup\n# ...","category":"page"},{"location":"guide/distributed_refs/#Explicit-Cleanup","page":"Distributed Reference Management","title":"Explicit Cleanup","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"You can manually trigger cleanup:","category":"page"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"# Force immediate cleanup\nSafeMPI.check_and_destroy!()\n\n# Or with throttling\nSafeMPI.check_and_destroy!(max_check_count=10)","category":"page"},{"location":"guide/distributed_refs/#Disabling-Assertions","page":"Distributed Reference Management","title":"Disabling Assertions","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"For performance in production:","category":"page"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"SafeMPI.set_assert(false)  # Disable @mpiassert checks","category":"page"},{"location":"guide/distributed_refs/#Best-Practices","page":"Distributed Reference Management","title":"Best Practices","text":"","category":"section"},{"location":"guide/distributed_refs/#1.-Let-Scoping-Work-for-You","page":"Distributed Reference Management","title":"1. Let Scoping Work for You","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"function compute_something()\n    A = Mat_uniform(...)\n    b = Vec_uniform(...)\n    x = A \\ b\n    # A, b, x cleaned up when function exits\n    return extract_result(x)\nend","category":"page"},{"location":"guide/distributed_refs/#2.-Avoid-Premature-Manual-Cleanup","page":"Distributed Reference Management","title":"2. Avoid Premature Manual Cleanup","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"# Don't do this:\nv = Vec_uniform([1.0, 2.0])\nSafeMPI.check_and_destroy!()  # v might still be in use!\ny = v .+ 1.0  # May crash\n\n# Instead, let automatic cleanup handle it\nv = Vec_uniform([1.0, 2.0])\ny = v .+ 1.0\n# cleanup happens automatically at safe points","category":"page"},{"location":"guide/distributed_refs/#3.-Explicit-Cleanup-in-Long-Running-Loops","page":"Distributed Reference Management","title":"3. Explicit Cleanup in Long-Running Loops","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"for i in 1:1000000\n    v = Vec_uniform(data[i])\n    result[i] = compute(v)\n\n    # Periodic cleanup to avoid accumulation\n    if i % 100 == 0\n        SafeMPI.check_and_destroy!()\n    end\nend","category":"page"},{"location":"guide/distributed_refs/#Debugging","page":"Distributed Reference Management","title":"Debugging","text":"","category":"section"},{"location":"guide/distributed_refs/#Check-Reference-Counts","page":"Distributed Reference Management","title":"Check Reference Counts","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"# Access the default manager\nmanager = SafeMPI.default_manager[]\n\n# Inspect state (rank 0 only)\nif MPI.Comm_rank(MPI.COMM_WORLD) == 0\n    println(\"Active objects: \", length(manager.counter_pool))\n    println(\"Free IDs: \", length(manager.free_ids))\nend","category":"page"},{"location":"guide/distributed_refs/#Enable-Verbose-Assertions","page":"Distributed Reference Management","title":"Enable Verbose Assertions","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"# Assertions are enabled by default\nSafeMPI.enable_assert[]  # true\n\n# Use @mpiassert for collective checks\n@mpiassert all_data_valid \"Data validation failed\"","category":"page"},{"location":"guide/distributed_refs/#Performance-Considerations","page":"Distributed Reference Management","title":"Performance Considerations","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"Cleanup Cost: check_and_destroy! triggers a full GC and MPI synchronization\nThrottling: The max_check_count parameter reduces overhead by skipping some cleanup points\nID Recycling: Released IDs are reused to prevent integer overflow","category":"page"},{"location":"guide/distributed_refs/#See-Also","page":"Distributed Reference Management","title":"See Also","text":"","category":"section"},{"location":"guide/distributed_refs/","page":"Distributed Reference Management","title":"Distributed Reference Management","text":"DRef\nDistributedRefManager\ncheck_and_destroy!","category":"page"},{"location":"developer/#Developer-Guide","page":"Developer Guide","title":"Developer Guide","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"This guide is for developers who want to contribute to SafePETSc or extend it with custom distributed types.","category":"page"},{"location":"developer/#Architecture-Overview","page":"Developer Guide","title":"Architecture Overview","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"SafePETSc consists of two main layers:","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"SafeMPI: Low-level distributed reference management\nSafePETSc: High-level PETSc wrappers using SafeMPI","category":"page"},{"location":"developer/#SafeMPI-Layer","page":"Developer Guide","title":"SafeMPI Layer","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"The SafeMPI module implements reference counting across MPI ranks:","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"┌─────────────────────────────────────────────┐\n│  User Code                                  │\n│  creates DRef-wrapped objects               │\n└─────────────┬───────────────────────────────┘\n              │\n┌─────────────▼───────────────────────────────┐\n│  SafeMPI.DRef{T}                            │\n│  - Wraps object                             │\n│  - Finalizer calls _release!                │\n│  - Enqueues release message                 │\n└─────────────┬───────────────────────────────┘\n              │\n┌─────────────▼───────────────────────────────┐\n│  DistributedRefManager (Rank 0)             │\n│  - Receives release messages                │\n│  - Tracks reference counts                  │\n│  - Broadcasts destroy commands              │\n└─────────────┬───────────────────────────────┘\n              │\n┌─────────────▼───────────────────────────────┐\n│  destroy_obj!(obj)                          │\n│  - Called on all ranks simultaneously       │\n│  - User-defined cleanup routine             │\n└─────────────────────────────────────────────┘","category":"page"},{"location":"developer/#SafePETSc-Layer","page":"Developer Guide","title":"SafePETSc Layer","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Wraps PETSc objects with DRef:","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"struct _Vec{T}\n    v::PETSc.Vec{T}\n    row_partition::Vector{Int}\n    prefix::String\nend\n\nconst Vec{T} = DRef{_Vec{T}}\n\n# Opt-in to distributed management\nSafeMPI.destroy_trait(::Type{_Vec{T}}) = SafeMPI.CanDestroy()\n\n# Define cleanup\nfunction SafeMPI.destroy_obj!(x::_Vec{T})\n    _destroy_petsc_vec!(x.v)\nend","category":"page"},{"location":"developer/#Adding-New-Distributed-Types","page":"Developer Guide","title":"Adding New Distributed Types","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"To add your own distributed type:","category":"page"},{"location":"developer/#1.-Define-the-Internal-Type","page":"Developer Guide","title":"1. Define the Internal Type","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"struct _MyDistributedType\n    # Your fields here\n    handle::Ptr{Cvoid}  # e.g., MPI handle\n    data::Vector{Float64}\n    # ... other fields\nend","category":"page"},{"location":"developer/#2.-Create-Type-Alias","page":"Developer Guide","title":"2. Create Type Alias","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"const MyDistributedType = SafeMPI.DRef{_MyDistributedType}","category":"page"},{"location":"developer/#3.-Opt-In-to-Management","page":"Developer Guide","title":"3. Opt-In to Management","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"SafeMPI.destroy_trait(::Type{_MyDistributedType}) = SafeMPI.CanDestroy()","category":"page"},{"location":"developer/#4.-Implement-Cleanup","page":"Developer Guide","title":"4. Implement Cleanup","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"function SafeMPI.destroy_obj!(obj::_MyDistributedType)\n    # IMPORTANT: This is called on ALL ranks simultaneously\n    # Must be a collective operation\n\n    # Example: Free MPI resource\n    if obj.handle != C_NULL\n        MPI.Free(obj.handle)\n    end\n\n    # Clean up other resources\n    # ...\nend","category":"page"},{"location":"developer/#5.-Create-Constructor","page":"Developer Guide","title":"5. Create Constructor","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"function MyDistributedType(data::Vector{Float64})\n    # Allocate distributed resource\n    handle = allocate_mpi_resource(data)\n\n    # Wrap in internal type\n    obj = _MyDistributedType(handle, data)\n\n    # Wrap in DRef (triggers cleanup coordination)\n    return DRef(obj)\nend","category":"page"},{"location":"developer/#Testing","page":"Developer Guide","title":"Testing","text":"","category":"section"},{"location":"developer/#Unit-Tests-Structure","page":"Developer Guide","title":"Unit Tests Structure","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"SafePETSc uses a dual-file testing approach:","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"test/runtests.jl: Entry point that spawns MPI processes\ntest/test_*.jl: Individual test files run with MPI","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Example test file:","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"# test/test_myfeature.jl\nusing SafePETSc\nusing Test\nusing MPI\n\nSafePETSc.Init()\n\n@testset \"My Feature\" begin\n    rank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n    # Test uniform distribution\n    v = Vec_uniform([1.0, 2.0, 3.0])\n    @test size(v) == (3,)\n\n    # Test operations\n    y = v .+ 1.0\n    @test eltype(y) == Float64\n\n    # Cleanup\n    SafeMPI.check_and_destroy!()\nend","category":"page"},{"location":"developer/#Running-Tests","page":"Developer Guide","title":"Running Tests","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"# Run all tests\njulia --project=. -e 'using Pkg; Pkg.test()'\n\n# Run specific test\njulia --project=. -e 'using MPI; run(`$(MPI.mpiexec()) -n 4 $(Base.julia_cmd()) --project=. test/test_myfeature.jl`)'","category":"page"},{"location":"developer/#Coding-Guidelines","page":"Developer Guide","title":"Coding Guidelines","text":"","category":"section"},{"location":"developer/#Reference-Management","page":"Developer Guide","title":"Reference Management","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Always use DRef: Wrap distributed objects in DRef to ensure cleanup\nCleanup at creation: _make_ref automatically calls check_and_destroy!\nNo manual cleanup in operations: Avoid check_and_destroy! in regular functions\nCollective operations: destroy_obj! must be collective","category":"page"},{"location":"developer/#Error-Handling","page":"Developer Guide","title":"Error Handling","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Use @mpiassert: For collective error checking\nCoalesce assertions: Combine conditions into single @mpiassert\nInformative messages: Include context in error messages","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"# Good: single assertion with multiple conditions\n@mpiassert (size(A, 2) == size(B, 1) &&\n            A.obj.col_partition == B.obj.row_partition) \"Matrix dimensions and partitions must match for multiplication\"\n\n# Less good: multiple assertions\n@mpiassert size(A, 2) == size(B, 1) \"Dimension mismatch\"\n@mpiassert A.obj.col_partition == B.obj.row_partition \"Partition mismatch\"","category":"page"},{"location":"developer/#PETSc-Interop","page":"Developer Guide","title":"PETSc Interop","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Use PETSc.@for_libpetsc: For multi-precision support\nGPU-friendly operations: Prefer bulk operations over element access\nConst for MATINITIALMATRIX: Use module constant MAT_INITIAL_MATRIX = Cint(0)","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"PETSc.@for_libpetsc begin\n    function my_petsc_operation(mat::PETSc.Mat{$PetscScalar})\n        PETSc.@chk ccall((:PetscFunction, $libpetsc), ...)\n    end\nend","category":"page"},{"location":"developer/#Performance-Considerations","page":"Developer Guide","title":"Performance Considerations","text":"","category":"section"},{"location":"developer/#Cleanup-Overhead","page":"Developer Guide","title":"Cleanup Overhead","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"check_and_destroy! triggers full GC and MPI synchronization\nDefault: cleanup every 10 object creations (max_check_count=10 in _make_ref)\nTune based on application: more frequent cleanup = less memory, more overhead","category":"page"},{"location":"developer/#Memory-Management","page":"Developer Guide","title":"Memory Management","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Use DRef scoping to control lifetimes\nAvoid global DRef variables (prevent cleanup)\nConsider explicit cleanup in long loops","category":"page"},{"location":"developer/#GPU-Support","page":"Developer Guide","title":"GPU Support","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"SafePETSc prioritizes GPU-friendly PETSc operations\nSet PETSc options for GPU: -mat_type aijcusparse -vec_type cuda\nAvoid element-wise access (causes GPU↔CPU transfers)","category":"page"},{"location":"developer/#Documentation","page":"Developer Guide","title":"Documentation","text":"","category":"section"},{"location":"developer/#Docstrings","page":"Developer Guide","title":"Docstrings","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Follow Julia documentation conventions:","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"\"\"\"\n    my_function(x::Type; option=default) -> ReturnType\n\nBrief one-line description.\n\nExtended description with more details about the function's behavior,\nparameters, and return values.\n\n# Arguments\n- `x::Type`: Description of x\n- `option::Type=default`: Description of optional parameter\n\n# Returns\n- `ReturnType`: Description of return value\n\n# Examples","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"julia result = my_function(input)","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"\nSee also: [`related_function`](@ref), [`another_function`](@ref)\n\"\"\"\nfunction my_function(x; option=default)\n    # Implementation\nend","category":"page"},{"location":"developer/#Adding-Documentation-Pages","page":"Developer Guide","title":"Adding Documentation Pages","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Create markdown file in docs/src/\nAdd to pages in docs/make.jl\nBuild: julia --project=docs docs/make.jl","category":"page"},{"location":"developer/#Contributing","page":"Developer Guide","title":"Contributing","text":"","category":"section"},{"location":"developer/#Pull-Request-Process","page":"Developer Guide","title":"Pull Request Process","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Fork the repository\nCreate a feature branch\nAdd tests for new functionality\nUpdate documentation\nRun tests: julia --project=. -e 'using Pkg; Pkg.test()'\nSubmit pull request","category":"page"},{"location":"developer/#Code-Review-Checklist","page":"Developer Guide","title":"Code Review Checklist","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"[ ] Tests pass\n[ ] Documentation updated\n[ ] Docstrings added for public API\n[ ] Reference management correct\n[ ] Collective operations properly coordinated\n[ ] Performance considerations addressed","category":"page"},{"location":"developer/#Debugging-Tips","page":"Developer Guide","title":"Debugging Tips","text":"","category":"section"},{"location":"developer/#MPI-Hangs","page":"Developer Guide","title":"MPI Hangs","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"If program hangs, likely causes:","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Non-collective operation: One rank skipped a collective call\nUnbalanced branching: Ranks took different code paths\nMissing @mpiassert: Error on one rank, others waiting","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Debug with:","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"# Add at suspicious points\nif MPI.Comm_rank(MPI.COMM_WORLD) == 0\n    println(\"Reached checkpoint A\")\nend\nMPI.Barrier(MPI.COMM_WORLD)","category":"page"},{"location":"developer/#Memory-Leaks","page":"Developer Guide","title":"Memory Leaks","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Check for:","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Global DRef variables\nSkipped check_and_destroy! in long loops\nCircular references preventing GC","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Inspect manager state:","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"manager = SafeMPI.default_manager[]\nif MPI.Comm_rank(MPI.COMM_WORLD) == 0\n    println(\"Active objects: \", length(manager.counter_pool))\n    println(\"Pending releases: \", length(manager.pending_releases))\nend","category":"page"},{"location":"developer/#Assertion-Failures","page":"Developer Guide","title":"Assertion Failures","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"Enable verbose output:","category":"page"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"# Assertions enabled by default\nSafeMPI.enable_assert[]  # true\n\n# Check conditions\n@mpiassert condition \"Detailed error message\"","category":"page"},{"location":"developer/#Resources","page":"Developer Guide","title":"Resources","text":"","category":"section"},{"location":"developer/","page":"Developer Guide","title":"Developer Guide","text":"PETSc Documentation\nMPI.jl Documentation\nDocumenter.jl Guide","category":"page"},{"location":"guide/solvers/#Linear-Solvers","page":"Linear Solvers","title":"Linear Solvers","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"SafePETSc provides linear solver functionality through PETSc's KSP (Krylov Subspace) interface, wrapped with automatic memory management.","category":"page"},{"location":"guide/solvers/#Basic-Usage","page":"Linear Solvers","title":"Basic Usage","text":"","category":"section"},{"location":"guide/solvers/#Direct-Solve","page":"Linear Solvers","title":"Direct Solve","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"The simplest way to solve a linear system:","category":"page"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"# Solve Ax = b\nx = A \\ b\n\n# Solve A^T x = b\nx = A' \\ b","category":"page"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"This creates a solver internally, solves the system, and cleans up automatically.","category":"page"},{"location":"guide/solvers/#Multiple-Right-Hand-Sides","page":"Linear Solvers","title":"Multiple Right-Hand Sides","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"# Matrix RHS: solve AX = B\nX = A \\ B\n\n# Transpose: solve A^T X = B\nX = A' \\ B","category":"page"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Note: B and X must be dense matrices (MATMPIDENSE).","category":"page"},{"location":"guide/solvers/#Reusable-Solvers","page":"Linear Solvers","title":"Reusable Solvers","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"For multiple solves with the same matrix, create a Solver object:","category":"page"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"# Create solver once\nksp = Solver(A)\n\n# Solve multiple systems\nx1 = zeros_like(b1)\nldiv!(ksp, x1, b1)\n\nx2 = zeros_like(b2)\nldiv!(ksp, x2, b2)\n\n# Solver is cleaned up automatically when ksp goes out of scope","category":"page"},{"location":"guide/solvers/#Benefits-of-Reuse","page":"Linear Solvers","title":"Benefits of Reuse","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Performance: Avoids repeated factorization/preconditioner setup\nMemory: Single solver object instead of multiple temporary solvers\nConfiguration: Set PETSc options once","category":"page"},{"location":"guide/solvers/#In-Place-Operations","page":"Linear Solvers","title":"In-Place Operations","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"For pre-allocated result vectors/matrices:","category":"page"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"# Vector solve\nx = zeros_like(b)\nldiv!(x, A, b)  # x = A \\ b (creates solver internally)\n\n# With reusable solver\nldiv!(ksp, x, b)  # Reuse solver\n\n# Matrix solve\nX = zeros_like(B)\nldiv!(X, A, B)  # Solve AX = B\nldiv!(ksp, X, B)  # With reusable solver","category":"page"},{"location":"guide/solvers/#Right-Division","page":"Linear Solvers","title":"Right Division","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Solve systems where the unknown is on the left:","category":"page"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"# Solve x^T A = b^T (equivalent to A^T x = b)\nx_adj = b' / A  # Returns adjoint vector\n\n# Solve XA = B\nX = B / A\n\n# Transpose: solve XA^T = B\nX = B / A'","category":"page"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Note: B and X must be dense matrices.","category":"page"},{"location":"guide/solvers/#Configuring-Solvers","page":"Linear Solvers","title":"Configuring Solvers","text":"","category":"section"},{"location":"guide/solvers/#PETSc-Options","page":"Linear Solvers","title":"PETSc Options","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Control solver behavior via PETSc options:","category":"page"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"# Global configuration\npetsc_options_insert_string(\"-ksp_type gmres\")\npetsc_options_insert_string(\"-ksp_rtol 1e-8\")\npetsc_options_insert_string(\"-pc_type bjacobi\")\n\n# With prefix for specific solvers\npetsc_options_insert_string(\"-my_ksp_type cg\")\nA = Mat_uniform(data; prefix=\"my_\")\nksp = Solver(A)  # Will use CG","category":"page"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Common KSP options:","category":"page"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"-ksp_type: Solver type (cg, gmres, bcgs, etc.)\n-ksp_rtol: Relative tolerance\n-ksp_atol: Absolute tolerance\n-ksp_max_it: Maximum iterations\n-pc_type: Preconditioner (jacobi, bjacobi, ilu, etc.)","category":"page"},{"location":"guide/solvers/#Monitoring-Convergence","page":"Linear Solvers","title":"Monitoring Convergence","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"petsc_options_insert_string(\"-ksp_monitor\")\npetsc_options_insert_string(\"-ksp_converged_reason\")\n\nx = A \\ b\n# PETSc will print convergence information","category":"page"},{"location":"guide/solvers/#Solver-Types","page":"Linear Solvers","title":"Solver Types","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"SafePETSc supports all PETSc KSP types. Common choices:","category":"page"},{"location":"guide/solvers/#Direct-Methods","page":"Linear Solvers","title":"Direct Methods","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"# For small to medium problems\npetsc_options_insert_string(\"-ksp_type preonly -pc_type lu\")\nx = A \\ b","category":"page"},{"location":"guide/solvers/#Iterative-Methods","page":"Linear Solvers","title":"Iterative Methods","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"# Conjugate Gradient (symmetric positive definite)\npetsc_options_insert_string(\"-ksp_type cg -pc_type jacobi\")\n\n# GMRES (general nonsymmetric)\npetsc_options_insert_string(\"-ksp_type gmres -ksp_gmres_restart 30\")\n\n# BiCGStab\npetsc_options_insert_string(\"-ksp_type bcgs\")","category":"page"},{"location":"guide/solvers/#Examples","page":"Linear Solvers","title":"Examples","text":"","category":"section"},{"location":"guide/solvers/#Basic-Linear-System","page":"Linear Solvers","title":"Basic Linear System","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"using SafePETSc\nusing MPI\n\nSafePETSc.Init()\n\n# Create a simple system\nn = 100\nA_dense = zeros(n, n)\nfor i in 1:n\n    A_dense[i, i] = 2.0\n    if i > 1\n        A_dense[i, i-1] = -1.0\n    end\n    if i < n\n        A_dense[i, i+1] = -1.0\n    end\nend\n\nA = Mat_uniform(A_dense)\nb = Vec_uniform(ones(n))\n\n# Solve\nx = A \\ b\n\n# Check residual\nr = b - A * x\n# (In practice, use PETSc's built-in convergence monitoring)","category":"page"},{"location":"guide/solvers/#Iterative-Solver-with-Monitoring","page":"Linear Solvers","title":"Iterative Solver with Monitoring","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"using SafePETSc\n\nSafePETSc.Init()\n\n# Configure solver\npetsc_options_insert_string(\"-ksp_type cg\")\npetsc_options_insert_string(\"-ksp_rtol 1e-10\")\npetsc_options_insert_string(\"-ksp_monitor\")\npetsc_options_insert_string(\"-pc_type jacobi\")\n\n# Build system (e.g., Laplacian)\nn = 1000\ndiag = Vec_uniform(2.0 * ones(n))\noff = Vec_uniform(-1.0 * ones(n-1))\nA = spdiagm(-1 => off, 0 => diag, 1 => off)\n\nb = Vec_uniform(ones(n))\n\n# Solve (will print iteration info)\nx = A \\ b","category":"page"},{"location":"guide/solvers/#Multiple-Solves","page":"Linear Solvers","title":"Multiple Solves","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"using SafePETSc\n\nSafePETSc.Init()\n\n# System matrix\nA = Mat_uniform(...)\n\n# Create solver once\nksp = Solver(A)\n\n# Solve many RHS vectors\nfor i in 1:100\n    b = Vec_uniform(rhs_data[i])\n    x = zeros_like(b)\n    ldiv!(ksp, x, b)\n    results[i] = extract_result(x)\nend\n\n# Solver automatically cleaned up","category":"page"},{"location":"guide/solvers/#Block-Solves","page":"Linear Solvers","title":"Block Solves","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"using SafePETSc\n\nSafePETSc.Init()\n\n# System matrix\nA = Mat_uniform(...)\n\n# Multiple right-hand sides as columns of a dense matrix\nB = Mat_uniform(rhs_matrix)  # Must be dense\n\n# Solve all systems at once\nX = A \\ B\n\n# Each column of X is a solution","category":"page"},{"location":"guide/solvers/#Performance-Tips","page":"Linear Solvers","title":"Performance Tips","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Reuse Solvers: Create Solver once for multiple solves\nChoose Appropriate Method: Direct for small problems, iterative for large\nTune Preconditioner: Can dramatically affect convergence\nMonitor Convergence: Use -ksp_monitor to tune parameters\nGPU Acceleration: Set PETSc options for GPU execution","category":"page"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"# GPU configuration example\npetsc_options_insert_string(\"-mat_type aijcusparse\")\npetsc_options_insert_string(\"-vec_type cuda\")","category":"page"},{"location":"guide/solvers/#Solver-Properties","page":"Linear Solvers","title":"Solver Properties","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Check solver dimensions:","category":"page"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"ksp = Solver(A)\n\nm, n = size(ksp)  # Matrix dimensions\nm = size(ksp, 1)  # Rows\nn = size(ksp, 2)  # Columns","category":"page"},{"location":"guide/solvers/#Troubleshooting","page":"Linear Solvers","title":"Troubleshooting","text":"","category":"section"},{"location":"guide/solvers/#Convergence-Issues","page":"Linear Solvers","title":"Convergence Issues","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"# Increase iterations\npetsc_options_insert_string(\"-ksp_max_it 10000\")\n\n# Relax tolerance\npetsc_options_insert_string(\"-ksp_rtol 1e-6\")\n\n# Try different solver/preconditioner\npetsc_options_insert_string(\"-ksp_type gmres -pc_type asm\")\n\n# View solver details\npetsc_options_insert_string(\"-ksp_view\")","category":"page"},{"location":"guide/solvers/#Memory-Issues","page":"Linear Solvers","title":"Memory Issues","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"# Use iterative method instead of direct\npetsc_options_insert_string(\"-ksp_type cg\")\n\n# Reduce GMRES restart\npetsc_options_insert_string(\"-ksp_gmres_restart 10\")","category":"page"},{"location":"guide/solvers/#Assertion-Failures","page":"Linear Solvers","title":"Assertion Failures","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Ensure:","category":"page"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Matrix is square for \\ operator\nPartitions match (A.rowpartition == b.rowpartition)\nSame prefix on all objects","category":"page"},{"location":"guide/solvers/#See-Also","page":"Linear Solvers","title":"See Also","text":"","category":"section"},{"location":"guide/solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Solver\nBase.:\\\nLinearAlgebra.ldiv!\nBase.:/","category":"page"},{"location":"api/solvers/#Solvers-API-Reference","page":"Solvers","title":"Solvers API Reference","text":"","category":"section"},{"location":"api/solvers/","page":"Solvers","title":"Solvers","text":"Linear solver functionality in SafePETSc.","category":"page"},{"location":"api/solvers/#Type","page":"Solvers","title":"Type","text":"","category":"section"},{"location":"api/solvers/#SafePETSc.Solver","page":"Solvers","title":"SafePETSc.Solver","text":"Solver{T}\n\nA PETSc KSP (Krylov Subspace) linear solver with element type T, managed by SafePETSc's reference counting system.\n\nSolver{T} is actually a type alias for DRef{_KSP{T}}, meaning solvers are automatically tracked across MPI ranks and destroyed collectively when all ranks release their references.\n\nSolvers can be reused for multiple linear systems with the same matrix, avoiding the cost of repeated factorization or preconditioner setup.\n\nConstruction\n\nSee the Solver constructor for creating solver instances.\n\nUsage\n\nSolvers can be used implicitly via the backslash operator, or explicitly for reuse:\n\n# Implicit (creates and destroys solver internally)\nx = A \\ b\n\n# Explicit (reuse solver for multiple solves)\nksp = Solver(A)\nx1 = similar(b)\nx2 = similar(b)\nLinearAlgebra.ldiv!(ksp, x1, b1)  # First solve\nLinearAlgebra.ldiv!(ksp, x2, b2)  # Second solve with same matrix\n\nSee also: Mat, Vec, the Solver constructor\n\n\n\n\n\n","category":"type"},{"location":"api/solvers/#Initialization","page":"Solvers","title":"Initialization","text":"","category":"section"},{"location":"api/solvers/#SafePETSc.Init","page":"Solvers","title":"SafePETSc.Init","text":"Init() -> nothing\n\nEnsure MPI and PETSc are initialized in the recommended order (MPI first, then PETSc). Safe to call multiple times. Does not register custom finalizers; rely on library defaults for shutdown (MPI.jl finalizes at exit; PETSc may remain initialized).\n\n\n\n\n\n","category":"function"},{"location":"api/solvers/#SafePETSc.Initialized","page":"Solvers","title":"SafePETSc.Initialized","text":"Initialized() -> Bool\n\nReturn true if both MPI and PETSc are initialized. This is a simple conjunction of MPI.Initialized() and PETSc.initialized for the active PETSc library.\n\n\n\n\n\n","category":"function"},{"location":"api/solvers/#SafePETSc.petsc_options_insert_string","page":"Solvers","title":"SafePETSc.petsc_options_insert_string","text":"petsc_options_insert_string(options_string::String)\n\nInsert command-line style options into PETSc's global options database.\n\nExample: petsc_options_insert_string(\"-dense_mat_type mpidense\")\n\nThis sets options that will be used by PETSc objects created with the corresponding prefix. PETSc must already be initialized by the caller.\n\n\n\n\n\n","category":"function"},{"location":"api/solvers/#Linear-Solves","page":"Solvers","title":"Linear Solves","text":"","category":"section"},{"location":"api/solvers/#Direct-Solve","page":"Solvers","title":"Direct Solve","text":"","category":"section"},{"location":"api/solvers/","page":"Solvers","title":"Solvers","text":"# Vector RHS\nx = A \\ b                              # Solve Ax = b\nx = A' \\ b                             # Solve A^T x = b\n\n# Matrix RHS (must be dense)\nX = A \\ B                              # Solve AX = B\nX = A' \\ B                             # Solve A^T X = B","category":"page"},{"location":"api/solvers/#In-Place-Solve","page":"Solvers","title":"In-Place Solve","text":"","category":"section"},{"location":"api/solvers/","page":"Solvers","title":"Solvers","text":"# Vector RHS\nLinearAlgebra.ldiv!(x, A, b)           # x = A \\ b\nLinearAlgebra.ldiv!(ksp, x, b)         # Using reusable solver\n\n# Matrix RHS (must be dense)\nLinearAlgebra.ldiv!(X, A, B)           # X = A \\ B\nLinearAlgebra.ldiv!(ksp, X, B)         # Using reusable solver","category":"page"},{"location":"api/solvers/#Right-Division","page":"Solvers","title":"Right Division","text":"","category":"section"},{"location":"api/solvers/","page":"Solvers","title":"Solvers","text":"# Vector\nx_adj = b' / A                         # Solve x^T A = b^T\n\n# Matrix (must be dense)\nX = B / A                              # Solve XA = B\nX = B / A'                             # Solve XA^T = B","category":"page"},{"location":"api/solvers/#Properties","page":"Solvers","title":"Properties","text":"","category":"section"},{"location":"api/solvers/","page":"Solvers","title":"Solvers","text":"m, n = size(ksp)                       # Solver matrix dimensions","category":"page"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This guide will help you get started with SafePETSc.jl for distributed parallel computing with MPI and PETSc.","category":"page"},{"location":"getting_started/#Prerequisites","page":"Getting Started","title":"Prerequisites","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"SafePETSc requires:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Julia 1.6 or later\nMPI installation (OpenMPI, MPICH, etc.)\nPETSc installation","category":"page"},{"location":"getting_started/#Running-with-MPI","page":"Getting Started","title":"Running with MPI","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"SafePETSc programs must be run with MPI. Use the MPI.jl wrapper to ensure compatibility:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Run with 4 MPI processes\njulia -e 'using MPI; run(`$(MPI.mpiexec()) -n 4 $(Base.julia_cmd()) --project=. your_script.jl`)'","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This ensures the correct MPI implementation and Julia executable are used.","category":"page"},{"location":"getting_started/#Using-System-MPI-on-HPC-Clusters","page":"Getting Started","title":"Using System MPI on HPC Clusters","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"On HPC clusters, you typically need to use the cluster's native MPI library (not the one Julia ships with) for optimal performance and compatibility with the job scheduler. Here's how to configure this:","category":"page"},{"location":"getting_started/#Step-1:-Load-the-MPI-Module-(Shell-Command)","page":"Getting Started","title":"Step 1: Load the MPI Module (Shell Command)","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"First, load your cluster's MPI module. This is a shell command (run in your terminal):","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Example for clusters using the module system\nmodule load openmpi  # or module load mpich, module load intel-mpi, etc.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Check which MPI was loaded:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Shell command\nwhich mpiexec","category":"page"},{"location":"getting_started/#Step-2:-Configure-Julia-to-Use-System-MPI","page":"Getting Started","title":"Step 2: Configure Julia to Use System MPI","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"You need to tell Julia's MPI.jl package to use the system MPI library instead of its bundled version. This is done using MPIPreferences.jl.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Run this Julia code (in a Julia REPL or as a script):","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Julia code - run this once per project\nusing MPIPreferences\nMPIPreferences.use_system_binary()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Alternatively, if you want to specify the MPI library explicitly:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Julia code - specify the exact MPI library path\nusing MPIPreferences\nMPIPreferences.use_system_binary(\n    mpiexec = \"/path/to/your/mpiexec\",  # Get this from 'which mpiexec'\n    vendor = \"OpenMPI\"  # or \"MPICH\", \"IntelMPI\", etc.\n)","category":"page"},{"location":"getting_started/#Step-3:-Rebuild-MPI.jl","page":"Getting Started","title":"Step 3: Rebuild MPI.jl","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"After configuring MPIPreferences, you must rebuild MPI.jl. Run this Julia code:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Julia code - rebuild MPI.jl to use the system library\nusing Pkg\nPkg.build(\"MPI\"; verbose=true)","category":"page"},{"location":"getting_started/#Step-4:-Verify-the-Configuration","page":"Getting Started","title":"Step 4: Verify the Configuration","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Check that MPI.jl is now using the system MPI. Run this Julia code:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Julia code - verify MPI configuration\nusing MPI\nMPI.versioninfo()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"You should see your cluster's MPI library listed (e.g., OpenMPI 4.1.x, not MPItrampoline).","category":"page"},{"location":"getting_started/#Step-5:-Run-Your-Code","page":"Getting Started","title":"Step 5: Run Your Code","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Now you can run your SafePETSc code. On clusters, you typically use the cluster's job scheduler:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Shell command - example SLURM job submission\nsbatch my_job.sh","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Example my_job.sh script:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=16\n#SBATCH --time=1:00:00\n\n# Load MPI module\nmodule load openmpi\n\n# Run Julia with MPI\njulia -e 'using MPI; run(`$(MPI.mpiexec()) -n 32 $(Base.julia_cmd()) --project=. my_script.jl`)'","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Or for PBS/Torque:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"#!/bin/bash\n#PBS -l nodes=2:ppn=16\n#PBS -l walltime=1:00:00\n\ncd $PBS_O_WORKDIR\nmodule load openmpi\n\njulia -e 'using MPI; run(`$(MPI.mpiexec()) -n 32 $(Base.julia_cmd()) --project=. my_script.jl`)'","category":"page"},{"location":"getting_started/#Important-Notes","page":"Getting Started","title":"Important Notes","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"One-time setup: Steps 1-3 only need to be done once per project/environment\nModule loading: You must load the MPI module in your job scripts (Step 1) every time you submit a job\nConsistency: Use the same MPI library that PETSc was built against on your cluster\nProject-specific: The MPI configuration is stored in your project's LocalPreferences.toml file","category":"page"},{"location":"getting_started/#Basic-Workflow","page":"Getting Started","title":"Basic Workflow","text":"","category":"section"},{"location":"getting_started/#1.-Initialize","page":"Getting Started","title":"1. Initialize","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Always start by initializing MPI and PETSc:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using SafePETSc\nusing MPI\n\nSafePETSc.Init()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This ensures both MPI and PETSc are properly initialized.","category":"page"},{"location":"getting_started/#2.-Create-Distributed-Objects","page":"Getting Started","title":"2. Create Distributed Objects","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"SafePETSc provides two main patterns for creating distributed objects:","category":"page"},{"location":"getting_started/#Uniform-Distribution","page":"Getting Started","title":"Uniform Distribution","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Use when all ranks have the same data:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Same matrix on all ranks\nA = Mat_uniform([1.0 2.0; 3.0 4.0])\n\n# Same vector on all ranks\nv = Vec_uniform([1.0, 2.0])","category":"page"},{"location":"getting_started/#Sum-Distribution","page":"Getting Started","title":"Sum Distribution","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Use when ranks contribute different sparse data:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using SparseArrays\n\n# Each rank contributes sparse entries\n# Entries are summed across ranks\nA = Mat_sum(sparse([1], [1], [rank_value], 10, 10))\nv = Vec_sum(sparsevec([rank_id], [rank_value], 10))","category":"page"},{"location":"getting_started/#3.-Perform-Operations","page":"Getting Started","title":"3. Perform Operations","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Matrix-vector multiplication\ny = A * v\n\n# In-place operations\ny .= A * v .+ 1.0\n\n# Linear solve\nx = A \\ b\n\n# Matrix operations\nC = A * B\nD = A'  # Transpose","category":"page"},{"location":"getting_started/#4.-Cleanup","page":"Getting Started","title":"4. Cleanup","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Objects are automatically cleaned up when they go out of scope. You can explicitly trigger cleanup:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"SafeMPI.check_and_destroy!()","category":"page"},{"location":"getting_started/#Complete-Example","page":"Getting Started","title":"Complete Example","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Here's a complete example that solves a linear system:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using SafePETSc\nusing MPI\n\n# Initialize\nSafePETSc.Init()\n\n# Get MPI info\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nnranks = MPI.Comm_size(comm)\n\n# Create a simple 2D Laplacian matrix (uniform on all ranks)\nn = 100\nA_dense = zeros(n, n)\nfor i in 1:n\n    A_dense[i, i] = 2.0\n    if i > 1\n        A_dense[i, i-1] = -1.0\n    end\n    if i < n\n        A_dense[i, i+1] = -1.0\n    end\nend\n\n# Create distributed PETSc matrix\nA = Mat_uniform(A_dense)\n\n# Create right-hand side\nb = Vec_uniform(ones(n))\n\n# Solve the system\nx = A \\ b\n\n# Extract local portion for output (if needed)\nif rank == 0\n    println(\"System solved successfully\")\nend\n\n# Explicit cleanup (optional - happens automatically at scope exit)\nSafeMPI.check_and_destroy!()","category":"page"},{"location":"getting_started/#Running-the-Example","page":"Getting Started","title":"Running the Example","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Save the above code as example.jl and run:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia -e 'using MPI; run(`$(MPI.mpiexec()) -n 4 $(Base.julia_cmd()) --project=. example.jl`)'","category":"page"},{"location":"getting_started/#Next-Steps","page":"Getting Started","title":"Next Steps","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Learn about Distributed Reference Management\nExplore Vectors\nUnderstand Matrices\nUse Linear Solvers","category":"page"},{"location":"api/vectors/#Vectors-API-Reference","page":"Vectors","title":"Vectors API Reference","text":"","category":"section"},{"location":"api/vectors/","page":"Vectors","title":"Vectors","text":"Distributed vector operations in SafePETSc.","category":"page"},{"location":"api/vectors/#Type","page":"Vectors","title":"Type","text":"","category":"section"},{"location":"api/vectors/#SafePETSc.Vec","page":"Vectors","title":"SafePETSc.Vec","text":"Vec{T}\n\nA distributed PETSc vector with element type T, managed by SafePETSc's reference counting system.\n\nVec{T} is actually a type alias for DRef{_Vec{T}}, meaning vectors are automatically tracked across MPI ranks and destroyed collectively when all ranks release their references.\n\nConstruction\n\nUse Vec_uniform or Vec_sum to create distributed vectors:\n\n# Create from uniform data (same on all ranks)\nv = Vec_uniform([1.0, 2.0, 3.0, 4.0])\n\n# Create from sparse contributions (summed across ranks)\nusing SparseArrays\nv = Vec_sum(sparsevec([1, 3], [1.0, 3.0], 4))\n\nOperations\n\nVectors support standard arithmetic operations via broadcasting:\n\ny = x .+ 1.0        # Element-wise addition\ny .= 2.0 .* x       # In-place scaling\nz = x .+ y          # Vector addition\n\nMatrix-vector multiplication:\n\ny = A * x           # Matrix-vector product\nLinearAlgebra.mul!(y, A, x)  # In-place version\n\nSee also: Vec_uniform, Vec_sum, Mat, zeros_like\n\n\n\n\n\n","category":"type"},{"location":"api/vectors/#Constructors","page":"Vectors","title":"Constructors","text":"","category":"section"},{"location":"api/vectors/#SafePETSc.Vec_uniform","page":"Vectors","title":"SafePETSc.Vec_uniform","text":"Vec_uniform(v::Vector{T}; row_partition=default_row_partition(length(v), MPI.Comm_size(MPI.COMM_WORLD)), prefix=\"\") -> DRef{Vec{T}}\n\nCreate a distributed PETSc vector from a Julia vector, asserting uniform distribution across ranks (on MPI.COMM_WORLD).\n\nv::Vector{T} must be identical on all ranks (mpi_uniform).\nrow_partition is a Vector{Int} of length nranks+1 where partition[i] is the start row (1-indexed) for rank i-1.\nprefix is an optional string prefix for VecSetOptionsPrefix() to set vector-specific command-line options.\nReturns a DRef that will destroy the PETSc Vec collectively when all ranks release their reference.\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#SafePETSc.Vec_sum","page":"Vectors","title":"SafePETSc.Vec_sum","text":"Vec_sum(v::SparseVector{T}; row_partition=default_row_partition(length(v), MPI.Comm_size(MPI.COMM_WORLD)), prefix=\"\", own_rank_only=false) -> DRef{Vec{T}}\n\nCreate a distributed PETSc vector by summing sparse vectors across ranks (on MPI.COMM_WORLD).\n\nv::SparseVector{T} can differ across ranks; nonzero entries are summed across all ranks.\nrow_partition is a Vector{Int} of length nranks+1 where partition[i] is the start row (1-indexed) for rank i-1.\nprefix is an optional string prefix for VecSetOptionsPrefix() to set vector-specific command-line options.\nown_rank_only::Bool (default=false): if true, asserts that all nonzero indices fall within this rank's row partition.\nReturns a DRef that will destroy the PETSc Vec collectively when all ranks release their reference.\n\nUses VecSetValues with ADD_VALUES mode to sum contributions from all ranks.\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#Helper-Constructors","page":"Vectors","title":"Helper Constructors","text":"","category":"section"},{"location":"api/vectors/#SafePETSc.zeros_like","page":"Vectors","title":"SafePETSc.zeros_like","text":"zeros_like(x::Vec{T}; T2::Type{S}=T, prefix::String=x.obj.prefix) -> Vec{S}\n\nCreate a new distributed vector with the same size and partition as x, filled with zeros.\n\nArguments\n\nx: Template vector to match size and partition\nT2: Element type of the result (defaults to same type as x)\nprefix: PETSc options prefix (defaults to same prefix as x)\n\nSee also: ones_like, fill_like, Vec_uniform\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#SafePETSc.ones_like","page":"Vectors","title":"SafePETSc.ones_like","text":"ones_like(x::Vec{T}; T2::Type{S}=T, prefix::String=x.obj.prefix) -> Vec{S}\n\nCreate a new distributed vector with the same size and partition as x, filled with ones.\n\nArguments\n\nx: Template vector to match size and partition\nT2: Element type of the result (defaults to same type as x)\nprefix: PETSc options prefix (defaults to same prefix as x)\n\nSee also: zeros_like, fill_like, Vec_uniform\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#SafePETSc.fill_like","page":"Vectors","title":"SafePETSc.fill_like","text":"fill_like(x::Vec{T}, val; T2::Type{S}=typeof(val), prefix::String=x.obj.prefix) -> Vec{S}\n\nCreate a new distributed vector with the same size and partition as x, filled with val.\n\nArguments\n\nx: Template vector to match size and partition\nval: Value to fill the vector with\nT2: Element type of the result (defaults to type of val)\nprefix: PETSc options prefix (defaults to same prefix as x)\n\nExample\n\ny = fill_like(x, 3.14)  # Create a vector like x, filled with 3.14\n\nSee also: zeros_like, ones_like, Vec_uniform\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#Partitioning","page":"Vectors","title":"Partitioning","text":"","category":"section"},{"location":"api/vectors/#SafePETSc.default_row_partition","page":"Vectors","title":"SafePETSc.default_row_partition","text":"default_row_partition(n::Int, nranks::Int) -> Vector{Int}\n\nCreate a default row partition that divides n rows equally among nranks.\n\nReturns a Vector{Int} of length nranks+1 where partition[i] is the start row (1-indexed) for rank i-1.\n\n\n\n\n\n","category":"function"},{"location":"api/vectors/#Operations","page":"Vectors","title":"Operations","text":"","category":"section"},{"location":"api/vectors/#Arithmetic","page":"Vectors","title":"Arithmetic","text":"","category":"section"},{"location":"api/vectors/","page":"Vectors","title":"Vectors","text":"Vectors support standard Julia arithmetic operations via broadcasting:","category":"page"},{"location":"api/vectors/","page":"Vectors","title":"Vectors","text":"y = x .+ 1.0        # Element-wise addition\ny = 2.0 .* x        # Scaling\nz = x .+ y          # Vector addition\ny .= x .+ 1.0       # In-place operation","category":"page"},{"location":"api/vectors/","page":"Vectors","title":"Vectors","text":"Standard operators are also overloaded:","category":"page"},{"location":"api/vectors/","page":"Vectors","title":"Vectors","text":"z = x + y           # Addition\nz = x - y           # Subtraction\nz = -x              # Negation","category":"page"},{"location":"api/vectors/#Linear-Algebra","page":"Vectors","title":"Linear Algebra","text":"","category":"section"},{"location":"api/vectors/","page":"Vectors","title":"Vectors","text":"y = A * x                              # Matrix-vector multiplication\nLinearAlgebra.mul!(y, A, x)            # In-place multiplication\nw = v' * A                             # Adjoint-vector times matrix\nLinearAlgebra.mul!(w, v', A)           # In-place","category":"page"},{"location":"api/vectors/#Properties","page":"Vectors","title":"Properties","text":"","category":"section"},{"location":"api/vectors/","page":"Vectors","title":"Vectors","text":"T = eltype(v)                          # Element type\nn = length(v)                          # Vector length\nn = size(v, 1)                         # Size in dimension 1","category":"page"},{"location":"api/matrices/#Matrices-API-Reference","page":"Matrices","title":"Matrices API Reference","text":"","category":"section"},{"location":"api/matrices/","page":"Matrices","title":"Matrices","text":"Distributed matrix operations in SafePETSc.","category":"page"},{"location":"api/matrices/#Type","page":"Matrices","title":"Type","text":"","category":"section"},{"location":"api/matrices/#SafePETSc.Mat","page":"Matrices","title":"SafePETSc.Mat","text":"Mat{T}\n\nA distributed PETSc matrix with element type T, managed by SafePETSc's reference counting system.\n\nMat{T} is actually a type alias for DRef{_Mat{T}}, meaning matrices are automatically tracked across MPI ranks and destroyed collectively when all ranks release their references.\n\nConstruction\n\nUse Mat_uniform or Mat_sum to create distributed matrices:\n\n# Create from uniform data (same on all ranks)\nA = Mat_uniform([1.0 2.0; 3.0 4.0])\n\n# Create from sparse contributions (summed across ranks)\nusing SparseArrays\nA = Mat_sum(sparse([1, 2], [1, 2], [1.0, 4.0], 2, 2))\n\nOperations\n\nMatrices support standard linear algebra operations:\n\n# Matrix-vector multiplication\ny = A * x\n\n# Matrix-matrix multiplication\nC = A * B\n\n# Matrix transpose\nB = A'\nB = Mat(A')  # Materialize transpose\n\n# Linear solve\nx = A \\ b\n\n# Concatenation\nC = vcat(A, B)  # or cat(A, B; dims=1)\nD = hcat(A, B)  # or cat(A, B; dims=2)\nE = blockdiag(A, B)\n\n# Diagonal matrix from vectors\nusing SparseArrays\nA = spdiagm(0 => diag_vec, 1 => upper_diag)\n\nSee also: Mat_uniform, Mat_sum, Vec, Solver\n\n\n\n\n\n","category":"type"},{"location":"api/matrices/#Constructors","page":"Matrices","title":"Constructors","text":"","category":"section"},{"location":"api/matrices/#SafePETSc.Mat_uniform","page":"Matrices","title":"SafePETSc.Mat_uniform","text":"Mat_uniform(A::Matrix{T}; row_partition=default_row_partition(size(A, 1), MPI.Comm_size(MPI.COMM_WORLD)), col_partition=default_row_partition(size(A, 2), MPI.Comm_size(MPI.COMM_WORLD)), prefix=\"\") -> DRef{Mat{T}}\n\nCreate a distributed PETSc matrix from a Julia matrix, asserting uniform distribution across ranks (on MPI.COMM_WORLD).\n\nA::Matrix{T} must be identical on all ranks (mpi_uniform).\nrow_partition is a Vector{Int} of length nranks+1 where partition[i] is the start row (1-indexed) for rank i-1.\ncol_partition is a Vector{Int} of length nranks+1 where partition[i] is the start column (1-indexed) for rank i-1.\nprefix is an optional string prefix for MatSetOptionsPrefix() to set matrix-specific command-line options.\nReturns a DRef that will destroy the PETSc Mat collectively when all ranks release their reference.\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#SafePETSc.Mat_sum","page":"Matrices","title":"SafePETSc.Mat_sum","text":"Mat_sum(A::SparseMatrixCSC{T}; row_partition=default_row_partition(size(A, 1), MPI.Comm_size(MPI.COMM_WORLD)), col_partition=default_row_partition(size(A, 2), MPI.Comm_size(MPI.COMM_WORLD)), prefix=\"\", own_rank_only=false) -> DRef{Mat{T}}\n\nCreate a distributed PETSc matrix by summing sparse matrices across ranks (on MPI.COMM_WORLD).\n\nA::SparseMatrixCSC{T} can differ across ranks; nonzero entries are summed across all ranks.\nrow_partition is a Vector{Int} of length nranks+1 where partition[i] is the start row (1-indexed) for rank i-1.\ncol_partition is a Vector{Int} of length nranks+1 where partition[i] is the start column (1-indexed) for rank i-1.\nprefix is an optional string prefix for MatSetOptionsPrefix() to set matrix-specific command-line options.\nown_rank_only::Bool (default=false): if true, asserts that all nonzero entries fall within this rank's row partition.\nReturns a DRef that will destroy the PETSc Mat collectively when all ranks release their reference.\n\nUses MatSetValues with ADD_VALUES mode to sum contributions from all ranks.\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#Concatenation","page":"Matrices","title":"Concatenation","text":"","category":"section"},{"location":"api/matrices/#Base.cat","page":"Matrices","title":"Base.cat","text":"Base.cat(As::Mat{T}...; dims) -> Mat{T}\n\nConcatenate distributed PETSc matrices along dimension dims.\n\nAll input matrices must:\n\nHave the same element type T\nHave the same prefix\nHave compatible sizes and partitions for the concatenation dimension\n\nThe concatenation is performed by:\n\nEach rank extracts its owned rows from each input matrix as a Julia sparse matrix\nStandard Julia cat is applied locally on each rank\nThe results are summed across ranks using Mat_sum\n\nExamples\n\nC = cat(A, B; dims=1)  # Vertical concatenation (vcat)\nD = cat(A, B; dims=2)  # Horizontal concatenation (hcat)\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#Base.vcat","page":"Matrices","title":"Base.vcat","text":"Base.vcat(As::Mat{T}...) -> Mat{T}\n\nVertically concatenate distributed PETSc matrices.\n\nEquivalent to cat(As...; dims=1). All matrices must have the same number of columns and the same column partition.\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#Base.hcat","page":"Matrices","title":"Base.hcat","text":"Base.hcat(As::Mat{T}...) -> Mat{T}\n\nHorizontally concatenate distributed PETSc matrices.\n\nEquivalent to cat(As...; dims=2). All matrices must have the same number of rows and the same row partition.\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#SparseArrays.blockdiag","page":"Matrices","title":"SparseArrays.blockdiag","text":"blockdiag(As::Mat{T}...) -> Mat{T}\n\nCreate a block diagonal matrix from distributed PETSc matrices.\n\nThe result is a matrix with the input matrices along the diagonal and zeros elsewhere. All matrices must have the same prefix and element type.\n\nExample\n\n# If A is m×n and B is p×q, then blockdiag(A, B) is (m+p)×(n+q)\nC = blockdiag(A, B)\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#Sparse-Diagonal-Matrices","page":"Matrices","title":"Sparse Diagonal Matrices","text":"","category":"section"},{"location":"api/matrices/#SparseArrays.spdiagm","page":"Matrices","title":"SparseArrays.spdiagm","text":"spdiagm(kv::Pair{<:Integer, <:Vec{T}}...) -> Mat{T}\nspdiagm(m::Integer, n::Integer, kv::Pair{<:Integer, <:Vec{T}}...) -> Mat{T}\n\nCreate a sparse diagonal matrix from distributed PETSc vectors.\n\nEach pair k => v places the vector v on the k-th diagonal:\n\nk = 0: main diagonal\nk > 0: superdiagonal\nk < 0: subdiagonal\n\nAll vectors must have the same element type T and prefix. The matrix dimensions are inferred from the diagonal positions and vector lengths, or can be specified explicitly.\n\nExamples\n\n# Create a tridiagonal matrix\nA = spdiagm(-1 => lower, 0 => diag, 1 => upper)\n\n# Create a 100×100 matrix with specified vectors on diagonals\nB = spdiagm(100, 100, 0 => v1, 1 => v2)\n\n\n\n\n\n","category":"function"},{"location":"api/matrices/#Operations","page":"Matrices","title":"Operations","text":"","category":"section"},{"location":"api/matrices/#Linear-Algebra","page":"Matrices","title":"Linear Algebra","text":"","category":"section"},{"location":"api/matrices/","page":"Matrices","title":"Matrices","text":"# Matrix-vector multiplication\ny = A * x\nLinearAlgebra.mul!(y, A, x)\n\n# Matrix-matrix multiplication\nC = A * B\nLinearAlgebra.mul!(C, A, B)\n\n# Transpose\nB = A'\nB = Mat(A')                            # Materialize transpose\nLinearAlgebra.transpose!(B, A)         # In-place transpose\n\n# Adjoint-vector multiplication\nw = v' * A\nLinearAlgebra.mul!(w, v', A)","category":"page"},{"location":"api/matrices/#Properties","page":"Matrices","title":"Properties","text":"","category":"section"},{"location":"api/matrices/","page":"Matrices","title":"Matrices","text":"T = eltype(A)                          # Element type\nm, n = size(A)                         # Dimensions\nm = size(A, 1)                         # Rows\nn = size(A, 2)                         # Columns","category":"page"},{"location":"api/matrices/#Iteration","page":"Matrices","title":"Iteration","text":"","category":"section"},{"location":"api/matrices/","page":"Matrices","title":"Matrices","text":"# Iterate over rows (dense matrices only)\nfor row in eachrow(A)\n    # row is a view of the matrix row\n    process(row)\nend","category":"page"},{"location":"guide/matrices/#Matrices","page":"Matrices","title":"Matrices","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"SafePETSc provides distributed matrices through the Mat{T} type, which wraps PETSc's distributed matrix functionality with GPU-friendly operations and automatic memory management.","category":"page"},{"location":"guide/matrices/#Creating-Matrices","page":"Matrices","title":"Creating Matrices","text":"","category":"section"},{"location":"guide/matrices/#Uniform-Distribution","page":"Matrices","title":"Uniform Distribution","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"Use Mat_uniform when all ranks have the same data:","category":"page"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"# Create from dense matrix\nA = Mat_uniform([1.0 2.0; 3.0 4.0])\n\n# With custom partitions\nrow_part = [1, 2, 3]  # 2 ranks\ncol_part = [1, 2, 3]\nA = Mat_uniform(data; row_partition=row_part, col_partition=col_part)\n\n# With PETSc options prefix\nA = Mat_uniform(data; prefix=\"my_mat_\")","category":"page"},{"location":"guide/matrices/#Sum-Distribution","page":"Matrices","title":"Sum Distribution","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"Use Mat_sum when ranks contribute sparse entries:","category":"page"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"using SparseArrays\n\n# Each rank contributes different sparse entries\n# All contributions are summed\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nI = [1, rank+1]\nJ = [1, rank+1]\nV = [1.0, 2.0]\nA = Mat_sum(sparse(I, J, V, 10, 10))\n\n# Assert local ownership for validation\nA = Mat_sum(sparse_local; own_rank_only=true)","category":"page"},{"location":"guide/matrices/#Matrix-Operations","page":"Matrices","title":"Matrix Operations","text":"","category":"section"},{"location":"guide/matrices/#Linear-Algebra","page":"Matrices","title":"Linear Algebra","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"# Matrix-vector multiplication\ny = A * x\n\n# In-place\nmul!(y, A, x)\n\n# Matrix-matrix multiplication\nC = A * B\n\n# In-place\nmul!(C, A, B)\n\n# Transpose\nB = A'\nB = Mat(A')  # Materialize transpose\n\n# In-place transpose (reuses B)\ntranspose!(B, A)","category":"page"},{"location":"guide/matrices/#Concatenation","page":"Matrices","title":"Concatenation","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"# Vertical concatenation\nC = vcat(A, B)\nC = cat(A, B; dims=1)\n\n# Horizontal concatenation\nD = hcat(A, B)\nD = cat(A, B; dims=2)\n\n# Block diagonal\nE = blockdiag(A, B, C)","category":"page"},{"location":"guide/matrices/#Sparse-Diagonal-Matrices","page":"Matrices","title":"Sparse Diagonal Matrices","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"using SparseArrays\n\n# Create diagonal matrix from vectors\ndiag_vec = Vec_uniform(ones(100))\nupper_vec = Vec_uniform(ones(99))\nlower_vec = Vec_uniform(ones(99))\n\n# Tridiagonal matrix\nA = spdiagm(-1 => lower_vec, 0 => diag_vec, 1 => upper_vec)\n\n# Explicit dimensions\nA = spdiagm(100, 100, 0 => diag_vec, 1 => upper_vec)","category":"page"},{"location":"guide/matrices/#Transpose-Operations","page":"Matrices","title":"Transpose Operations","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"SafePETSc uses PETSc's efficient transpose operations:","category":"page"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"# Create transpose (new matrix)\nB = Mat(A')\n\n# Reuse transpose storage\nB = Mat(A')  # Initial creation\n# ... later, after A changes:\ntranspose!(B, A)  # Reuse B's storage","category":"page"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"Note: For transpose! to work correctly with PETSc's reuse mechanism, B should have been created as a transpose of A initially.","category":"page"},{"location":"guide/matrices/#Properties","page":"Matrices","title":"Properties","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"# Element type\nT = eltype(A)\n\n# Size\nm, n = size(A)\nm = size(A, 1)\nn = size(A, 2)\n\n# Partition information\nrow_part = A.obj.row_partition\ncol_part = A.obj.col_partition\nprefix = A.obj.prefix","category":"page"},{"location":"guide/matrices/#Partitioning","page":"Matrices","title":"Partitioning","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"Matrices have both row and column partitions.","category":"page"},{"location":"guide/matrices/#Default-Partitioning","page":"Matrices","title":"Default Partitioning","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"m, n = 100, 80\nnranks = MPI.Comm_size(MPI.COMM_WORLD)\n\nrow_part = default_row_partition(m, nranks)\ncol_part = default_row_partition(n, nranks)","category":"page"},{"location":"guide/matrices/#Requirements","page":"Matrices","title":"Requirements","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"Row operations require matching row partitions\nColumn operations require matching column partitions\nMatrix multiplication: C = A * B requires A.col_partition == B.row_partition","category":"page"},{"location":"guide/matrices/#GPU-Friendly-Operations","page":"Matrices","title":"GPU-Friendly Operations","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"SafePETSc prioritizes PETSc's native GPU-compatible operations:","category":"page"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"# Good: Uses PETSc's MatTranspose (GPU-friendly)\nB = Mat(A')\n\n# Good: Uses PETSc's MatMatMult (GPU-friendly)\nC = A * B\n\n# Good: Uses PETSc's MatConvert (GPU-friendly)\n# (internal to SafePETSc operations)","category":"page"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"Avoid element-by-element access patterns that cause excessive GPU↔CPU transfers.","category":"page"},{"location":"guide/matrices/#Advanced-Features","page":"Matrices","title":"Advanced Features","text":"","category":"section"},{"location":"guide/matrices/#Iterating-Over-Dense-Matrix-Rows","page":"Matrices","title":"Iterating Over Dense Matrix Rows","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"For MATMPIDENSE matrices:","category":"page"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"# Iterate over local rows efficiently\nfor row in eachrow(A)\n    # row is a view of the matrix row\n    process(row)\nend","category":"page"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"This uses a single MatDenseGetArrayRead call for the entire iteration.","category":"page"},{"location":"guide/matrices/#PETSc-Options","page":"Matrices","title":"PETSc Options","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"Configure matrix behavior via options:","category":"page"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"# Set global options\npetsc_options_insert_string(\"-dense_mat_type mpidense\")\n\n# Use prefix for specific matrices\nA = Mat_uniform(data; prefix=\"my_mat_\")","category":"page"},{"location":"guide/matrices/#Examples","page":"Matrices","title":"Examples","text":"","category":"section"},{"location":"guide/matrices/#Assemble-a-Sparse-Matrix","page":"Matrices","title":"Assemble a Sparse Matrix","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"using SafePETSc\nusing SparseArrays\nusing MPI\n\nSafePETSc.Init()\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nnranks = MPI.Comm_size(MPI.COMM_WORLD)\n\nn = 100\n\n# Each rank builds a local piece\nrow_part = default_row_partition(n, nranks)\nlo = row_part[rank + 1]\nhi = row_part[rank + 2] - 1\n\n# Build local sparse matrix (only owned rows)\nI = Int[]\nJ = Int[]\nV = Float64[]\n\nfor i in lo:hi\n    # Diagonal\n    push!(I, i)\n    push!(J, i)\n    push!(V, 2.0)\n\n    # Off-diagonal\n    if i > 1\n        push!(I, i)\n        push!(J, i-1)\n        push!(V, -1.0)\n    end\n    if i < n\n        push!(I, i)\n        push!(J, i+1)\n        push!(V, -1.0)\n    end\nend\n\nlocal_sparse = sparse(I, J, V, n, n)\n\n# Assemble global matrix\nA = Mat_sum(local_sparse; own_rank_only=true)","category":"page"},{"location":"guide/matrices/#Build-Block-Matrices","page":"Matrices","title":"Build Block Matrices","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"# Create blocks\nA11 = Mat_uniform(...)\nA12 = Mat_uniform(...)\nA21 = Mat_uniform(...)\nA22 = Mat_uniform(...)\n\n# Assemble block matrix\nA = vcat(hcat(A11, A12), hcat(A21, A22))\n\n# Or equivalently\nA = [A11 A12; A21 A22]  # (if block-matrix syntax is supported)","category":"page"},{"location":"guide/matrices/#Tridiagonal-System","page":"Matrices","title":"Tridiagonal System","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"n = 1000\n\n# Create diagonal vectors\ndiag = Vec_uniform(2.0 * ones(n))\nupper = Vec_uniform(-1.0 * ones(n-1))\nlower = Vec_uniform(-1.0 * ones(n-1))\n\n# Build tridiagonal matrix\nA = spdiagm(-1 => lower, 0 => diag, 1 => upper)\n\n# Create RHS\nb = Vec_uniform(ones(n))\n\n# Solve\nx = A \\ b","category":"page"},{"location":"guide/matrices/#Performance-Tips","page":"Matrices","title":"Performance Tips","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"Use Native Operations: Prefer PETSc operations over element access\nBatch Assembly: Build sparse matrices locally, then sum once\nAppropriate Matrix Type: Use dense vs. sparse based on structure\nReuse Solver Objects: Create Solver once, reuse for multiple solves\nGPU Configuration: Set PETSc options for GPU matrices","category":"page"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"# Good: bulk assembly\nlocal_matrix = sparse(I, J, V, m, n)\nA = Mat_sum(local_matrix)\n\n# Less good: element-by-element (if it were supported)\n# A = Mat_sum(...)\n# for each element\n#     set_value(A, i, j, val)  # Repeated MPI calls","category":"page"},{"location":"guide/matrices/#Compatibility-Notes","page":"Matrices","title":"Compatibility Notes","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"Transpose Reuse: transpose!(B, A) requires that B was created via Mat(A') or has a compatible precursor\nMatrix Multiplication Reuse: mul!(C, A, B) requires pre-allocated C with correct partitions\nDense Operations: Some operations (e.g., \\ with matrix RHS) require dense matrices","category":"page"},{"location":"guide/matrices/#See-Also","page":"Matrices","title":"See Also","text":"","category":"section"},{"location":"guide/matrices/","page":"Matrices","title":"Matrices","text":"Mat_uniform\nMat_sum\nspdiagm\nvcat, hcat, blockdiag","category":"page"},{"location":"#SafePETSc.jl","page":"Home","title":"SafePETSc.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SafePETSc is a Julia package that provides distributed reference management for MPI-based parallel computing with PETSc. The core purpose is to safely manage the lifecycle of distributed objects across MPI ranks, ensuring objects are destroyed only when all ranks have released their references.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Automatic Memory Management: Distributed objects are automatically tracked and destroyed when all MPI ranks have released their references\nGPU-Friendly Operations: Prioritizes PETSc's native GPU-compatible operations\nType-Safe API: Uses Julia's trait system to ensure only appropriate types are managed\nEfficient Cleanup: Centralized cleanup with configurable throttling to reduce overhead\nRich Linear Algebra: Comprehensive support for distributed vectors, matrices, and solvers","category":"page"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using SafePETSc\nusing MPI\n\n# Initialize MPI and PETSc\nSafePETSc.Init()\n\n# Create a distributed matrix\nA = Mat_uniform([2.0 1.0; 1.0 3.0])\n\n# Create a distributed vector\nb = Vec_uniform([1.0, 2.0])\n\n# Solve the linear system\nx = A \\ b\n\n# Objects are automatically cleaned up when they go out of scope","category":"page"},{"location":"#Package-Architecture","page":"Home","title":"Package Architecture","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SafePETSc consists of two main modules:","category":"page"},{"location":"#SafeMPI","page":"Home","title":"SafeMPI","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The SafeMPI module implements the distributed reference management system:","category":"page"},{"location":"","page":"Home","title":"Home","text":"DRef{T}: A distributed reference wrapper that tracks objects across MPI ranks\nDistributedRefManager: Coordinates reference counting with rank 0 as the coordinator\nTrait-based destruction: Types must opt-in to distributed management\nAutomatic cleanup: Finalizers trigger cleanup, with explicit check_and_destroy! control points","category":"page"},{"location":"#SafePETSc","page":"Home","title":"SafePETSc","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The main module wraps PETSc functionality with safe distributed reference management:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Vec{T}: Distributed vectors with automatic memory management\nMat{T}: Distributed matrices with GPU-friendly operations\nSolver{T}: Linear solver objects that can be reused","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"SafePETSc\")","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See the Getting Started guide for a tutorial on using SafePETSc.","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
